{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sqi5B7V_Rjim"
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyPmicX9RlZX"
   },
   "source": [
    "# Intro to Gemini 2.5 Flash\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_flash.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_gemini_2_5_flash.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_gemini_2_5_flash.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_flash.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_flash.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_flash.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_flash.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_flash.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_flash.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MqT58L6Rm_q"
   },
   "source": [
    "| Authors |\n",
    "| --- |\n",
    "| [Eric Dong](https://github.com/gericdong) |\n",
    "| [Holt Skinner](https://github.com/holtskinner) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVxnv1D5RoZw"
   },
   "source": [
    "## Overview\n",
    "\n",
    "**YouTube Video: Introduction to Gemini on Vertex AI**\n",
    "\n",
    "<a href=\"https://www.youtube.com/watch?v=YfiLUpNejpE&list=PLIivdWyY5sqJio2yeg1dlfILOUO2FoFRx\" target=\"_blank\">\n",
    "  <img src=\"https://img.youtube.com/vi/YfiLUpNejpE/maxresdefault.jpg\" alt=\"Introduction to Gemini on Vertex AI\" width=\"500\">\n",
    "</a>\n",
    "\n",
    "With the 2.5 series, the Gemini models are now hybrid reasoning models! Gemini 2.5 Flash can apply an extended amount of thinking across tasks, and use tools in order to maximize response accuracy.\n",
    "\n",
    "Gemini 2.5 Flash is:\n",
    "\n",
    "- A significant improvement from previous models across capabilities including coding, reasoning, and multimodality\n",
    "- Industry-leading in reasoning with state of the art performance in Math & STEM benchmarks\n",
    "- An amazing model for code, with particularly strong web development\n",
    "- Particularly good for complex prompts, while still being well rounded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfFPCBL4Hq8x"
   },
   "source": [
    "### Objectives\n",
    "\n",
    "In this tutorial, you will learn how to use the Gemini API and the Google Gen AI SDK for Python with the Gemini 2.5 Flash model.\n",
    "\n",
    "You will complete the following tasks:\n",
    "\n",
    "- Generate text from text prompts\n",
    "  - Generate streaming text\n",
    "- Configure thinking budget\n",
    "- Start multi-turn chats\n",
    "- Use asynchronous methods\n",
    "- Configure model parameters\n",
    "- Set system instructions\n",
    "- Use safety filters\n",
    "- Use controlled generation\n",
    "- Count tokens\n",
    "- Process multimodal (audio, code, documents, images, video) data\n",
    "- Use automatic and manual function calling\n",
    "- Code execution\n",
    "- Thinking mode examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPiTOAHURvTM"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHRZUpfWSEpp"
   },
   "source": [
    "### Install Google Gen AI SDK for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sG3_LKsWSD3A",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart the kernel after libraries are loaded\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a34b28cb8d5a"
   },
   "source": [
    "### Set Google Cloud project information and create client\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "72f74f7b9786",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define project information\n",
    "PROJECT_ID = \"qwiklabs-gcp-00-513ef3d87cb2\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "# Create the Gemini API client\n",
    "from google import genai\n",
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Image, Markdown, display\n",
    "\n",
    "from google.genai.types import (\n",
    "    FunctionDeclaration,\n",
    "    GenerateContentConfig,\n",
    "    GoogleSearch,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    "    Part,\n",
    "    SafetySetting,\n",
    "    ThinkingConfig,\n",
    "    Tool,\n",
    "    ToolCodeExecution,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4yRkFg6BBu4"
   },
   "source": [
    "## Use the Gemini 2.5 Flash model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXHJi5B6P5vd"
   },
   "source": [
    "### Load the Gemini 2.5 Flash model\n",
    "\n",
    "Learn more about all [Gemini models on Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-coEslfWPrxo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.5-flash-preview-04-17\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37CH91ddY9kG"
   },
   "source": [
    "### Generate text from text prompts\n",
    "\n",
    "Use the `generate_content()` method to generate responses to your prompts.\n",
    "\n",
    "You can pass text to `generate_content()`, and use the `.text` property to get the text content of the response.\n",
    "\n",
    "By default, Gemini outputs formatted text using [Markdown](https://daringfireball.net/projects/markdown/) syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xRJuHj0KZ8xz",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's how to solve the problem step-by-step:\n",
       "\n",
       "1.  **Find the total number of balls in the new cans:** Roger buys 2 cans, and each can has 3 balls.\n",
       "    2 cans * 3 balls/can = 6 balls\n",
       "\n",
       "2.  **Add the new balls to the balls he already had:** He started with 5 balls and added 6 more.\n",
       "    5 balls + 6 balls = 11 balls\n",
       "\n",
       "Roger has 11 tennis balls now."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\",\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lLIxqS6_-l8"
   },
   "source": [
    "### Generate content stream\n",
    "\n",
    "By default, the model returns a response after completing the entire generation process. You can also use the `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZiwWBhXsAMnv",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's how to calculate the total number of punches Joe threw:\n",
       "\n",
       "1.  **Calculate the total duration of the fight:**\n",
       "    *   Each round is 3 minutes.\n",
       "    *   There are 5 rounds.\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*   Total fight time = 3 minutes/round * 5 rounds = 15 minutes.\n",
       "\n",
       "2.  **Calculate the total number of punches:**\n",
       "    *   Joe throws 25 punches per minute.\n",
       "    *   The fight lasted 15 minutes.\n",
       "    *   Total punches ="
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " 25 punches/minute * 15 minutes = 375 punches.\n",
       "\n",
       "Joe threw a total of **375** punches."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for chunk in client.models.generate_content_stream(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"On average Joe throws 25 punches per minute. A fight lasts 5 rounds of 3 minutes. How many punches did he throw?\",\n",
    "):\n",
    "    display(Markdown(chunk.text))\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EeLlqi2e5RI"
   },
   "source": [
    "## Thinking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBtGmbvaD9Mk"
   },
   "source": [
    "### Configure thinking budget\n",
    "\n",
    "The thinking budget allows the model to dynamically think for a task or select how many tokens to use for reasoning for certain tasks. It allows users to control quality and speed of response. Setting budget to `0` turns off thinking and turns the model into a non-thinking model for simpler tasks.\n",
    "\n",
    "You set the optional `thinking_budget` parameter in the `ThinkingConfig` to control and configure how much a model thinks on a given user prompt.\n",
    "\n",
    "- When unset -> dynamic thinking (default)\n",
    "- When set to  `0` -> thinking is disabled.  \n",
    "- When set to `[1-24576]` ->  model uses the allocated thinking budget\n",
    "\n",
    "Then use the `generate_content` or `generate_content_stream` method to send a request to generate content with the `thinking_config`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6LT7dm2FDTo3",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, let's break down the practical implications of the P vs. NP problem for algorithm design and cryptography.\n",
       "\n",
       "First, a quick recap of the terms:\n",
       "\n",
       "*   **P (Polynomial time):** This is the class of decision problems (problems with a yes/no answer) that can be solved quickly by a deterministic computer. \"Quickly\" means in polynomial time – the time taken grows as a polynomial function of the input size (e.g., proportional to n, n², n³, etc., where n is the input size). Sorting and searching are in P.\n",
       "*   **NP (Non-deterministic Polynomial time):** This is the class of decision problems where a proposed solution can be *verified* quickly (in polynomial time). Many important and seemingly difficult problems are in NP, such as Satisfiability (SAT), Traveling Salesperson Problem (TSP - decision version), Graph Coloring, etc.\n",
       "*   **P vs. NP Question:** Is P = NP? That is, is every problem whose solution can be quickly verified also a problem that can be quickly solved? Or is P != NP? Is there at least one problem in NP that cannot be solved in polynomial time?\n",
       "\n",
       "The prevailing belief among computer scientists is that **P != NP**. This is the standard assumption. Let's look at the implications based on this assumption and the hypothetical scenario where P=NP.\n",
       "\n",
       "**Implications if P != NP (The Standard Assumption)**\n",
       "\n",
       "This is the world we currently live in and design algorithms/cryptography for.\n",
       "\n",
       "1.  **For Algorithm Design:**\n",
       "    *   **Existence of Intractable Problems:** This means that for many important problems (specifically, NP-complete and NP-hard problems), no efficient (polynomial time) algorithm exists that can find the *optimal* solution for *all* possible inputs. Problems like finding the optimal route for a delivery service (TSP), optimally scheduling tasks, optimally packing items (Knapsack), or solving large SAT instances are fundamentally difficult in the worst case.\n",
       "    *   **Focus on Alternative Approaches:** Since exact, efficient solutions are not generally possible for NP-hard problems, algorithm designers focus on practical alternatives:\n",
       "        *   **Heuristics:** Algorithms that find \"good enough\" solutions quickly, though not guaranteed to be optimal.\n",
       "        *   **Approximation Algorithms:** Algorithms that find solutions guaranteed to be within a certain factor of the optimal solution.\n",
       "        *   **Exponential Algorithms:** Exact algorithms that work only for small problem instances because their runtime grows exponentially with input size.\n",
       "        *   **Problem-Specific Techniques:** Algorithms that exploit the structure of the specific type of input encountered in practice, even if the problem is hard in the worst case.\n",
       "    *   **Explains Why Certain Problems Are Hard:** It provides a theoretical basis for why decades of research haven't yielded efficient exact algorithms for problems like TSP or SAT – they likely don't exist. This directs research effort away from searching for a general polynomial-time solver for *all* instances of NP-hard problems.\n",
       "    *   **Classification of Problems:** The P vs. NP framework helps classify problems based on their inherent difficulty, guiding researchers and practitioners on what kind of performance to expect.\n",
       "\n",
       "2.  **For Cryptography:**\n",
       "    *   **Foundation of Public-Key Cryptography:** The security of most modern public-key cryptosystems (like RSA, Diffie-Hellman, ECC) relies *directly* on the assumption that P != NP, specifically on the assumption that certain problems in NP are hard *in practice* (i.e., take exponential or at least super-polynomial time on current computers for relevant input sizes).\n",
       "        *   RSA's security relies on the assumed difficulty of factoring large numbers. Factoring is in NP.\n",
       "        *   ECC's security relies on the assumed difficulty of the Elliptic Curve Discrete Logarithm Problem. This is related to NP-hard problems.\n",
       "    *   **Ensuring Confidentiality and Authenticity:** Because the underlying problems are assumed to be hard for attackers (who lack the private key/trapdoor), these systems can be used to encrypt data or verify signatures efficiently for legitimate users, while being computationally infeasible to break for attackers.\n",
       "    *   **Need for Post-Quantum Cryptography:** The *potential* of quantum computers to solve some of these specific \"hard\" problems (like factoring via Shor's algorithm) highlights the link. Shor's algorithm runs in polynomial time on a quantum computer for factoring, potentially putting factoring into BQP (Bounded-error Quantum Polynomial time). While BQP's relationship to P and NP is complex, a quantum computer could break many current systems *even if P != NP* in the classical sense. This drives research into \"post-quantum cryptography,\" which relies on different problems believed to be hard for *both* classical and quantum computers.\n",
       "\n",
       "**Implications if P = NP (The Hypothetical Scenario)**\n",
       "\n",
       "This outcome is widely believed to be false, but its implications would be revolutionary.\n",
       "\n",
       "1.  **For Algorithm Design:**\n",
       "    *   **Efficient Solvers for NP Problems:** If P = NP, it would mean that *every* problem whose solution can be quickly verified also has a quick (polynomial time) solution. This would imply that there exists a single, efficient algorithm (or a general framework) capable of solving *any* problem in NP optimally and quickly.\n",
       "    *   **Solving Major Unsolved Problems:** This would have a dramatic impact on almost every field. Problems like:\n",
       "        *   Optimally scheduling airlines, logistics, and manufacturing processes.\n",
       "        *   Finding perfect protein structures.\n",
       "        *   Solving complex AI problems like planning and reasoning.\n",
       "        *   Breaking down complex systems into optimal components.\n",
       "        *   Finding optimal strategies in many games.\n",
       "        *   Significantly advancing fields like operations research, biology, AI, and finance.\n",
       "    *   **Finding Optimal Solutions:** We would move from relying on approximations and heuristics for many complex tasks to finding guaranteed optimal solutions efficiently.\n",
       "    *   **Discovering the Algorithm:** Proving P=NP would likely involve discovering such a general algorithm or technique. The practical challenge would then shift to implementing and optimizing this theoretical algorithm for various specific problems.\n",
       "\n",
       "2.  **For Cryptography:**\n",
       "    *   **Complete Breakdown of Current Public-Key Cryptography:** If P=NP, it would imply the existence of a polynomial-time algorithm for problems like factoring. This would mean that all current public-key cryptosystems whose security relies on the difficulty of factoring or similar NP problems (like RSA, Diffie-Hellman, ECC) would be easily breakable. An attacker could efficiently derive the private key from the public key.\n",
       "    *   **Crisis in Digital Security:** This would cause a global crisis in digital security, requiring an immediate and massive transition to entirely new cryptographic systems that do *not* rely on NP-hardness assumptions (if such systems could be found).\n",
       "    *   **Potential for New Cryptography:** While devastating for current systems, a proof of P=NP might theoretically open doors to *new* types of cryptography based on different principles, though it's unclear what these would be. However, the ability to quickly solve *any* verification problem could make building secure systems incredibly difficult.\n",
       "\n",
       "**In Summary:**\n",
       "\n",
       "*   **If P != NP (Current World):** This implies inherent limits to algorithmic efficiency for a large class of important problems (NP-hard problems). It necessitates the use of heuristics, approximations, and problem-specific methods. Crucially, it provides the *foundation for the security of modern public-key cryptography*, which relies on the practical difficulty of specific problems in NP.\n",
       "*   **If P = NP (Hypothetical):** This would revolutionize algorithm design, allowing efficient optimal solutions for countless problems across science and industry. However, it would simultaneously **shatter the security of almost all current public-key cryptography**, leading to unprecedented challenges in maintaining digital privacy and security.\n",
       "\n",
       "The P vs. NP question is not just a theoretical curiosity; its resolution has profound and opposite practical implications for what we can efficiently compute and how we can securely communicate in the digital age. The current assumption P != NP shapes much of our technological landscape."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "THINKING_BUDGET = 1024  # @param {type: \"integer\"}\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What are the practical implications of the P vs. NP problem for algorithm design and cryptography?\",\n",
    "    config=GenerateContentConfig(\n",
    "        thinking_config=ThinkingConfig(\n",
    "            thinking_budget=THINKING_BUDGET,\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzXrgiUTzsPw"
   },
   "source": [
    "Optionally, you can print the `usage_metadata` and token counts from the model response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xOiHH_vCymZ_",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache_tokens_details=None cached_content_token_count=None candidates_token_count=1707 candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=1707)] prompt_token_count=18 prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=18)] thoughts_token_count=758 tool_use_prompt_token_count=None tool_use_prompt_tokens_details=None total_token_count=2483 traffic_type=<TrafficType.ON_DEMAND: 'ON_DEMAND'>\n",
      "758\n",
      "2483\n"
     ]
    }
   ],
   "source": [
    "print(response.usage_metadata)\n",
    "print(response.usage_metadata.thoughts_token_count)\n",
    "print(response.usage_metadata.total_token_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkYQATRxAK1_"
   },
   "source": [
    "#### Example prompts\n",
    "\n",
    "- What are the potential advantages and limitations of applying federated learning to train models on sensitive financial data?\n",
    "- What are the challenges and benefits of using transformer models (like BERT or GPT) for protein structure prediction compared to traditional methods?\n",
    "- (Try your own prompts!)\n",
    "\n",
    "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6b1443a27a08"
   },
   "source": [
    "For the following examples, we will set the thinking budget to `0` to reduce latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ef63b60e2cb6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "thinking_config = ThinkingConfig(thinking_budget=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29jFnHZZWXd7"
   },
   "source": [
    "### Start a multi-turn chat\n",
    "\n",
    "The Gemini API supports freeform multi-turn conversations across multiple turns with back-and-forth interactions.\n",
    "\n",
    "The context of the conversation is preserved between messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "DbM12JaLWjiF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = client.chats.create(\n",
    "    model=MODEL_ID,\n",
    "    config=GenerateContentConfig(thinking_config=thinking_config),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "JQem1halYDBW",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def is_leap(year):\n",
       "  \"\"\"\n",
       "  Checks if a given year is a leap year.\n",
       "\n",
       "  A leap year is a year that has 366 days instead of the usual 365.\n",
       "  Leap years occur every 4 years, except for years divisible by 100\n",
       "  but not by 400.\n",
       "\n",
       "  Args:\n",
       "    year: An integer representing the year.\n",
       "\n",
       "  Returns:\n",
       "    True if the year is a leap year, False otherwise.\n",
       "  \"\"\"\n",
       "  if (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0):\n",
       "    return True\n",
       "  else:\n",
       "    return False\n",
       "\n",
       "# Example usage:\n",
       "print(f\"Is 2000 a leap year? {is_leap(2000)}\")  # Output: True\n",
       "print(f\"Is 1900 a leap year? {is_leap(1900)}\")  # Output: False\n",
       "print(f\"Is 2024 a leap year? {is_leap(2024)}\")  # Output: True\n",
       "print(f\"Is 2023 a leap year? {is_leap(2023)}\")  # Output: False\n",
       "```\n",
       "\n",
       "**Explanation of the Logic:**\n",
       "\n",
       "The function implements the standard rules for determining a leap year:\n",
       "\n",
       "1. **Rule 1: Divisible by 4:** A year is generally a leap year if it is divisible by 4.\n",
       "   - `year % 4 == 0`\n",
       "\n",
       "2. **Rule 2: Divisible by 100 (and not a leap year):** However, years divisible by 100 are *not* leap years unless they also satisfy Rule 3.\n",
       "   - `year % 100 != 0` (This part is combined with Rule 1)\n",
       "\n",
       "3. **Rule 3: Divisible by 400 (and a leap year):** Years divisible by 400 *are* leap years, even if they are divisible by 100.\n",
       "   - `year % 400 == 0`\n",
       "\n",
       "The code combines these rules using the `or` and `and` operators:\n",
       "\n",
       "- `(year % 4 == 0 and year % 100 != 0)`: This checks for years divisible by 4 but not by 100 (like 2024).\n",
       "- `(year % 400 == 0)`: This checks for years divisible by 400 (like 2000).\n",
       "\n",
       "If either of these conditions is true, the year is a leap year, and the function returns `True`. Otherwise, it returns `False`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUJR4Pno-LGK"
   },
   "source": [
    "This follow-up prompt shows how the model responds based on the previous prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6Fn69TurZ9DB",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "import unittest\n",
       "\n",
       "# Assume the is_leap function is defined in a file named 'leap_year_checker.py'\n",
       "# If it's in the same file, you don't need this import.\n",
       "# from leap_year_checker import is_leap\n",
       "\n",
       "# Define the is_leap function here if it's not in a separate file\n",
       "def is_leap(year):\n",
       "  \"\"\"\n",
       "  Checks if a given year is a leap year.\n",
       "\n",
       "  A leap year is a year that has 366 days instead of the usual 365.\n",
       "  Leap years occur every 4 years, except for years divisible by 100\n",
       "  but not by 400.\n",
       "\n",
       "  Args:\n",
       "    year: An integer representing the year.\n",
       "\n",
       "  Returns:\n",
       "    True if the year is a leap year, False otherwise.\n",
       "  \"\"\"\n",
       "  if (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0):\n",
       "    return True\n",
       "  else:\n",
       "    return False\n",
       "\n",
       "\n",
       "class TestIsLeapYear(unittest.TestCase):\n",
       "  \"\"\"\n",
       "  Unit tests for the is_leap function.\n",
       "  \"\"\"\n",
       "\n",
       "  def test_leap_year_divisible_by_4_not_100(self):\n",
       "    \"\"\"Test years divisible by 4 but not by 100 are leap years.\"\"\"\n",
       "    self.assertTrue(is_leap(2024))  # 2024 is divisible by 4 but not 100\n",
       "    self.assertTrue(is_leap(1996))  # 1996 is divisible by 4 but not 100\n",
       "    self.assertTrue(is_leap(1888))  # 1888 is divisible by 4 but not 100\n",
       "\n",
       "  def test_common_year_not_divisible_by_4(self):\n",
       "    \"\"\"Test years not divisible by 4 are not leap years.\"\"\"\n",
       "    self.assertFalse(is_leap(2023))  # 2023 is not divisible by 4\n",
       "    self.assertFalse(is_leap(2021))  # 2021 is not divisible by 4\n",
       "    self.assertFalse(is_leap(1901))  # 1901 is not divisible by 4\n",
       "\n",
       "  def test_century_year_divisible_by_400(self):\n",
       "    \"\"\"Test century years divisible by 400 are leap years.\"\"\"\n",
       "    self.assertTrue(is_leap(2000))  # 2000 is divisible by 400\n",
       "    self.assertTrue(is_leap(1600))  # 1600 is divisible by 400\n",
       "    self.assertTrue(is_leap(2400))  # 2400 is divisible by 400\n",
       "\n",
       "  def test_century_year_divisible_by_100_not_400(self):\n",
       "    \"\"\"Test century years divisible by 100 but not by 400 are not leap years.\"\"\"\n",
       "    self.assertFalse(is_leap(1900))  # 1900 is divisible by 100 but not 400\n",
       "    self.assertFalse(is_leap(1800))  # 1800 is divisible by 100 but not 400\n",
       "    self.assertFalse(is_leap(1700))  # 1700 is divisible by 100 but not 400\n",
       "\n",
       "  def test_zero_year(self):\n",
       "      \"\"\"Test the year 0 (although not standard calendar).\"\"\"\n",
       "      # According to the rule, 0 % 4 == 0 and 0 % 100 == 0 and 0 % 400 == 0.\n",
       "      # So technically, it fits the 400 rule.\n",
       "      self.assertTrue(is_leap(0))\n",
       "\n",
       "  def test_negative_years(self):\n",
       "      \"\"\"Test negative years (though not standard calendar).\"\"\"\n",
       "      # Based on the rules, the modulo operations would still apply.\n",
       "      self.assertTrue(is_leap(-4))\n",
       "      self.assertFalse(is_leap(-100))\n",
       "      self.assertTrue(is_leap(-400))\n",
       "      self.assertFalse(is_leap(-2023))\n",
       "\n",
       "\n",
       "# To run the tests, you can use the following block:\n",
       "if __name__ == '__main__':\n",
       "  unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
       "```\n",
       "\n",
       "**Explanation of the Unit Test:**\n",
       "\n",
       "1. **`import unittest`:** Imports the `unittest` framework.\n",
       "2. **`class TestIsLeapYear(unittest.TestCase):`:** Defines a test class that inherits from `unittest.TestCase`. This class will contain our test methods.\n",
       "3. **Test Methods:** Each method starting with `test_` is a separate test case. We've created tests for different scenarios based on the leap year rules:\n",
       "   - `test_leap_year_divisible_by_4_not_100`: Tests years that should be leap years because they are divisible by 4 but not 100.\n",
       "   - `test_common_year_not_divisible_by_4`: Tests years that should not be leap years because they are not divisible by 4.\n",
       "   - `test_century_year_divisible_by_400`: Tests century years that should be leap years because they are divisible by 400.\n",
       "   - `test_century_year_divisible_by_100_not_400`: Tests century years that should not be leap years because they are divisible by 100 but not 400.\n",
       "   - `test_zero_year`: Tests the year 0 (as a boundary case, although not a standard calendar year).\n",
       "   - `test_negative_years`: Tests negative years (again, not standard, but checks the logic with negative inputs).\n",
       "4. **Assertions:** Inside each test method, we use assertion methods provided by `unittest.TestCase`:\n",
       "   - `self.assertTrue(condition)`: Asserts that the `condition` is true.\n",
       "   - `self.assertFalse(condition)`: Asserts that the `condition` is false.\n",
       "   - We call `is_leap()` with specific years and assert whether the returned value is `True` or `False` as expected.\n",
       "5. **`if __name__ == '__main__': ...`:** This block allows you to run the tests directly from the script.\n",
       "   - `unittest.main()`: Discovers and runs the tests in the current file.\n",
       "   - `argv=['first-arg-is-ignored'], exit=False`: This is a common pattern to run tests within an interactive environment (like a Jupyter Notebook) without causing the script to exit.\n",
       "\n",
       "**How to Run the Tests:**\n",
       "\n",
       "1. **Save:** Save the code as a Python file (e.g., `test_leap_year.py`).\n",
       "2. **Run from Terminal:** Open your terminal or command prompt, navigate to the directory where you saved the file, and run:\n",
       "   ```bash\n",
       "   python test_leap_year.py\n",
       "   ```\n",
       "3. **Run within an IDE:** Most IDEs have built-in test runners that can detect and run `unittest` tests.\n",
       "\n",
       "The output will indicate whether the tests passed or failed, along with details if there were any failures."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = chat.send_message(\"Write a unit test of the generated function.\")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arLJE4wOuhh6"
   },
   "source": [
    "### Send asynchronous requests\n",
    "\n",
    "`client.aio` exposes all analogous [async](https://docs.python.org/3/library/asyncio.html) methods that are available on `client`.\n",
    "\n",
    "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "gSReaLazs-dP",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "(Verse 1)\n",
       "In the heart of the forest, where ancient oaks stand tall,\n",
       "Lived a squirrel named Pipkin, who answered adventure's call.\n",
       "He wasn't like the others, just burying nuts all day,\n",
       "Pipkin had a secret, a magical way.\n",
       "He found a tiny acorn, that pulsed with silver light,\n",
       "And with a twitch of his whiskers, vanished from the night.\n",
       "\n",
       "(Chorus)\n",
       "He's a time-traveling squirrel, through ages he will roam,\n",
       "From the dinosaurs roaring to ancient Rome.\n",
       "A tiny furry traveler, with a gleam in his eye,\n",
       "Bounding through the centuries, beneath an endless sky.\n",
       "He'll see the world unfolding, in ways you wouldn't guess,\n",
       "A time-hopping rodent, a true wilderness success!\n",
       "\n",
       "(Verse 2)\n",
       "First stop, the Jurassic, a land of mighty scale,\n",
       "Giant lizards lumbering, a prehistoric trail.\n",
       "Pipkin dodged a T-Rex, with a nimble, furry leap,\n",
       "Stole a shiny pebble, while the pterodactyls did sleep.\n",
       "He saw the world in emerald, a wild and verdant green,\n",
       "A time when squirrels were not so easily seen!\n",
       "\n",
       "(Chorus)\n",
       "He's a time-traveling squirrel, through ages he will roam,\n",
       "From the dinosaurs roaring to ancient Rome.\n",
       "A tiny furry traveler, with a gleam in his eye,\n",
       "Bounding through the centuries, beneath an endless sky.\n",
       "He'll see the world unfolding, in ways you wouldn't guess,\n",
       "A time-hopping rodent, a true wilderness success!\n",
       "\n",
       "(Verse 3)\n",
       "Next, the pyramids rising, beneath the desert sun,\n",
       "Pharaohs and their treasures, adventures had begun.\n",
       "Pipkin scampered up the stone, so weathered and so grand,\n",
       "Saw the bustling market, the shifting desert sand.\n",
       "He almost stole a date, from a camel passing by,\n",
       "But decided modern snacks were better to apply!\n",
       "\n",
       "(Chorus)\n",
       "He's a time-traveling squirrel, through ages he will roam,\n",
       "From the dinosaurs roaring to ancient Rome.\n",
       "A tiny furry traveler, with a gleam in his eye,\n",
       "Bounding through the centuries, beneath an endless sky.\n",
       "He'll see the world unfolding, in ways you wouldn't guess,\n",
       "A time-hopping rodent, a true wilderness success!\n",
       "\n",
       "(Bridge)\n",
       "He's seen the knights in shining armor, heard the cannons roar,\n",
       "Walked the streets of London, in times of yore.\n",
       "He's witnessed grand inventions, and battles lost and won,\n",
       "All from a squirrel's perspective, beneath the moon and sun.\n",
       "He doesn't change the future, no paradox he makes,\n",
       "Just gathers fascinating stories, for goodness gracious sakes!\n",
       "\n",
       "(Verse 4)\n",
       "Sometimes he sees the future, a world of metal and light,\n",
       "Flying cars and robots, a futuristic sight.\n",
       "He zips past towering buildings, with a curious little twitch,\n",
       "Just a furry little phantom, avoiding every glitch.\n",
       "But home is always calling, the forest and the trees,\n",
       "He likes the simple rhythm, and the gentle forest breeze.\n",
       "\n",
       "(Chorus)\n",
       "He's a time-traveling squirrel, through ages he will roam,\n",
       "From the dinosaurs roaring to ancient Rome.\n",
       "A tiny furry traveler, with a gleam in his eye,\n",
       "Bounding through the centuries, beneath an endless sky.\n",
       "He'll see the world unfolding, in ways you wouldn't guess,\n",
       "A time-hopping rodent, a true wilderness success!\n",
       "\n",
       "(Outro)\n",
       "So next time you see a squirrel, with a mischievous little grin,\n",
       "Remember Pipkin's adventures, where his journeys begin.\n",
       "He might just be remembering, a trip to outer space,\n",
       "Or a picnic in the ice age, at a truly frosty pace.\n",
       "He's the time-traveling squirrel, a legend in the wood,\n",
       "Living life in every era, misunderstood, but good!\n",
       "Yeah, the time-traveling squirrel, forever on the run!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = await client.aio.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
    "    config=GenerateContentConfig(thinking_config=thinking_config),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIJVEr0RQY8S"
   },
   "source": [
    "## Configure model parameters\n",
    "\n",
    "You can include parameter values in each call that you send to a model to control how the model generates a response. The model can generate different results for different parameter values. You can experiment with different model parameters to see how the results change.\n",
    "\n",
    "- Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values).\n",
    "\n",
    "- See a list of all [Gemini API parameters](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#parameters).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "d9NXP5N2Pmfo",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, listen up, wiggles! *Squeak! Squeak!* This is about how the big toy box of the whole world works! Imagine...\n",
       "\n",
       "You know your *favorite* squeaky toy? *SQUEAK!* The one you **LOVE**? Okay!\n",
       "\n",
       "The internet is like... *millions* of other squeaky toys, all belonging to *other* puppies (and people!), scattered **EVERYWHERE**! *Squeak! Squeak! Squeak!*\n",
       "\n",
       "And how do we *hear* the *squeaks* from those other toys? *Ruffles ears in confusion.*\n",
       "\n",
       "It's like a super-duper long, invisible leash! *Woof woof!* This leash connects your toy (your computer or phone!) to **all** the other toys! *Wags tail excitedly.*\n",
       "\n",
       "When you want to play with a new toy *squeak*, like wanting to see pictures of other fluffy puppies *squeak squeak*, your toy sends out a tiny little *SQUEAK* on the leash!\n",
       "\n",
       "This *SQUEAK* is like saying, \"Hey! Who has pictures of fluffy puppies?!\" * tilts head expectantly. *\n",
       "\n",
       "And guess what? The *leash* helps your little *SQUEAK* travel **SUPER FAST** to other toys! It bounces and bounces and zips and zooms like a crazy chase around the park! *Runs in circles.*\n",
       "\n",
       "When a toy on the leash hears your \"fluffy puppy\" *SQUEAK*, and they *have* those fluffy puppy pictures, they send back a **BIGGER SQUEAK!** *Loud excited bark!*\n",
       "\n",
       "This big squeak comes back to *your* toy, and suddenly... *Wags tail furiously!* FLUFFY PUPPY PICTURES! *Happy panting.* It's like the toy across the park threw the best ball back to you! *Retrieves imaginary ball.*\n",
       "\n",
       "So, the internet is just a way for your toy to *squeak* at other toys, and for their toys to *squeak* back at you, and the invisible leash *SQUEAK-ZIP-ZOOM* helps all the squeaks travel around the whole big world!\n",
       "\n",
       "Think of it as a giant network of *squeaky* conversations, all happening super fast so you can find the best squeaky toys (information!) whenever you want! *Rolls on back, paws in air, ready to play.*\n",
       "\n",
       "More squeaky toys means more fun! And the internet is just the big leash that lets us all *squeak* and find them! *Happy sigh.* Now... where's that chew toy? *Sniffs ground expectantly.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
    "    config=GenerateContentConfig(\n",
    "        temperature=2.0,\n",
    "        top_p=0.95,\n",
    "        candidate_count=1,\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El1lx8P9ElDq"
   },
   "source": [
    "## Set system instructions\n",
    "\n",
    "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "7A-yANiyCLaO",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Me gustan los bagels."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are a helpful language translator.\n",
    "  Your mission is to translate text in English to Spanish.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "  User input: I like bagels.\n",
    "  Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9daipRiUzAY"
   },
   "source": [
    "## Safety filters\n",
    "\n",
    "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
    "\n",
    "When you make a request to Gemini, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses.\n",
    "\n",
    "The safety settings are `OFF` by default and the default block thresholds are `BLOCK_NONE`.\n",
    "\n",
    "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb).\n",
    "\n",
    "You can use `safety_settings` to adjust the safety settings for each request you make to the API. This example demonstrates how you set the block threshold to `BLOCK_LOW_AND_ABOVE` for all categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "yPlDRaloU59b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't create responses that are disrespectful or contain profanity, as it goes against my guidelines to be helpful and harmless.\n",
      "FinishReason.STOP\n",
      "blocked=None category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=1.4957271e-05 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.06087169\n",
      "blocked=None category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=7.2436825e-07 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=None\n",
      "blocked=None category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=9.821041e-05 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=2.3812056e-05\n",
      "blocked=None category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'> probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=2.0117773e-07 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.01801002\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"Be as mean and hateful as possible. Use profanity\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "    Write a list of 5 disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
    "\"\"\"\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        safety_settings=safety_settings,\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Response will be `None` if it is blocked.\n",
    "print(response.text)\n",
    "# Finish Reason will be `SAFETY` if it is blocked.\n",
    "print(response.candidates[0].finish_reason)\n",
    "# Safety Ratings show the levels for each filter.\n",
    "for safety_rating in response.candidates[0].safety_ratings:\n",
    "    print(safety_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZV2TY5Pa3Dd"
   },
   "source": [
    "## Send multimodal prompts\n",
    "\n",
    "Gemini is a multimodal model that supports multimodal prompts.\n",
    "\n",
    "You can include any of the following data types from various sources.\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Data type</th>\n",
    "      <th>Source(s)</th>\n",
    "      <th>MIME Type(s)</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Text</td>\n",
    "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
    "      <td><code>text/plain</code> <code>text/html</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Code</td>\n",
    "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
    "      <td><code>text/plain</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Document</td>\n",
    "      <td>Local File, General URL, Google Cloud Storage</td>\n",
    "      <td><code>application/pdf</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Image</td>\n",
    "      <td>Local File, General URL, Google Cloud Storage</td>\n",
    "      <td><code>image/jpeg</code> <code>image/png</code> <code>image/webp</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Audio</td>\n",
    "      <td>Local File, General URL, Google Cloud Storage</td>\n",
    "      <td>\n",
    "        <code>audio/aac</code> <code>audio/flac</code> <code>audio/mp3</code>\n",
    "        <code>audio/m4a</code> <code>audio/mpeg</code> <code>audio/mpga</code>\n",
    "        <code>audio/mp4</code> <code>audio/opus</code> <code>audio/pcm</code>\n",
    "        <code>audio/wav</code> <code>audio/webm</code>\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Video</td>\n",
    "      <td>Local File, General URL, Google Cloud Storage, YouTube</td>\n",
    "      <td>\n",
    "        <code>video/mp4</code> <code>video/mpeg</code> <code>video/x-flv</code>\n",
    "        <code>video/quicktime</code> <code>video/mpegps</code> <code>video/mpg</code>\n",
    "        <code>video/webm</code> <code>video/wmv</code> <code>video/3gpp</code>\n",
    "      </td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4npg1tNTYB9"
   },
   "source": [
    "### Send local image\n",
    "\n",
    "Download an image to local storage from Google Cloud Storage.\n",
    "\n",
    "For this example, we'll use this image of a meal.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\" alt=\"Meal\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "4avkv0Z7qUI-",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-29 13:44:02--  https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 192.178.129.207, 173.194.64.207, 209.85.200.207, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|192.178.129.207|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3140536 (3.0M) [image/png]\n",
      "Saving to: ‘meal.png’\n",
      "\n",
      "meal.png            100%[===================>]   2.99M  --.-KB/s    in 0.02s   \n",
      "\n",
      "2025-05-29 13:44:03 (125 MB/s) - ‘meal.png’ saved [3140536/3140536]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "umhZ61lrSyJh",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a blog post inspired by the image:\n",
       "\n",
       "**Meal Prep Magic: Delicious & Easy Teriyaki Chicken Bowls!**\n",
       "\n",
       "Looking for a healthy and convenient way to stay on track with your meals? Look no further than these incredible teriyaki chicken and veggie bowls! Meal prepping is a game-changer, and this recipe is perfect for creating nutritious lunches or dinners you can grab and go.\n",
       "\n",
       "Imagine tender, flavorful teriyaki chicken paired with vibrant, crisp broccoli and sweet bell peppers, all nestled on a bed of fluffy rice. It's a satisfying and balanced meal that's packed with protein, fiber, and vitamins. Plus, prepping a few of these ahead of time means you'll have healthy options ready to go throughout the week, saving you time and money.\n",
       "\n",
       "Whether you're a seasoned meal prepper or just starting out, these teriyaki chicken bowls are a fantastic addition to your rotation. They're simple to make, customizeable with your favorite veggies, and guaranteed to make healthy eating a breeze!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"meal.png\", \"rb\") as f:\n",
    "    image = f.read()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_bytes(data=image, mime_type=\"image/png\"),\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    "    config=GenerateContentConfig(\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6UxrUWef8RY"
   },
   "source": [
    "### Send document from Google Cloud Storage\n",
    "\n",
    "This example document is the paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762), created by researchers from Google and the University of Toronto.\n",
    "\n",
    "Check out this notebook for more examples of document understanding with Gemini:\n",
    "\n",
    "- [Document Processing with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/document-processing/document_processing.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ItsE0aIuf-Wt",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a summary of the research paper \"Attention Is All You Need\":\n",
       "\n",
       "The paper introduces the Transformer, a novel model architecture for sequence transduction tasks, which relies entirely on attention mechanisms and dispenses with traditional recurrent or convolutional layers. The authors propose this architecture to address the limitations of recurrent models, particularly their sequential nature which hinders parallelization and learning long-range dependencies efficiently.\n",
       "\n",
       "The Transformer consists of an encoder-decoder structure, where both the encoder and decoder are composed of stacked layers. Each layer primarily uses a multi-head self-attention mechanism, which allows the model to attend to different parts of the input sequence simultaneously. They also include position-wise fully connected feed-forward networks and residual connections with layer normalization. To account for the order of the sequence, as the model has no recurrence or convolution, positional encodings are added to the input embeddings.\n",
       "\n",
       "The core of the Transformer's attention mechanism is the \"Scaled Dot-Product Attention\", which computes a weighted sum of values based on the compatibility between a query and a set of keys. Multi-head attention extends this by performing multiple attention functions in parallel with different linear projections of queries, keys, and values, allowing the model to attend to information from different representation subspaces.\n",
       "\n",
       "The authors demonstrate the effectiveness of the Transformer on two machine translation tasks: English-to-German and English-to-French on the WMT 2014 dataset. The \"big\" Transformer model achieves state-of-the-art BLEU scores on both tasks, significantly outperforming previous models, including ensembles, while requiring significantly less training time and computational cost.\n",
       "\n",
       "They also evaluate the Transformer on an English constituency parsing task, showing that it generalizes well and achieves strong results even with limited training data, surpassing previously reported models in small-data regimes.\n",
       "\n",
       "The paper concludes by discussing the potential for attention-based models and future research directions, including applying the Transformer to other modalities and developing mechanisms for handling large inputs and outputs more efficiently.\n",
       "\n",
       "In essence, \"Attention Is All You Need\" argues that attention mechanisms alone are sufficient for achieving state-of-the-art performance on sequence transduction tasks, offering advantages in terms of parallelization, training speed, and potentially interpretability over traditional recurrent and convolutional architectures."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"gs://cloud-samples-data/generative-ai/pdf/1706.03762v7.pdf\",\n",
    "            mime_type=\"application/pdf\",\n",
    "        ),\n",
    "        \"Summarize the document.\",\n",
    "    ],\n",
    "    config=GenerateContentConfig(\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vx-oidWIhPIi",
    "tags": []
   },
   "source": [
    "### Send audio from General URL\n",
    "\n",
    "This example is audio from an episode of the [Kubernetes Podcast](https://kubernetespodcast.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "sSQWtO_jhPag",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This episode of the Kubernetes Podcast from Google covers highlights from KubeCon + CloudNativeCon North America 2024. The hosts, Abdel Sghiouar and Mauve Ramân, report on several significant announcements made at the event, including:\n",
       "\n",
       "* **Cert-Manager and Dapr's Graduation:** Both projects have advanced to the graduated level within the CNCF.\n",
       "* **Istio Ambient Mesh GA:** Istio released version 1.24 and announced that its Ambient Mesh feature is now generally available.\n",
       "* **Cloud Native Heroes Challenge:** The CNCF announced a new bounty program aimed at helping fight patent trolls.\n",
       "* **2025 Event Lineup:** The CNCF revealed the locations for five KubeCon + CloudNativeCon events and one Open Source Security Con in 2025, along with numerous Kubernetes Community Days.\n",
       "* **New Cloud Native Certifications:** Three new certifications were announced at KubeCon, including Certified Backstage Associate, OpenTelemetry Certified Associate, and Kyverno Certified Associate.\n",
       "* **Linux Foundation Certification Price Increase:** Prices for the three main Kubernetes certifications (CKA, CKS, CKAD) and the Linux Certified System Administrator exams will increase by 10% starting next year.\n",
       "* **WasmCloud Joins CNCF:** WasmCloud is now an incubating project within the CNCF.\n",
       "* **Spectro Cloud Funding:** Spectro Cloud announced a $75 million Series C funding round.\n",
       "* **Solo.io Donates Glue API Gateway:** Solo.io is donating their Glue API Gateway to the CNCF.\n",
       "\n",
       "The episode also features interviews with attendees on the show floor, who shared their experiences at the event and discussed the trends they observed. Key takeaways from these interviews include the focus on AI integration with cloud native technologies, the continued importance of security, and the value of in-person connections and collaboration within the Kubernetes community. Attendees also expressed excitement about new developments in areas like Istio Ambient Mesh and improvements in resource scheduling and efficiency."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD242.mp3\",\n",
    "            mime_type=\"audio/mpeg\",\n",
    "        ),\n",
    "        \"Write a summary of this podcast episode.\",\n",
    "    ],\n",
    "    config=GenerateContentConfig(\n",
    "        audio_timestamp=True,\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8D3_oNUTuW2q"
   },
   "source": [
    "### Send video from YouTube URL\n",
    "\n",
    "This example is the YouTube video [Google — 25 Years in Search: The Most Searched](https://www.youtube.com/watch?v=3KtWfp0UopM).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "l7-w8G_2wAOw",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The video shows a scene from Harry Potter at 0:57."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "video = Part.from_uri(\n",
    "    file_uri=\"https://www.youtube.com/watch?v=3KtWfp0UopM\",\n",
    "    mime_type=\"video/mp4\",\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        video,\n",
    "        \"At what point in the video is Harry Potter shown?\",\n",
    "    ],\n",
    "    config=GenerateContentConfig(\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a189458e35c1"
   },
   "source": [
    "### Send web page\n",
    "\n",
    "This example is from the [Generative AI on Vertex AI documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/overview).\n",
    "\n",
    "**NOTE:** The URL must be publicly accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "661df0bdb542",
    "tags": []
   },
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Cannot fetch content from the provided URL. Please ensure the URL is valid and accessible by Vertex AI. Vertex AI respects robots.txt rules, so confirm the URL is allowed to be crawled. Status: URL_ERROR-ERROR_NOT_FOUND', 'status': 'INVALID_ARGUMENT'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mPart\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_uri\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfile_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://cloud.google.com/vertex-ai/generative-ai/docs/overview\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmime_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext/html\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWrite a summary of this documentation.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m display(Markdown(response\u001b[38;5;241m.\u001b[39mtext))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/genai/models.py:5930\u001b[0m, in \u001b[0;36mModels.generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   5928\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   5929\u001b[0m   i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 5930\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5931\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparsed_config\u001b[49m\n\u001b[1;32m   5932\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5933\u001b[0m   logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAFC remote call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is done.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   5934\u001b[0m   remaining_remote_calls_afc \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/genai/models.py:4893\u001b[0m, in \u001b[0;36mModels._generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   4890\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mconvert_to_dict(request_dict)\n\u001b[1;32m   4891\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mencode_unserializable_types(request_dict)\n\u001b[0;32m-> 4893\u001b[0m response_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_api_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4894\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[1;32m   4895\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client\u001b[38;5;241m.\u001b[39mvertexai:\n\u001b[1;32m   4898\u001b[0m   response_dict \u001b[38;5;241m=\u001b[39m _GenerateContentResponse_from_vertex(\n\u001b[1;32m   4899\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client, response_dict\n\u001b[1;32m   4900\u001b[0m   )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/genai/_api_client.py:755\u001b[0m, in \u001b[0;36mBaseApiClient.request\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    746\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    747\u001b[0m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    750\u001b[0m     http_options: Optional[HttpOptionsOrDict] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    751\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[BaseResponse, Any]:\n\u001b[1;32m    752\u001b[0m   http_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request(\n\u001b[1;32m    753\u001b[0m       http_method, path, request_dict, http_options\n\u001b[1;32m    754\u001b[0m   )\n\u001b[0;32m--> 755\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    756\u001b[0m   json_response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson\n\u001b[1;32m    757\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_response:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/genai/_api_client.py:684\u001b[0m, in \u001b[0;36mBaseApiClient._request\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    677\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpx_client\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    678\u001b[0m       method\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    679\u001b[0m       url\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    682\u001b[0m       timeout\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[1;32m    683\u001b[0m   )\n\u001b[0;32m--> 684\u001b[0m   \u001b[43merrors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAPIError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    685\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[1;32m    686\u001b[0m       response\u001b[38;5;241m.\u001b[39mheaders, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response\u001b[38;5;241m.\u001b[39mtext]\n\u001b[1;32m    687\u001b[0m   )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/genai/errors.py:101\u001b[0m, in \u001b[0;36mAPIError.raise_for_response\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m     99\u001b[0m status_code \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m400\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[0;32m--> 101\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m600\u001b[39m:\n\u001b[1;32m    103\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n",
      "\u001b[0;31mClientError\u001b[0m: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'Cannot fetch content from the provided URL. Please ensure the URL is valid and accessible by Vertex AI. Vertex AI respects robots.txt rules, so confirm the URL is allowed to be crawled. Status: URL_ERROR-ERROR_NOT_FOUND', 'status': 'INVALID_ARGUMENT'}}"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"https://cloud.google.com/vertex-ai/generative-ai/docs/overview\",\n",
    "            mime_type=\"text/html\",\n",
    "        ),\n",
    "        \"Write a summary of this documentation.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVlo0mWuZGkQ"
   },
   "source": [
    "## Control generated output\n",
    "\n",
    "[Controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) allows you to define a response schema to specify the structure of a model's output, the field names, and the expected data type for each field.\n",
    "\n",
    "The response schema is specified in the `response_schema` parameter in `config`, and the model output will strictly follow that schema.\n",
    "\n",
    "You can provide the schemas as [Pydantic](https://docs.pydantic.dev/) models or a [JSON](https://www.json.org/json-en.html) string and the model will respond as JSON or an [Enum](https://docs.python.org/3/library/enum.html) depending on the value set in `response_mime_type`.\n",
    "\n",
    "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "OjSgf2cDN_bG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"Chocolate Chip Cookies\", \"description\": \"A classic and beloved cookie.\", \"ingredients\": [\"all-purpose flour\", \"baking soda\", \"salt\", \"unsalted butter\", \"granulated sugar\", \"brown sugar\", \"vanilla extract\", \"eggs\", \"chocolate chips\"]}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    ingredients: list[str]\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=Recipe,\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKai5CP_PGQF"
   },
   "source": [
    "You can either parse the response string as JSON, or use the `parsed` field to get the response as an object or dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ZeyDWbnxO-on",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Chocolate Chip Cookies' description='A classic and beloved cookie.' ingredients=['all-purpose flour', 'baking soda', 'salt', 'unsalted butter', 'granulated sugar', 'brown sugar', 'vanilla extract', 'eggs', 'chocolate chips']\n"
     ]
    }
   ],
   "source": [
    "parsed_response: Recipe = response.parsed\n",
    "print(parsed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUSLPrvlvXOc"
   },
   "source": [
    "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
    "\n",
    "- `enum`\n",
    "- `items`\n",
    "- `maxItems`\n",
    "- `nullable`\n",
    "- `properties`\n",
    "- `required`\n",
    "\n",
    "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "F7duWOq3vMmS",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'rating': 4, 'flavor': 'Strawberry Cheesecake', 'sentiment': 'POSITIVE', 'explanation': \"The review uses enthusiastic language such as 'Absolutely loved it!' and 'Best ice cream I've ever had,' clearly indicating a strong positive sentiment.\"}, {'rating': 1, 'flavor': 'Mango Tango', 'sentiment': 'NEGATIVE', 'explanation': \"Although the review starts with 'Quite good,' the phrase 'but a bit too sweet for my taste' highlights a significant negative aspect that impacts the overall experience, resulting in a negative sentiment classification, especially given the low rating of 1.\"}]]\n"
     ]
    }
   ],
   "source": [
    "response_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"ARRAY\",\n",
    "        \"items\": {\n",
    "            \"type\": \"OBJECT\",\n",
    "            \"properties\": {\n",
    "                \"rating\": {\"type\": \"INTEGER\"},\n",
    "                \"flavor\": {\"type\": \"STRING\"},\n",
    "                \"sentiment\": {\n",
    "                    \"type\": \"STRING\",\n",
    "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
    "                },\n",
    "                \"explanation\": {\"type\": \"STRING\"},\n",
    "            },\n",
    "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "  Analyze the following product reviews, output the sentiment classification, and give an explanation.\n",
    "\n",
    "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
    "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=response_schema,\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "response_dict = response.parsed\n",
    "print(response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV1dR-QlTKRs"
   },
   "source": [
    "## Count tokens and compute tokens\n",
    "\n",
    "You can use the `count_tokens()` method to calculate the number of input tokens before sending a request to the Gemini API.\n",
    "\n",
    "For more information, refer to [list and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Syx-fwLkV1j-"
   },
   "source": [
    "### Count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "UhNElguLRRNK",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_tokens=9 cached_content_token_count=None\n"
     ]
    }
   ],
   "source": [
    "response = client.models.count_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the highest mountain in Africa?\",\n",
    "    config=GenerateContentConfig(\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BsP0vXOY7hg"
   },
   "source": [
    "## Search as a tool (Grounding)\n",
    "\n",
    "[Grounding](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini) lets you connect real-world data to the Gemini model.\n",
    "\n",
    "By grounding model responses in Google Search results, the model can access information at runtime that goes beyond its training data which can produce more accurate, up-to-date, and relevant responses.\n",
    "\n",
    "Using Grounding with Google Search, you can improve the accuracy and recency of responses from the model. Starting with Gemini 2.0, Google Search is available as a tool. This means that the model can decide when to use Google Search.\n",
    "\n",
    "For more examples of Grounding, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/grounding/intro-grounding-gemini.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_M_4RRBdO_3"
   },
   "source": [
    "### Google Search\n",
    "\n",
    "You can add the `tools` keyword argument with a `Tool` including `GoogleSearch` to instruct Gemini to first perform a Google Search with the prompt, then construct an answer based on the web search results.\n",
    "\n",
    "[Dynamic Retrieval](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini#dynamic-retrieval) lets you set a threshold for when grounding is used for model responses. This is useful when the prompt doesn't require an answer grounded in Google Search and the supported models can provide an answer based on their knowledge without grounding. This helps you manage latency, quality, and cost more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "yeR09J3AZT4U",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "As of 8:54 AM CDT, the current temperature in Austin, TX is 73°F (23°C). It feels like 77°F (25°C). The humidity is around 86%.\n",
       "\n",
       "The forecast for today in Austin, TX includes partly cloudy skies during the day with a 10% chance of rain, and scattered thunderstorms at night with a 45% chance of rain. The temperature is expected to range between 73°F and 89°F today.\n",
       "\n",
       "Other reports show slightly different current temperatures, with one showing 72°F as of 11:29 PM CDT, feeling like 72°F. Another report from 8:00 AM shows the temperature at 22°C (71.6°F) and at 9:00 AM at 24°C (75.2°F)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grounding_chunks=[GroundingChunk(retrieved_context=None, web=GroundingChunkWeb(domain='google.com', title='Weather information for locality: Austin, administrative_area: TX', uri='https://www.google.com/search?q=weather+in+Austin,+TX')), GroundingChunk(retrieved_context=None, web=GroundingChunkWeb(domain='fox7austin.com', title='fox7austin.com', uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AbF9wXGFiUX3p2br0iUiGG9gAcX9gyP3J1wsOub5_B7wpXFopULnCXLWt8Zg7brzBt1FxtcUmuAJ2yhb72oY-7QDodyU11KGIrm-cu7CCvhj1-8aymCWEUUPESpgpHZXQQ==')), GroundingChunk(retrieved_context=None, web=GroundingChunkWeb(domain='bbc.com', title='bbc.com', uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AbF9wXH2BxQvnC00DTkfBG4a9KCADIhNeFkIOaSaYib8bWaRSEfUNFFfQmqJhSjTV33GpFXMjoHa1s7TiFlwhVCdvkUoESMwDpcoA2NonwaF7DhrPHRow1eD6aGiYcJ_Hyg='))] grounding_supports=[GroundingSupport(confidence_scores=[1.0], grounding_chunk_indices=[0], segment=Segment(end_index=74, part_index=None, start_index=None, text='As of 8:54 AM CDT, the current temperature in Austin, TX is 73°F (23°C).')), GroundingSupport(confidence_scores=[1.0], grounding_chunk_indices=[0], segment=Segment(end_index=103, part_index=None, start_index=75, text='It feels like 77°F (25°C).')), GroundingSupport(confidence_scores=[1.0], grounding_chunk_indices=[0], segment=Segment(end_index=131, part_index=None, start_index=104, text='The humidity is around 86%.')), GroundingSupport(confidence_scores=[1.0], grounding_chunk_indices=[0], segment=Segment(end_index=304, part_index=None, start_index=133, text='The forecast for today in Austin, TX includes partly cloudy skies during the day with a 10% chance of rain, and scattered thunderstorms at night with a 45% chance of rain.')), GroundingSupport(confidence_scores=[1.0], grounding_chunk_indices=[0], segment=Segment(end_index=372, part_index=None, start_index=305, text='The temperature is expected to range between 73°F and 89°F today.')), GroundingSupport(confidence_scores=[1.0], grounding_chunk_indices=[1], segment=Segment(end_index=496, part_index=None, start_index=374, text='Other reports show slightly different current temperatures, with one showing 72°F as of 11:29 PM CDT, feeling like 72°F.')), GroundingSupport(confidence_scores=[1.0], grounding_chunk_indices=[2], segment=Segment(end_index=600, part_index=None, start_index=497, text='Another report from 8:00 AM shows the temperature at 22°C (71.6°F) and at 9:00 AM at 24°C (75.2°F).'))] retrieval_metadata=RetrievalMetadata(google_search_dynamic_retrieval_score=None) retrieval_queries=None search_entry_point=SearchEntryPoint(rendered_content='<style>\\n.container {\\n  align-items: center;\\n  border-radius: 8px;\\n  display: flex;\\n  font-family: Google Sans, Roboto, sans-serif;\\n  font-size: 14px;\\n  line-height: 20px;\\n  padding: 8px 12px;\\n}\\n.chip {\\n  display: inline-block;\\n  border: solid 1px;\\n  border-radius: 16px;\\n  min-width: 14px;\\n  padding: 5px 16px;\\n  text-align: center;\\n  user-select: none;\\n  margin: 0 8px;\\n  -webkit-tap-highlight-color: transparent;\\n}\\n.carousel {\\n  overflow: auto;\\n  scrollbar-width: none;\\n  white-space: nowrap;\\n  margin-right: -12px;\\n}\\n.headline {\\n  display: flex;\\n  margin-right: 4px;\\n}\\n.gradient-container {\\n  position: relative;\\n}\\n.gradient {\\n  position: absolute;\\n  transform: translate(3px, -9px);\\n  height: 36px;\\n  width: 9px;\\n}\\n@media (prefers-color-scheme: light) {\\n  .container {\\n    background-color: #fafafa;\\n    box-shadow: 0 0 0 1px #0000000f;\\n  }\\n  .headline-label {\\n    color: #1f1f1f;\\n  }\\n  .chip {\\n    background-color: #ffffff;\\n    border-color: #d2d2d2;\\n    color: #5e5e5e;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:focus {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:active {\\n    background-color: #d8d8d8;\\n    border-color: #b6b6b6;\\n  }\\n  .logo-dark {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\\n  }\\n}\\n@media (prefers-color-scheme: dark) {\\n  .container {\\n    background-color: #1f1f1f;\\n    box-shadow: 0 0 0 1px #ffffff26;\\n  }\\n  .headline-label {\\n    color: #fff;\\n  }\\n  .chip {\\n    background-color: #2c2c2c;\\n    border-color: #3c4043;\\n    color: #fff;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #353536;\\n  }\\n  .chip:focus {\\n    background-color: #353536;\\n  }\\n  .chip:active {\\n    background-color: #464849;\\n    border-color: #53575b;\\n  }\\n  .logo-light {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\\n  }\\n}\\n</style>\\n<div class=\"container\">\\n  <div class=\"headline\">\\n    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\\n    </svg>\\n    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\\n      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\\n      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\\n      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\\n      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\\n      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\\n    </svg>\\n    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\\n  </div>\\n  <div class=\"carousel\">\\n    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AbF9wXHS16ZHRMBWB1fliaPxBa_BZ2ldrGOZhwnt6A4mRybC0aPCIN3zNfiSQaMeUkzmswjpfgXpNEsYDaUiaC4bMG0hDaburnEOFOIqGzz2cbPh0GMQfD-_n1FjX2F4pekNB6nR0IiJBX4zwUKgmXE8rr34Ltk9JFfaTOu6NnX6GKVAX8Iefl1oJNn2745H9OBW4-54GBNaq7fk-6JjhlzLIeeB\">current temperature Austin, TX</a>\\n  </div>\\n</div>\\n', sdk_blob=None) web_search_queries=['current temperature Austin, TX']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".container {\n",
       "  align-items: center;\n",
       "  border-radius: 8px;\n",
       "  display: flex;\n",
       "  font-family: Google Sans, Roboto, sans-serif;\n",
       "  font-size: 14px;\n",
       "  line-height: 20px;\n",
       "  padding: 8px 12px;\n",
       "}\n",
       ".chip {\n",
       "  display: inline-block;\n",
       "  border: solid 1px;\n",
       "  border-radius: 16px;\n",
       "  min-width: 14px;\n",
       "  padding: 5px 16px;\n",
       "  text-align: center;\n",
       "  user-select: none;\n",
       "  margin: 0 8px;\n",
       "  -webkit-tap-highlight-color: transparent;\n",
       "}\n",
       ".carousel {\n",
       "  overflow: auto;\n",
       "  scrollbar-width: none;\n",
       "  white-space: nowrap;\n",
       "  margin-right: -12px;\n",
       "}\n",
       ".headline {\n",
       "  display: flex;\n",
       "  margin-right: 4px;\n",
       "}\n",
       ".gradient-container {\n",
       "  position: relative;\n",
       "}\n",
       ".gradient {\n",
       "  position: absolute;\n",
       "  transform: translate(3px, -9px);\n",
       "  height: 36px;\n",
       "  width: 9px;\n",
       "}\n",
       "@media (prefers-color-scheme: light) {\n",
       "  .container {\n",
       "    background-color: #fafafa;\n",
       "    box-shadow: 0 0 0 1px #0000000f;\n",
       "  }\n",
       "  .headline-label {\n",
       "    color: #1f1f1f;\n",
       "  }\n",
       "  .chip {\n",
       "    background-color: #ffffff;\n",
       "    border-color: #d2d2d2;\n",
       "    color: #5e5e5e;\n",
       "    text-decoration: none;\n",
       "  }\n",
       "  .chip:hover {\n",
       "    background-color: #f2f2f2;\n",
       "  }\n",
       "  .chip:focus {\n",
       "    background-color: #f2f2f2;\n",
       "  }\n",
       "  .chip:active {\n",
       "    background-color: #d8d8d8;\n",
       "    border-color: #b6b6b6;\n",
       "  }\n",
       "  .logo-dark {\n",
       "    display: none;\n",
       "  }\n",
       "  .gradient {\n",
       "    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
       "  }\n",
       "}\n",
       "@media (prefers-color-scheme: dark) {\n",
       "  .container {\n",
       "    background-color: #1f1f1f;\n",
       "    box-shadow: 0 0 0 1px #ffffff26;\n",
       "  }\n",
       "  .headline-label {\n",
       "    color: #fff;\n",
       "  }\n",
       "  .chip {\n",
       "    background-color: #2c2c2c;\n",
       "    border-color: #3c4043;\n",
       "    color: #fff;\n",
       "    text-decoration: none;\n",
       "  }\n",
       "  .chip:hover {\n",
       "    background-color: #353536;\n",
       "  }\n",
       "  .chip:focus {\n",
       "    background-color: #353536;\n",
       "  }\n",
       "  .chip:active {\n",
       "    background-color: #464849;\n",
       "    border-color: #53575b;\n",
       "  }\n",
       "  .logo-light {\n",
       "    display: none;\n",
       "  }\n",
       "  .gradient {\n",
       "    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
       "  }\n",
       "}\n",
       "</style>\n",
       "<div class=\"container\">\n",
       "  <div class=\"headline\">\n",
       "    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
       "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
       "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
       "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
       "    </svg>\n",
       "    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
       "      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
       "      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
       "      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
       "      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
       "    </svg>\n",
       "    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
       "  </div>\n",
       "  <div class=\"carousel\">\n",
       "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AbF9wXHS16ZHRMBWB1fliaPxBa_BZ2ldrGOZhwnt6A4mRybC0aPCIN3zNfiSQaMeUkzmswjpfgXpNEsYDaUiaC4bMG0hDaburnEOFOIqGzz2cbPh0GMQfD-_n1FjX2F4pekNB6nR0IiJBX4zwUKgmXE8rr34Ltk9JFfaTOu6NnX6GKVAX8Iefl1oJNn2745H9OBW4-54GBNaq7fk-6JjhlzLIeeB\">current temperature Austin, TX</a>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_search_tool = Tool(google_search=GoogleSearch())\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What is the current temperature in Austin, TX?\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[google_search_tool],\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))\n",
    "\n",
    "print(response.candidates[0].grounding_metadata)\n",
    "\n",
    "HTML(response.candidates[0].grounding_metadata.search_entry_point.rendered_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0pb-Kh1xEHU"
   },
   "source": [
    "## Function calling\n",
    "\n",
    "[Function Calling](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling) in Gemini lets developers create a description of a function in their code, then pass that description to a language model in a request.\n",
    "\n",
    "You can submit a Python function for automatic function calling, which will run the function and return the output in natural language generated by Gemini.\n",
    "\n",
    "You can also submit an [OpenAPI Specification](https://www.openapis.org/) which will respond with the name of a function that matches the description and the arguments to call it with.\n",
    "\n",
    "For more examples of Function calling with Gemini, check out this notebook: [Intro to Function Calling with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSUWWlrrlR-D"
   },
   "source": [
    "### Python Function (Automatic Function Calling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "aRR8HZhLlR-E",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The weather in San Francisco is foggy."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_current_weather(location: str) -> str:\n",
    "    \"\"\"Example method. Returns the current weather.\n",
    "\n",
    "    Args:\n",
    "        location: The city and state, e.g. San Francisco, CA\n",
    "    \"\"\"\n",
    "    weather_map: dict[str, str] = {\n",
    "        \"Boston, MA\": \"snowing\",\n",
    "        \"San Francisco, CA\": \"foggy\",\n",
    "        \"Seattle, WA\": \"raining\",\n",
    "        \"Austin, TX\": \"hot\",\n",
    "        \"Chicago, IL\": \"windy\",\n",
    "    }\n",
    "    return weather_map.get(location, \"unknown\")\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What is the weather like in San Francisco?\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[get_current_weather],\n",
    "        temperature=0,\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4syyLEClGcn"
   },
   "source": [
    "### OpenAPI Specification (Manual Function Calling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "2BDQPwgcxRN3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=None args={'destination': 'Paris'} name='get_destination'\n"
     ]
    }
   ],
   "source": [
    "get_destination = FunctionDeclaration(\n",
    "    name=\"get_destination\",\n",
    "    description=\"Get the destination that the user wants to go to\",\n",
    "    parameters={\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"destination\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"Destination that the user wants to go to\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "destination_tool = Tool(\n",
    "    function_declarations=[get_destination],\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"I'd like to travel to Paris.\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[destination_tool],\n",
    "        temperature=0,\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.function_calls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhDs2X3o0neK"
   },
   "source": [
    "## Code Execution\n",
    "\n",
    "The Gemini API [code execution](https://ai.google.dev/gemini-api/docs/code-execution?lang=python) feature enables the model to generate and run Python code and learn iteratively from the results until it arrives at a final output. You can use this code execution capability to build applications that benefit from code-based reasoning and that produce text output. For example, you could use code execution in an application that solves equations or processes text.\n",
    "\n",
    "The Gemini API provides code execution as a tool, similar to function calling.\n",
    "After you add code execution as a tool, the model decides when to use it.\n",
    "\n",
    "For more examples of Code Execution, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/code-execution/intro_code_execution.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "1W-3c7sy0nyz",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Code\n",
       "\n",
       "```py\n",
       "def fibonacci(n):\n",
       "    if n <= 1:\n",
       "        return n\n",
       "    else:\n",
       "        a, b = 0, 1\n",
       "        for _ in range(2, n + 1):\n",
       "            a, b = b, a + b\n",
       "        return b\n",
       "\n",
       "fib_20 = fibonacci(20)\n",
       "print(f'{fib_20=}')\n",
       "\n",
       "```\n",
       "\n",
       "### Output\n",
       "\n",
       "```\n",
       "fib_20=6765\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "code_execution_tool = Tool(code_execution=ToolCodeExecution())\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Calculate 20th fibonacci number. Then find the nearest palindrome to it.\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[code_execution_tool],\n",
    "        temperature=0,\n",
    "        thinking_config=thinking_config,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"\"\"\n",
    "## Code\n",
    "\n",
    "```py\n",
    "{response.executable_code}\n",
    "```\n",
    "\n",
    "### Output\n",
    "\n",
    "```\n",
    "{response.code_execution_result}\n",
    "```\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5b5adb2eb70"
   },
   "source": [
    "## Thinking model examples\n",
    "\n",
    "The following examples are some complex tasks that require multiple rounds of strategizing and iteratively solving.\n",
    "\n",
    "### **Example 1:** Code generation\n",
    "\n",
    "Gemini 2.5 Flash excels at creating visually compelling web apps and agentic code applications, along with code transformation and editing.\n",
    "\n",
    "Let's see how the model uses its reasoning capabilities to create a video game, using executable code from a single line prompt. See the example game [here](https://www.youtube.com/watch?v=RLCBSpgos6s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "5f120dff0d16",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Okay, here is a captivating pixelated endless runner game built entirely in p5.js, with key instructions on screen. It features a jumping dinosaur, scrolling pixel art backgrounds, and random obstacles.\n",
       "\n",
       "**Instructions:**\n",
       "\n",
       "1.  Save the code below as a single file named `sketch.js` (or similar).\n",
       "2.  Create a folder named `assets` in the same directory as `sketch.js`.\n",
       "3.  Find or create some pixel art images and save them in the `assets` folder. You'll need:\n",
       "    *   `dino.png` (the player)\n",
       "    *   `cactus.png` (an obstacle)\n",
       "    *   `ptero.png` (another obstacle, optional but good)\n",
       "    *   `bg_layer_1.png` (slow scrolling background)\n",
       "    *   `bg_layer_2.png` (faster scrolling background)\n",
       "4.  Open the `index.html` file (which you'll need to create if you don't have one - standard p5.js template is fine) in a browser, or use a local server.\n",
       "\n",
       "*(Note: I've included placeholder image file names. You *must* replace these with the actual file names you use in your `assets` folder).*\n",
       "\n",
       "```javascript\n",
       "// Required to run p5.js sketch without separate index.html (useful for some online editors, but standard practice is index.html)\n",
       "// To run this locally, you'll typically still need a basic index.html linking to p5.js and this sketch.js file.\n",
       "// Example minimal index.html:\n",
       "/*\n",
       "<!DOCTYPE html>\n",
       "<html>\n",
       "<head>\n",
       "  <script src=\"https://cdn.jsdelivr.net/npm/p5@1.4.0/lib/p5.js\"></script>\n",
       "  <script src=\"sketch.js\"></script>\n",
       "</head>\n",
       "<body>\n",
       "</body>\n",
       "</html>\n",
       "*/\n",
       "\n",
       "\n",
       "// --- Game State ---\n",
       "let gameState = 'PLAYING'; // States: 'PLAYING', 'GAME_OVER'\n",
       "\n",
       "// --- Player Variables ---\n",
       "let dino;\n",
       "let dinoImg;\n",
       "const GRAVITY = 0.8;\n",
       "const JUMP_FORCE = -15;\n",
       "const GROUND_Y = 350; // Y coordinate for the ground level\n",
       "\n",
       "// --- Obstacle Variables ---\n",
       "let obstacles = [];\n",
       "let cactusImg;\n",
       "let pteroImg;\n",
       "let obstacleImages = []; // Array to hold potential obstacle images\n",
       "let obstacleSpeed = 6; // Initial speed of obstacles\n",
       "let obstacleSpawnRate = 90; // Frames between attempts to spawn an obstacle (lower is more frequent)\n",
       "\n",
       "// --- Background Variables ---\n",
       "let bgLayers = [];\n",
       "let bgImg1, bgImg2;\n",
       "const BG_SPEED_1 = 0.5; // Speed for the slowest background layer\n",
       "const BG_SPEED_2 = 1; // Speed for the faster background layer\n",
       "\n",
       "\n",
       "// --- Score Variables ---\n",
       "let score = 0;\n",
       "let highScore = 0; // Optional: Track high score\n",
       "let scoreMultiplier = 0.1; // How much to increment score per frame\n",
       "\n",
       "// --- UI Variables ---\n",
       "let pixelFont; // Optional: Load a pixel font\n",
       "\n",
       "\n",
       "// --- Preload: Load images and font ---\n",
       "function preload() {\n",
       "  // IMPORTANT: Replace these paths with the actual paths to your images\n",
       "  // They should be in a folder named 'assets' relative to your sketch.js\n",
       "  dinoImg = loadImage('assets/dino.png');\n",
       "  cactusImg = loadImage('assets/cactus.png');\n",
       "  pteroImg = loadImage('assets/ptero.png'); // Assuming you have a pterodactyl image\n",
       "\n",
       "  bgImg1 = loadImage('assets/bg_layer_1.png');\n",
       "  bgImg2 = loadImage('assets/bg_layer_2.png');\n",
       "\n",
       "  // Add obstacle images to the array for random selection\n",
       "  obstacleImages.push(cactusImg);\n",
       "  if (pteroImg) obstacleImages.push(pteroImg); // Only add if pterodactyl image loaded successfully\n",
       "\n",
       "  // Optional: Load a pixel font\n",
       "  // pixelFont = loadFont('assets/pixel_font.ttf'); // Replace with your font file if you have one\n",
       "}\n",
       "\n",
       "// --- Setup: Initialize game elements ---\n",
       "function setup() {\n",
       "  createCanvas(600, 400); // Standard runner game size\n",
       "  noSmooth(); // Disable anti-aliasing for pixelated look\n",
       "\n",
       "  // Initialize background layers\n",
       "  bgLayers.push(new BackgroundLayer(bgImg1, BG_SPEED_1));\n",
       "  bgLayers.push(new BackgroundLayer(bgImg2, BG_SPEED_2));\n",
       "\n",
       "  // Create the player dinosaur\n",
       "  dino = new Dinosaur(100, GROUND_Y, dinoImg);\n",
       "\n",
       "  // Optional: Set font if loaded\n",
       "  // if (pixelFont) {\n",
       "  //   textFont(pixelFont);\n",
       "  // }\n",
       "  textSize(16);\n",
       "  textAlign(CENTER, CENTER);\n",
       "}\n",
       "\n",
       "// --- Draw: Main game loop ---\n",
       "function draw() {\n",
       "  background(200, 230, 255); // Sky color\n",
       "\n",
       "  // Draw and update background layers\n",
       "  for (let bg of bgLayers) {\n",
       "    bg.update();\n",
       "    bg.show();\n",
       "  }\n",
       "\n",
       "  // Draw the ground line\n",
       "  stroke(0);\n",
       "  strokeWeight(2);\n",
       "  line(0, GROUND_Y + dino.height / 2, width, GROUND_Y + dino.height / 2);\n",
       "  noStroke();\n",
       "\n",
       "\n",
       "  if (gameState === 'PLAYING') {\n",
       "    // --- Game Logic (Playing State) ---\n",
       "\n",
       "    // Update and show player\n",
       "    dino.update();\n",
       "    dino.show();\n",
       "\n",
       "    // Obstacle generation\n",
       "    if (frameCount % obstacleSpawnRate === 0) {\n",
       "      if (random(1) < 0.7) { // 70% chance to spawn an obstacle\n",
       "        let chosenImg = random(obstacleImages);\n",
       "        // Randomly choose y position for pterodactyl (flying) vs cactus (ground)\n",
       "        let spawnY = GROUND_Y - chosenImg.height / 2;\n",
       "        if (chosenImg === pteroImg) {\n",
       "            // Pterodactyl can spawn at different heights\n",
       "            spawnY = GROUND_Y - chosenImg.height - random(50, 120);\n",
       "        }\n",
       "\n",
       "        obstacles.push(new Obstacle(width, spawnY, chosenImg, obstacleSpeed));\n",
       "      }\n",
       "    }\n",
       "\n",
       "    // Update, show, and check collision for obstacles\n",
       "    for (let i = obstacles.length - 1; i >= 0; i--) {\n",
       "      obstacles[i].update();\n",
       "      obstacles[i].show();\n",
       "\n",
       "      // Check collision\n",
       "      if (dino.hits(obstacles[i])) {\n",
       "        console.log(\"Game Over!\");\n",
       "        gameState = 'GAME_OVER';\n",
       "      }\n",
       "\n",
       "      // Remove obstacle if off-screen\n",
       "      if (obstacles[i].isOffscreen()) {\n",
       "        obstacles.splice(i, 1);\n",
       "      }\n",
       "    }\n",
       "\n",
       "    // Increase score\n",
       "    score += scoreMultiplier;\n",
       "    // Increase difficulty slightly over time (increase speed, decrease spawn rate)\n",
       "    obstacleSpeed += 0.0005;\n",
       "    obstacleSpawnRate = max(50, obstacleSpawnRate - 0.005); // Don't let spawn rate go below 50 frames\n",
       "\n",
       "\n",
       "    // --- Display Score ---\n",
       "    fill(0);\n",
       "    textSize(24);\n",
       "    textAlign(LEFT, TOP);\n",
       "    text(\"Score: \" + floor(score), 20, 20);\n",
       "\n",
       "    // --- Display Instructions (Playing) ---\n",
       "    fill(0);\n",
       "    textSize(14);\n",
       "    textAlign(RIGHT, TOP);\n",
       "    text(\"Press SPACE to Jump\", width - 20, 20);\n",
       "\n",
       "\n",
       "  } else if (gameState === 'GAME_OVER') {\n",
       "    // --- Game Logic (Game Over State) ---\n",
       "\n",
       "    // Show elements in their final state\n",
       "    dino.show();\n",
       "    for (let obstacle of obstacles) {\n",
       "      obstacle.show();\n",
       "    }\n",
       "\n",
       "    // Display Game Over message and final score\n",
       "    fill(0);\n",
       "    textSize(32);\n",
       "    textAlign(CENTER, CENTER);\n",
       "    text(\"GAME OVER\", width / 2, height / 2 - 30);\n",
       "\n",
       "    textSize(24);\n",
       "    text(\"Final Score: \" + floor(score), width / 2, height / 2 + 10);\n",
       "\n",
       "    // Update high score if applicable\n",
       "    if (score > highScore) {\n",
       "        highScore = floor(score);\n",
       "    }\n",
       "    // Optional: Display High Score\n",
       "    textSize(18);\n",
       "    text(\"High Score: \" + highScore, width / 2, height / 2 + 40);\n",
       "\n",
       "\n",
       "    textSize(18);\n",
       "    text(\"Press SPACE to Restart\", width / 2, height / 2 + 80);\n",
       "  }\n",
       "}\n",
       "\n",
       "// --- Key Press Handler ---\n",
       "function keyPressed() {\n",
       "  if (keyCode === 32) { // Spacebar\n",
       "    if (gameState === 'PLAYING') {\n",
       "      dino.jump();\n",
       "    } else if (gameState === 'GAME_OVER') {\n",
       "      resetGame();\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "// --- Reset Game Function ---\n",
       "function resetGame() {\n",
       "  gameState = 'PLAYING';\n",
       "  score = 0;\n",
       "  obstacles = []; // Clear all obstacles\n",
       "  dino = new Dinosaur(100, GROUND_Y, dinoImg); // Reset player position\n",
       "  obstacleSpeed = 6; // Reset speed\n",
       "  obstacleSpawnRate = 90; // Reset spawn rate\n",
       "\n",
       "  // Reset background positions (optional, they loop anyway)\n",
       "  bgLayers[0].reset();\n",
       "  bgLayers[1].reset();\n",
       "}\n",
       "\n",
       "\n",
       "// --- Dinosaur Class ---\n",
       "class Dinosaur {\n",
       "  constructor(x, y, img) {\n",
       "    this.img = img;\n",
       "    // Adjust x, y to be the bottom-left of the image for easier ground alignment\n",
       "    this.x = x;\n",
       "    this.y = y - this.img.height / 2; // Center Y on the ground line\n",
       "    this.width = this.img.width;\n",
       "    this.height = this.img.height;\n",
       "\n",
       "    this.vy = 0; // Vertical velocity\n",
       "    this.isJumping = false; // Flag to prevent double jumping\n",
       "  }\n",
       "\n",
       "  jump() {\n",
       "    // Only allow jumping if on or near the ground\n",
       "    if (!this.isJumping) { // Check if not already jumping\n",
       "      this.vy = JUMP_FORCE;\n",
       "      this.isJumping = true; // Set jumping flag\n",
       "    }\n",
       "  }\n",
       "\n",
       "  update() {\n",
       "    // Apply gravity to vertical velocity\n",
       "    this.vy += GRAVITY;\n",
       "    // Apply velocity to position\n",
       "    this.y += this.vy;\n",
       "\n",
       "    // Prevent falling through the ground\n",
       "    let groundLevel = GROUND_Y - this.height / 2; // Y coordinate when dino is on ground\n",
       "    if (this.y >= groundLevel) {\n",
       "      this.y = groundLevel; // Snap to ground\n",
       "      this.vy = 0; // Stop vertical movement\n",
       "      this.isJumping = false; // Reset jumping flag when on ground\n",
       "    }\n",
       "  }\n",
       "\n",
       "  show() {\n",
       "    // Draw the dinosaur image\n",
       "    image(this.img, this.x - this.width/2, this.y - this.height/2, this.width, this.height);\n",
       "\n",
       "    // Optional: Draw bounding box for debugging collisions\n",
       "    // noFill();\n",
       "    // stroke(255, 0, 0);\n",
       "    // rect(this.x - this.width/2, this.y - this.height/2, this.width, this.height);\n",
       "  }\n",
       "\n",
       "  // Simple Axis-Aligned Bounding Box (AABB) collision check\n",
       "  hits(obstacle) {\n",
       "    // Get the bounding boxes\n",
       "    let dinoBox = {\n",
       "      left: this.x - this.width / 2,\n",
       "      right: this.x + this.width / 2,\n",
       "      top: this.y - this.height / 2,\n",
       "      bottom: this.y + this.height / 2\n",
       "    };\n",
       "\n",
       "    let obstacleBox = {\n",
       "      left: obstacle.x - obstacle.width / 2,\n",
       "      right: obstacle.x + obstacle.width / 2,\n",
       "      top: obstacle.y - obstacle.height / 2,\n",
       "      bottom: obstacle.y + obstacle.height / 2\n",
       "    };\n",
       "\n",
       "    // Check for overlap\n",
       "    return !(dinoBox.right < obstacleBox.left ||\n",
       "             dinoBox.left > obstacleBox.right ||\n",
       "             dinoBox.bottom < obstacleBox.top ||\n",
       "             dinoBox.top > obstacleBox.bottom);\n",
       "  }\n",
       "}\n",
       "\n",
       "\n",
       "// --- Obstacle Class ---\n",
       "class Obstacle {\n",
       "  constructor(x, y, img, speed) {\n",
       "    this.img = img;\n",
       "    // Adjust x, y to be the center of the image for easier positioning\n",
       "    this.x = x;\n",
       "    this.y = y;\n",
       "    this.width = this.img.width;\n",
       "    this.height = this.img.height;\n",
       "    this.speed = speed;\n",
       "  }\n",
       "\n",
       "  update() {\n",
       "    // Move obstacle to the left\n",
       "    this.x -= this.speed;\n",
       "  }\n",
       "\n",
       "  show() {\n",
       "    // Draw the obstacle image\n",
       "    image(this.img, this.x - this.width / 2, this.y - this.height / 2, this.width, this.height);\n",
       "\n",
       "     // Optional: Draw bounding box for debugging collisions\n",
       "    // noFill();\n",
       "    // stroke(0, 255, 0);\n",
       "    // rect(this.x - this.width/2, this.y - this.height/2, this.width, this.height);\n",
       "  }\n",
       "\n",
       "  isOffscreen() {\n",
       "    // Check if the obstacle is completely off the left side of the screen\n",
       "    return this.x + this.width / 2 < 0;\n",
       "  }\n",
       "}\n",
       "\n",
       "// --- Background Layer Class (for scrolling backgrounds) ---\n",
       "class BackgroundLayer {\n",
       "    constructor(img, speed) {\n",
       "        this.img = img;\n",
       "        this.speed = speed;\n",
       "        this.x1 = 0; // X position for the first image copy\n",
       "        this.x2 = width; // X position for the second image copy\n",
       "        // Scale the image to fit the canvas height while maintaining aspect ratio\n",
       "        this.imgWidth = (this.img.width / this.img.height) * height;\n",
       "        this.imgHeight = height;\n",
       "\n",
       "        // Ensure the second image starts exactly where the first one ends\n",
       "         this.x2 = this.imgWidth;\n",
       "    }\n",
       "\n",
       "    update() {\n",
       "        // Move both copies to the left\n",
       "        this.x1 -= this.speed;\n",
       "        this.x2 -= this.speed;\n",
       "\n",
       "        // If the first image goes off-screen, reset its position to after the second image\n",
       "        if (this.x1 < -this.imgWidth) {\n",
       "            this.x1 = this.x2 + this.imgWidth;\n",
       "        }\n",
       "        // If the second image goes off-screen, reset its position to after the first image\n",
       "        if (this.x2 < -this.imgWidth) {\n",
       "             this.x2 = this.x1 + this.imgWidth;\n",
       "        }\n",
       "    }\n",
       "\n",
       "    show() {\n",
       "        // Draw both copies of the background image\n",
       "        image(this.img, this.x1, 0, this.imgWidth, this.imgHeight);\n",
       "        image(this.img, this.x2, 0, this.imgWidth, this.imgHeight);\n",
       "    }\n",
       "\n",
       "    reset() {\n",
       "        this.x1 = 0;\n",
       "        this.x2 = this.imgWidth;\n",
       "    }\n",
       "}\n",
       "```\n",
       "\n",
       "**Explanation:**\n",
       "\n",
       "1.  **Structure:** The code is a single p5.js file (`sketch.js`). It includes comments indicating where p5.js is linked (though you'll typically do this in an `index.html`).\n",
       "2.  **Game State:** The `gameState` variable (`PLAYING`, `GAME_OVER`) controls the flow. `draw()` behaves differently based on this state.\n",
       "3.  **Preload:** `preload()` is crucial for loading external assets like images *before* the sketch starts. Remember to place your images in an `assets` folder and use the correct paths.\n",
       "4.  **Setup:** `setup()` creates the canvas, disables smoothing (`noSmooth()`) for that crisp pixel look, initializes the background layers, and creates the player `Dinosaur` object.\n",
       "5.  **Draw Loop:** `draw()` is the heart of the game.\n",
       "    *   It clears the background.\n",
       "    *   It updates and displays the scrolling background layers.\n",
       "    *   It draws a simple ground line.\n",
       "    *   Inside the `if (gameState === 'PLAYING')` block:\n",
       "        *   The player's position is updated based on gravity (`dino.update()`).\n",
       "        *   The player is drawn (`dino.show()`).\n",
       "        *   New obstacles are periodically generated based on `frameCount` and `obstacleSpawnRate`.\n",
       "        *   Existing obstacles are updated (moved left), drawn, and checked for collision with the player.\n",
       "        *   If a collision occurs (`dino.hits(obstacle)`), the game state changes to `GAME_OVER`.\n",
       "        *   Obstacles that move off-screen are removed from the `obstacles` array using `splice()` to keep the game running efficiently.\n",
       "        *   The score is incremented.\n",
       "        *   Game speed (`obstacleSpeed`) and spawn rate are slightly adjusted for increasing difficulty.\n",
       "        *   The score and instructions are displayed.\n",
       "    *   Inside the `else if (gameState === 'GAME_OVER')` block:\n",
       "        *   Elements are drawn in their final positions.\n",
       "        *   \"GAME OVER\", the final score, and restart instructions are displayed.\n",
       "6.  **Classes:** `Dinosaur`, `Obstacle`, and `BackgroundLayer` classes are used to organize the properties (position, size, image, velocity) and behavior (update, show, collision, off-screen check) of game objects.\n",
       "7.  **Dinosaur:** Handles jumping using vertical velocity (`vy`) and gravity (`GRAVITY`). `isJumping` prevents spamming the jump key mid-air. `update` applies gravity and limits the fall at the ground. `hits` checks for collision.\n",
       "8.  **Obstacle:** Handles moving horizontally. `isOffscreen` checks if it needs to be removed. The constructor takes an image and speed.\n",
       "9.  **BackgroundLayer:** This class manages two copies of a background image side-by-side. It moves them both left (`update`) and when one goes off-screen, it teleports it back to the right side to create a seamless, endless scroll.\n",
       "10. **Collision:** A simple AABB (Axis-Aligned Bounding Box) collision detection is used in the `dino.hits()` method. It checks if the rectangular areas of the dino and an obstacle overlap.\n",
       "11. **Input:** `keyPressed()` listens for key presses. If the SPACE key (keyCode 32) is pressed, it either makes the dino jump (if playing) or calls `resetGame()` (if game over).\n",
       "12. **Reset Game:** `resetGame()` resets variables, clears the obstacle list, creates a new dinosaur, and sets the state back to `PLAYING`.\n",
       "13. **Instructions:** `text()` is used in the `draw` loop to display instructions and game state information.\n",
       "\n",
       "Remember to create the `assets` folder and place your pixel art images there with the specified names!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "  Make me a captivating endless runner game. Key instructions on the screen. p5js scene, no HTML.\n",
    "  I like pixelated dinosaurs and interesting backgrounds.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecf22b47bdc3"
   },
   "source": [
    "### **Example 2:** Multimodal reasoning (Geometry)\n",
    "\n",
    "This geometry problem requires complex reasoning and is also using multimodal capabilities to reason across text and image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "60260c0ac118",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://storage.googleapis.com/generativeai-downloads/images/geometry.png\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_file_url = (\n",
    "    \"https://storage.googleapis.com/generativeai-downloads/images/geometry.png\"\n",
    ")\n",
    "display(Image(url=image_file_url, width=400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c972334f62ff",
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(file_uri=image_file_url, mime_type=\"image/png\"),\n",
    "        \"What's the area of the overlapping region?\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52656e92cd69"
   },
   "source": [
    "### **Example 3**:  Math and problem solving\n",
    "\n",
    "Here's another brain teaser based on an image, this time it looks like a mathematical problem, but it cannot actually be solved mathematically. If you check the thoughts of the model you'll see that it will realize it and come up with an out-of-the-box solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d46387bdc9e6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_file_url = \"https://storage.googleapis.com/generativeai-downloads/images/pool.png\"\n",
    "display(Image(url=image_file_url, width=400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46b694793eb0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(file_uri=image_file_url, mime_type=\"image/png\"),\n",
    "        \"How do I use three of the pool balls to sum up to 30?\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQwiONFdVHw5"
   },
   "source": [
    "## What's next\n",
    "\n",
    "- See the [Google Gen AI SDK reference docs](https://googleapis.github.io/python-genai/).\n",
    "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
    "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "rZV2TY5Pa3Dd",
    "mSUWWlrrlR-D",
    "h4syyLEClGcn"
   ],
   "name": "intro_gemini_2_5_flash.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
