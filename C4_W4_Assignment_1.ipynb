{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project Part 1 - ETL and Data Modeling\n",
    "\n",
    "During this capstone project, you will develop a data pipeline as part of a new project in the company DeFtunes. You will showcase the abilities and tools you have been using during the whole specialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [ 1 - Introduction](#1)\n",
    "- [ 2 - Data Sources](#2)\n",
    "- [ 3 - Exploratory Data Analysis](#3)\n",
    "- [ 4 - ETL Pipeline with AWS Glue and Terraform](#4)\n",
    "  - [ 4.1 - Landing Zone](#4-1)\n",
    "  - [ 4.2 - Transformation Zone](#4-2)\n",
    "  - [ 4.3 - Serving Zone](#4-3)\n",
    "- [ 5 - Data Modeling with dbt and Redshift Spectrum](#5)\n",
    "  - [ 5.1 - Redshift Setup](#5-1)\n",
    "  - [ 5.2 - Redshift Test](#5-2)\n",
    "  - [ 5.3 - dbt Setup](#5-3)\n",
    "  - [ 5.4 - Data Modeling](#5-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1 - Introduction\n",
    "\n",
    "DeFtunes is a new company in the music industry, offering a subscription-based app for streaming songs. Recently, they have expanded their services to include digital song purchases. With this new retail feature, DeFtunes requires a data pipeline to extract purchase data from their new API and operational database, enrich and model this data, and ultimately deliver the comprehensive data model for the Data Analysis team to review and gain insights. Your task is to develop this pipeline, ensuring the data is accurately processed and ready for in-depth analysis.\n",
    "\n",
    "Here is the diagram with the main requirements for this project:\n",
    "\n",
    "![Capstone_Diagram](images/Capstone-diagram.png)\n",
    "\n",
    "1. The pipeline has to follow a medallion architecture with a landing, transform and serving zone.\n",
    "2. The data generated in the pipeline will be stored in the company's data lake, in this case, an S3 bucket.\n",
    "3. The silver layer should use Iceberg tables, and the gold layer should be inside Redshift.\n",
    "4. The pipeline should be reproducible, you will have to implement it using Terraform.\n",
    "5. The data should be modelled into a star schema in the serving layer, you should use dbt for the modelling part.\n",
    "\n",
    "Before starting, you will need to import some required libraries and modules for the capstone development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the terminal run the following command to set up the environment:\n",
    "\n",
    "```bash\n",
    "source scripts/setup.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2 - Data Sources\n",
    "\n",
    "The first data source you will be using is the DeFtunes operational database, which is running in RDS with a Postgres engine. This database contains a table with all the relevant information for the available songs that you can purchase. Let's connect to the table using the `%sql` magic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1. Run the following code to get the link to the AWS console.\n",
    "\n",
    "*Note*: For security reasons, the URL to access the AWS console will expire every 15 minutes, but any AWS resources you created will remain available for the 2 hour period. If you need to access the console after 15 minutes, please rerun this code cell to obtain a new active link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://signin.aws.amazon.com/federation?Action=login&SigninToken=GLVCmtCol5qw7wCOlRi9UNBGoLIPRnRWubDL2F5MNdK7GIVj0QQcvDPHB9Xw039A41EVaYvSHWk9dq57OMwAgymn-qq0TeEn5ncmItSkP72-ej-8oNgs4i6igaf1FZomc81raBdIf6yC5tOYT9irPvDBcOAw2lkMyIy4BIyyyzHUK911-FzTju15uKsnLw1E3c_whGV0JGVCsk0E0UmNxQs1K_V4dg1DkPKIvmsDYDDuQYpWgeM9-4U4MCqK202aDsQ39-2Wmvc290-C3bfHSxcmTja5ASqs5F4aI4w7RSEKC4f7bjNamhRLqe80OKY0ZdewcQfBW76cdSvW2VpXWJsA9dD7KEv3jgljLuPlF5UH-a3dEac7BPGoiTfCFrT8CE6Hu0xfm2hDSGvIS_ROLP2bdz2Q5dc4QSxwmKDRuDWqjbTter_ggVjCDuInzxlLnqEhMoE0ISJDLknkGZ-B8HOTUUQCv3isw0nTj8K7CWrCToliX_ue2E9-QEE7D75RmRDAxD9X8zH_jviKeUcmKsBBLZ-27D8bAonrGUWzJkES7Xwd_aLXtcx-LqVPue65wdbaKXKRl5tKeEncMGbDg9vzlAxol_iE8sQcy4FlYwOhzrOXOVFM45GYry7MHUR5De8tLtr-Fx6wE6FBaE9T-tQyR_yzPkH2hdHbEKh1AFoUNFdk_QLtiasP0vu7OT2u19pNTefwlNcIIcfMULf9cRjGkrpWw3mpkqvLjuHc7eu9R4KT5krMOJduQXxu_ZdWWFPrHxKEg5aRZhJ6EJC7mOXt_8Cfr66YTb_pI0XQNe_TZccs6R8B8hGlclaUrmEtLwyPl5C72KVZptRriJMAaMEGScDsYDqoWhsUWEzbIc83_7OcltBmyU-4qTkc4pPdN4zvnhpAgis7jtQFyNMNw1guBxcGwapj3UdRLZcgiKWcT1yNyT2x5s7Bkw89wN_vWkeiC-pzZiuVRONjMD8UyTlZUNVr_HAP4mttGjiq6KtPiZcmhyuzM92P3BEzTDJ3xEpcVTOyWWJiXIm4RzMq52fBXxkiVMiu8qCN3BDMECAzHS3ls1XHvvJ7__WfuywSd7BmpZovgdJW7ek1MU0lgUQ5-gOU6SYtcDwT1YJX5_RTFs7yDNesluJ-tJNW5nInZk3RIIShoO2ySFttRjatQMRN74M-Na-DR2aUyLMhQ_R1y908iDK_dgBi182SQZCwS9-gq9lNA0SkmGaNh8GY-3LFD7L7XKisr-F0mFOCj2N4wGgk7MXf2eVaVXNrpBDGBb7ltbBKEZ6oQeNYtXREJ6UG93fgSuVHdHNLaVrvK4LO4oPsT4URBaNRnioi2AbHEyH1To584OhSIpi7lOcrs4_YFWzlebA4QMUhw89FFVCtIirX1HTJ_hDbqlo93fKNYUze-QFO23BtDKdJWMgBIbXCzaNFrzU7pbX8FqYXUcsHOyiYj_yI4g&Issuer=https%3A%2F%2Fapi.vocareum.com&Destination=https%3A%2F%2Fconsole.aws.amazon.com%2Fconsole%2Fhome%3Fregion%3Dus-east-1\" target=\"_blank\">GO TO AWS CONSOLE</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../.aws/aws_console_url', 'r') as file:\n",
    "    aws_url = file.read().strip()\n",
    "\n",
    "HTML(f'<a href=\"{aws_url}\" target=\"_blank\">GO TO AWS CONSOLE</a>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* If you see the window like in the following printscreen, click on **logout** link, close the window and click on console link again.\n",
    "\n",
    "![AWSLogout](images/AWSLogout.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to **CloudFormation** in the AWS console. Click on the alphanumeric stack name and search for the **Outputs** tab. You will see the keys `PostgresEndpoint` and `ScriptsBucket`, copy the corresponding **Values** and replace the placeholders in the cells below (please, replace the whole placeholder including the brackets `<>`). Then run each cell code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPTS_BUCKET_NAME = 'de-c4w4a1-800994367402-us-east-1-scripts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDSDBHOST = 'de-c4w4a1-rds.c58i0c4u0qe2.us-east-1.rds.amazonaws.com'\n",
    "RDSDBPORT = '5432'\n",
    "RDSDBNAME = 'postgres'\n",
    "RDSDBUSER = 'postgresuser'\n",
    "RDSDBPASSWORD = 'adminpwrd'\n",
    "\n",
    "postgres_connection_url = f'postgresql+psycopg2://{RDSDBUSER}:{RDSDBPASSWORD}@{RDSDBHOST}:{RDSDBPORT}/{RDSDBNAME}'\n",
    "%sql {postgres_connection_url}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Test that the connection works by running the following query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql+psycopg2://postgresuser:***@de-c4w4a1-rds.c58i0c4u0qe2.us-east-1.rds.amazonaws.com:5432/postgres\n",
      "6 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>schema_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>aws_s3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>aws_commons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>deftunes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>information_schema</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>pg_catalog</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('public',),\n",
       " ('aws_s3',),\n",
       " ('aws_commons',),\n",
       " ('deftunes',),\n",
       " ('information_schema',),\n",
       " ('pg_catalog',)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT schema_name\n",
    "FROM information_schema.schemata;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3. Inside the `deftunes` schema there is a table `songs` which was mentioned before. Let's query a sample from it:\n",
    "\n",
    "*Note:* The `songs` table is based on the Million Song Dataset, more information can be found [here](http://millionsongdataset.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql+psycopg2://postgresuser:***@de-c4w4a1-rds.c58i0c4u0qe2.us-east-1.rds.amazonaws.com:5432/postgres\n",
      "5 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>track_id</th>\n",
       "        <th>title</th>\n",
       "        <th>song_id</th>\n",
       "        <th>release</th>\n",
       "        <th>artist_id</th>\n",
       "        <th>artist_mbid</th>\n",
       "        <th>artist_name</th>\n",
       "        <th>duration</th>\n",
       "        <th>artist_familiarity</th>\n",
       "        <th>artist_hotttnesss</th>\n",
       "        <th>year</th>\n",
       "        <th>track_7digitalid</th>\n",
       "        <th>shs_perf</th>\n",
       "        <th>shs_work</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>TRMBFJC12903CE2A89</td>\n",
       "        <td>Mystery Babylon</td>\n",
       "        <td>SOGOPFY12AB018E5B1</td>\n",
       "        <td>Healing of All Nations</td>\n",
       "        <td>ARP06GY1187B98B0F0</td>\n",
       "        <td>106e0414-95a7-45e9-8176-bbc938deed68</td>\n",
       "        <td>Yami Bolo</td>\n",
       "        <td>146.49425</td>\n",
       "        <td>0.4955022</td>\n",
       "        <td>0.3224826</td>\n",
       "        <td>2001</td>\n",
       "        <td>8562838</td>\n",
       "        <td>-1</td>\n",
       "        <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>TRMBFTN128F42665EA</td>\n",
       "        <td>Världen É Din</td>\n",
       "        <td>SOKFZOR12A8C1306B0</td>\n",
       "        <td>Omérta</td>\n",
       "        <td>ARZ6UKQ1187B9B0E35</td>\n",
       "        <td>a432b2e7-7598-419b-8760-e8accff3c725</td>\n",
       "        <td>The Latin Kings</td>\n",
       "        <td>268.22485</td>\n",
       "        <td>0.54002666</td>\n",
       "        <td>0.42142987</td>\n",
       "        <td>0</td>\n",
       "        <td>3164205</td>\n",
       "        <td>-1</td>\n",
       "        <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>TRMBFUD128F9318502</td>\n",
       "        <td>Working Underground</td>\n",
       "        <td>SOAVROI12AB0183312</td>\n",
       "        <td>My Land is Your Land</td>\n",
       "        <td>ARTOD2W1187B99FC16</td>\n",
       "        <td>41b79e6f-9621-45c9-836c-9f08bedba4eb</td>\n",
       "        <td>Ashley Hutchings_ Ernesto De Pascale</td>\n",
       "        <td>226.42892</td>\n",
       "        <td>0.4131989</td>\n",
       "        <td>0.33407375</td>\n",
       "        <td>0</td>\n",
       "        <td>3957236</td>\n",
       "        <td>-1</td>\n",
       "        <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>TRMBFNG12903CEA2A8</td>\n",
       "        <td>Alien Bzzing</td>\n",
       "        <td>SOCWCQV12AC3DF9B21</td>\n",
       "        <td>Uomini D&#x27;onore</td>\n",
       "        <td>ARUE65J1187B9AB4D9</td>\n",
       "        <td>644feeb5-0ad9-457f-9d29-98474d42d9d3</td>\n",
       "        <td>Fireside</td>\n",
       "        <td>345.96527</td>\n",
       "        <td>0.48547184</td>\n",
       "        <td>0.3672936</td>\n",
       "        <td>1997</td>\n",
       "        <td>8593681</td>\n",
       "        <td>-1</td>\n",
       "        <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>TRMBFSN128F4259499</td>\n",
       "        <td>Repente</td>\n",
       "        <td>SOGWDNA12A8C139BFC</td>\n",
       "        <td>Limite das Aguas</td>\n",
       "        <td>ARS8WH31187B9B8B04</td>\n",
       "        <td>e02d67b8-b581-478e-be33-c988627e4050</td>\n",
       "        <td>Edu Lobo</td>\n",
       "        <td>269.47873</td>\n",
       "        <td>0.34406215</td>\n",
       "        <td>0.0</td>\n",
       "        <td>0</td>\n",
       "        <td>2775420</td>\n",
       "        <td>-1</td>\n",
       "        <td>0</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('TRMBFJC12903CE2A89', 'Mystery Babylon', 'SOGOPFY12AB018E5B1', 'Healing of All Nations', 'ARP06GY1187B98B0F0', '106e0414-95a7-45e9-8176-bbc938deed68', 'Yami Bolo', 146.49425, 0.4955022, 0.3224826, 2001, 8562838, -1, 0),\n",
       " ('TRMBFTN128F42665EA', 'Världen É Din', 'SOKFZOR12A8C1306B0', 'Omérta', 'ARZ6UKQ1187B9B0E35', 'a432b2e7-7598-419b-8760-e8accff3c725', 'The Latin Kings', 268.22485, 0.54002666, 0.42142987, 0, 3164205, -1, 0),\n",
       " ('TRMBFUD128F9318502', 'Working Underground', 'SOAVROI12AB0183312', 'My Land is Your Land', 'ARTOD2W1187B99FC16', '41b79e6f-9621-45c9-836c-9f08bedba4eb', 'Ashley Hutchings_ Ernesto De Pascale', 226.42892, 0.4131989, 0.33407375, 0, 3957236, -1, 0),\n",
       " ('TRMBFNG12903CEA2A8', 'Alien Bzzing', 'SOCWCQV12AC3DF9B21', \"Uomini D'onore\", 'ARUE65J1187B9AB4D9', '644feeb5-0ad9-457f-9d29-98474d42d9d3', 'Fireside', 345.96527, 0.48547184, 0.3672936, 1997, 8593681, -1, 0),\n",
       " ('TRMBFSN128F4259499', 'Repente', 'SOGWDNA12A8C139BFC', 'Limite das Aguas', 'ARS8WH31187B9B8B04', 'e02d67b8-b581-478e-be33-c988627e4050', 'Edu Lobo', 269.47873, 0.34406215, 0.0, 0, 2775420, -1, 0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT *\n",
    "FROM deftunes.songs\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4. The second data source is a new API designed for the song purchase process. This API contains information on the purchases done by the users of the App and also contains relevant information about each user. Copy the endpoint value from the CloudFormation outputs tab and replace the placeholder `<API_ENDPOINT>` with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_ENDPOINT = \"ec2-44-214-242-221.compute-1.amazonaws.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also access the documentation to the API by opening the new browser tab, pasting the API endpoint value and adding `/docs` to it. You will see an interactive interface to test the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5. The first endpoint is to the `sessions` path in the API, which retrieves the transactional sessions. Let's test the API by performing a GET request to the endpoint with the next cell. If everything works you should get a `200` status code from the `sessions_response` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "request_start_date = \"2020-01-01\"\n",
    "request_end_date = \"2020-03-01\"\n",
    "sessions_response = requests.get(f'http://{API_ENDPOINT}/sessions?start_date={request_start_date}&end_date={request_end_date}')\n",
    "print(sessions_response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6. You can get the content of the response in JSON format using the `.json()` method, let's print the first record with the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"user_id\": \"6b287203-7cab-4f1a-b1a4-2b5076294682\",\n",
      "    \"session_id\": \"04a5e8ac-1acd-48dc-88b9-651c4ddf489c\",\n",
      "    \"session_items\": [\n",
      "        {\n",
      "            \"song_id\": \"TRXKAGX128F9342DD7\",\n",
      "            \"song_name\": \"3 Cards\",\n",
      "            \"artist_id\": \"AR475MP1187B9A5449\",\n",
      "            \"artist_name\": \"The Balancing Act\",\n",
      "            \"price\": 1.03,\n",
      "            \"currency\": \"USD\",\n",
      "            \"liked\": true,\n",
      "            \"liked_since\": \"2023-01-27T08:29:54.970697\"\n",
      "        },\n",
      "        {\n",
      "            \"song_id\": \"TRUKGBT128F4292C9B\",\n",
      "            \"song_name\": \"Parisian Walls (gband Version_ Barcelona)\",\n",
      "            \"artist_id\": \"ARP9HJX1187FB4E5DA\",\n",
      "            \"artist_name\": \"Apostle Of Hustle\",\n",
      "            \"price\": 1.31,\n",
      "            \"currency\": \"USD\",\n",
      "            \"liked\": true,\n",
      "            \"liked_since\": \"2023-06-14T00:27:55.876873\"\n",
      "        },\n",
      "        {\n",
      "            \"song_id\": \"TRCPHWV128F4228647\",\n",
      "            \"song_name\": \"Los Sabanales\",\n",
      "            \"artist_id\": \"ARRLMTZ1187B9AB6DD\",\n",
      "            \"artist_name\": \"Los Corraleros De Majagual\",\n",
      "            \"price\": 0.69,\n",
      "            \"currency\": \"USD\",\n",
      "            \"liked\": false,\n",
      "            \"liked_since\": null\n",
      "        },\n",
      "        {\n",
      "            \"song_id\": \"TRMHNOY12903CDD075\",\n",
      "            \"song_name\": \"Earth Messiah\",\n",
      "            \"artist_id\": \"ARHS5PJ1187FB3C50D\",\n",
      "            \"artist_name\": \"Cathedral\",\n",
      "            \"price\": 0.52,\n",
      "            \"currency\": \"USD\",\n",
      "            \"liked\": false,\n",
      "            \"liked_since\": null\n",
      "        },\n",
      "        {\n",
      "            \"song_id\": \"TRIBGDZ128F425A019\",\n",
      "            \"song_name\": \"Eu sei que vou te amar\",\n",
      "            \"artist_id\": \"ARJK24L1187B9AFF5E\",\n",
      "            \"artist_name\": \"Quarteto Em Cy\",\n",
      "            \"price\": 1.74,\n",
      "            \"currency\": \"USD\",\n",
      "            \"liked\": false,\n",
      "            \"liked_since\": null\n",
      "        },\n",
      "        {\n",
      "            \"song_id\": \"TROJSDS128F146E6E2\",\n",
      "            \"song_name\": \"Write My Ticket\",\n",
      "            \"artist_id\": \"AR227JP1187FB375BC\",\n",
      "            \"artist_name\": \"Tift Merritt\",\n",
      "            \"price\": 0.76,\n",
      "            \"currency\": \"USD\",\n",
      "            \"liked\": true,\n",
      "            \"liked_since\": \"2023-10-18T21:54:16.351564\"\n",
      "        }\n",
      "    ],\n",
      "    \"user_agent\": \"Mozilla/5.0 (Windows NT 11.0) AppleWebKit/531.2 (KHTML, like Gecko) Chrome/19.0.859.0 Safari/531.2\",\n",
      "    \"session_start_time\": \"2020-02-07T18:05:25.824461\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "sessions_json = sessions_response.json()\n",
    "print(json.dumps(sessions_json[0], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.7. The second endpoint is to the `users` path in the API, it retrieves the transactional sessions. Perform a GET request to the endpoint with the next cell, then print a sample with the cell after that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "users_request = requests.get(f'http://{API_ENDPOINT}/users')\n",
    "print(users_request.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"user_id\": \"a3141825-3a8c-4968-a3af-5362011ef7d5\",\n",
      "    \"user_name\": \"Elizabeth\",\n",
      "    \"user_lastname\": \"Carey\",\n",
      "    \"user_location\": [\n",
      "        \"46.32313\",\n",
      "        \"-0.45877\",\n",
      "        \"Niort\",\n",
      "        \"FR\",\n",
      "        \"Europe/Paris\"\n",
      "    ],\n",
      "    \"user_since\": \"2020-12-22T14:15:35.936090\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "users_json = users_request.json()\n",
    "print(json.dumps(users_json[0], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## 3 - Exploratory Data Analysis\n",
    "\n",
    "To better understand the data sources, start analyzing the data types and values that come from each source. You can use the pandas library to perform Exploratory Data Analysis (EDA) on samples of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1. Let's begin with the `songs` table in the source RDS database, we will take advantage of the `%sql` magic to bring a sample with SQL and convert it into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql+psycopg2://postgresuser:***@de-c4w4a1-rds.c58i0c4u0qe2.us-east-1.rds.amazonaws.com:5432/postgres\n",
      "5 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>title</th>\n",
       "      <th>song_id</th>\n",
       "      <th>release</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>artist_mbid</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>duration</th>\n",
       "      <th>artist_familiarity</th>\n",
       "      <th>artist_hotttnesss</th>\n",
       "      <th>year</th>\n",
       "      <th>track_7digitalid</th>\n",
       "      <th>shs_perf</th>\n",
       "      <th>shs_work</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRMBFJC12903CE2A89</td>\n",
       "      <td>Mystery Babylon</td>\n",
       "      <td>SOGOPFY12AB018E5B1</td>\n",
       "      <td>Healing of All Nations</td>\n",
       "      <td>ARP06GY1187B98B0F0</td>\n",
       "      <td>106e0414-95a7-45e9-8176-bbc938deed68</td>\n",
       "      <td>Yami Bolo</td>\n",
       "      <td>146.49425</td>\n",
       "      <td>0.495502</td>\n",
       "      <td>0.322483</td>\n",
       "      <td>2001</td>\n",
       "      <td>8562838</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRMBFTN128F42665EA</td>\n",
       "      <td>Världen É Din</td>\n",
       "      <td>SOKFZOR12A8C1306B0</td>\n",
       "      <td>Omérta</td>\n",
       "      <td>ARZ6UKQ1187B9B0E35</td>\n",
       "      <td>a432b2e7-7598-419b-8760-e8accff3c725</td>\n",
       "      <td>The Latin Kings</td>\n",
       "      <td>268.22485</td>\n",
       "      <td>0.540027</td>\n",
       "      <td>0.421430</td>\n",
       "      <td>0</td>\n",
       "      <td>3164205</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRMBFUD128F9318502</td>\n",
       "      <td>Working Underground</td>\n",
       "      <td>SOAVROI12AB0183312</td>\n",
       "      <td>My Land is Your Land</td>\n",
       "      <td>ARTOD2W1187B99FC16</td>\n",
       "      <td>41b79e6f-9621-45c9-836c-9f08bedba4eb</td>\n",
       "      <td>Ashley Hutchings_ Ernesto De Pascale</td>\n",
       "      <td>226.42892</td>\n",
       "      <td>0.413199</td>\n",
       "      <td>0.334074</td>\n",
       "      <td>0</td>\n",
       "      <td>3957236</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRMBFNG12903CEA2A8</td>\n",
       "      <td>Alien Bzzing</td>\n",
       "      <td>SOCWCQV12AC3DF9B21</td>\n",
       "      <td>Uomini D'onore</td>\n",
       "      <td>ARUE65J1187B9AB4D9</td>\n",
       "      <td>644feeb5-0ad9-457f-9d29-98474d42d9d3</td>\n",
       "      <td>Fireside</td>\n",
       "      <td>345.96527</td>\n",
       "      <td>0.485472</td>\n",
       "      <td>0.367294</td>\n",
       "      <td>1997</td>\n",
       "      <td>8593681</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRMBFSN128F4259499</td>\n",
       "      <td>Repente</td>\n",
       "      <td>SOGWDNA12A8C139BFC</td>\n",
       "      <td>Limite das Aguas</td>\n",
       "      <td>ARS8WH31187B9B8B04</td>\n",
       "      <td>e02d67b8-b581-478e-be33-c988627e4050</td>\n",
       "      <td>Edu Lobo</td>\n",
       "      <td>269.47873</td>\n",
       "      <td>0.344062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2775420</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             track_id                title             song_id  \\\n",
       "0  TRMBFJC12903CE2A89      Mystery Babylon  SOGOPFY12AB018E5B1   \n",
       "1  TRMBFTN128F42665EA        Världen É Din  SOKFZOR12A8C1306B0   \n",
       "2  TRMBFUD128F9318502  Working Underground  SOAVROI12AB0183312   \n",
       "3  TRMBFNG12903CEA2A8         Alien Bzzing  SOCWCQV12AC3DF9B21   \n",
       "4  TRMBFSN128F4259499              Repente  SOGWDNA12A8C139BFC   \n",
       "\n",
       "                  release           artist_id  \\\n",
       "0  Healing of All Nations  ARP06GY1187B98B0F0   \n",
       "1                  Omérta  ARZ6UKQ1187B9B0E35   \n",
       "2    My Land is Your Land  ARTOD2W1187B99FC16   \n",
       "3          Uomini D'onore  ARUE65J1187B9AB4D9   \n",
       "4        Limite das Aguas  ARS8WH31187B9B8B04   \n",
       "\n",
       "                            artist_mbid                           artist_name  \\\n",
       "0  106e0414-95a7-45e9-8176-bbc938deed68                             Yami Bolo   \n",
       "1  a432b2e7-7598-419b-8760-e8accff3c725                       The Latin Kings   \n",
       "2  41b79e6f-9621-45c9-836c-9f08bedba4eb  Ashley Hutchings_ Ernesto De Pascale   \n",
       "3  644feeb5-0ad9-457f-9d29-98474d42d9d3                              Fireside   \n",
       "4  e02d67b8-b581-478e-be33-c988627e4050                              Edu Lobo   \n",
       "\n",
       "    duration  artist_familiarity  artist_hotttnesss  year  track_7digitalid  \\\n",
       "0  146.49425            0.495502           0.322483  2001           8562838   \n",
       "1  268.22485            0.540027           0.421430     0           3164205   \n",
       "2  226.42892            0.413199           0.334074     0           3957236   \n",
       "3  345.96527            0.485472           0.367294  1997           8593681   \n",
       "4  269.47873            0.344062           0.000000     0           2775420   \n",
       "\n",
       "   shs_perf  shs_work  \n",
       "0        -1         0  \n",
       "1        -1         0  \n",
       "2        -1         0  \n",
       "3        -1         0  \n",
       "4        -1         0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs_result = %sql SELECT *FROM deftunes.songs LIMIT 5\n",
    "songs_df = songs_result.DataFrame()\n",
    "songs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1. Use Pandas info() method to print out a summary of information about the dataframe, including information about the columns such as their data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   track_id            5 non-null      object \n",
      " 1   title               5 non-null      object \n",
      " 2   song_id             5 non-null      object \n",
      " 3   release             5 non-null      object \n",
      " 4   artist_id           5 non-null      object \n",
      " 5   artist_mbid         5 non-null      object \n",
      " 6   artist_name         5 non-null      object \n",
      " 7   duration            5 non-null      float64\n",
      " 8   artist_familiarity  5 non-null      float64\n",
      " 9   artist_hotttnesss   5 non-null      float64\n",
      " 10  year                5 non-null      int64  \n",
      " 11  track_7digitalid    5 non-null      int64  \n",
      " 12  shs_perf            5 non-null      int64  \n",
      " 13  shs_work            5 non-null      int64  \n",
      "dtypes: float64(3), int64(4), object(7)\n",
      "memory usage: 692.0+ bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(songs_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2. Use the describe() method to generate a summary of statistics about the numerical columns in the DataFrame. The describe() method can also generate descriptive statistics for the categorical columns but by default only numerical columns are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>artist_familiarity</th>\n",
       "      <th>artist_hotttnesss</th>\n",
       "      <th>year</th>\n",
       "      <th>track_7digitalid</th>\n",
       "      <th>shs_perf</th>\n",
       "      <th>shs_work</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>251.318404</td>\n",
       "      <td>0.455652</td>\n",
       "      <td>0.289056</td>\n",
       "      <td>799.600000</td>\n",
       "      <td>5.410676e+06</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>72.768888</td>\n",
       "      <td>0.077219</td>\n",
       "      <td>0.166088</td>\n",
       "      <td>1094.898306</td>\n",
       "      <td>2.922813e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>146.494250</td>\n",
       "      <td>0.344062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.775420e+06</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>226.428920</td>\n",
       "      <td>0.413199</td>\n",
       "      <td>0.322483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.164205e+06</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>268.224850</td>\n",
       "      <td>0.485472</td>\n",
       "      <td>0.334074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.957236e+06</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>269.478730</td>\n",
       "      <td>0.495502</td>\n",
       "      <td>0.367294</td>\n",
       "      <td>1997.000000</td>\n",
       "      <td>8.562838e+06</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>345.965270</td>\n",
       "      <td>0.540027</td>\n",
       "      <td>0.421430</td>\n",
       "      <td>2001.000000</td>\n",
       "      <td>8.593681e+06</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         duration  artist_familiarity  artist_hotttnesss         year  \\\n",
       "count    5.000000            5.000000           5.000000     5.000000   \n",
       "mean   251.318404            0.455652           0.289056   799.600000   \n",
       "std     72.768888            0.077219           0.166088  1094.898306   \n",
       "min    146.494250            0.344062           0.000000     0.000000   \n",
       "25%    226.428920            0.413199           0.322483     0.000000   \n",
       "50%    268.224850            0.485472           0.334074     0.000000   \n",
       "75%    269.478730            0.495502           0.367294  1997.000000   \n",
       "max    345.965270            0.540027           0.421430  2001.000000   \n",
       "\n",
       "       track_7digitalid  shs_perf  shs_work  \n",
       "count      5.000000e+00       5.0       5.0  \n",
       "mean       5.410676e+06      -1.0       0.0  \n",
       "std        2.922813e+06       0.0       0.0  \n",
       "min        2.775420e+06      -1.0       0.0  \n",
       "25%        3.164205e+06      -1.0       0.0  \n",
       "50%        3.957236e+06      -1.0       0.0  \n",
       "75%        8.562838e+06      -1.0       0.0  \n",
       "max        8.593681e+06      -1.0       0.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3. Convert JSON objects `sessions_json` and `users_json` into pandas dataframes, and display the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>session_items</th>\n",
       "      <th>user_agent</th>\n",
       "      <th>session_start_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6b287203-7cab-4f1a-b1a4-2b5076294682</td>\n",
       "      <td>04a5e8ac-1acd-48dc-88b9-651c4ddf489c</td>\n",
       "      <td>[{'song_id': 'TRXKAGX128F9342DD7', 'song_name'...</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 11.0) AppleWebKit/531....</td>\n",
       "      <td>2020-02-07T18:05:25.824461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>958ba0c2-cfc0-405e-a037-e644a4f34981</td>\n",
       "      <td>143e7ad2-e172-4590-aeaa-b50ee449e7b3</td>\n",
       "      <td>[{'song_id': 'TRTHVBF128F935584D', 'song_name'...</td>\n",
       "      <td>Mozilla/5.0 (compatible; MSIE 5.0; Windows NT ...</td>\n",
       "      <td>2020-02-18T04:07:14.676057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7d13cf48-d80e-4cbe-8581-cce0fd301acf</td>\n",
       "      <td>6817fe3c-dd7f-4885-936b-dbbbb40923f2</td>\n",
       "      <td>[{'song_id': 'TRJOSHE12903CDBA6F', 'song_name'...</td>\n",
       "      <td>Opera/9.90.(X11; Linux i686; cv-RU) Presto/2.9...</td>\n",
       "      <td>2020-02-21T22:25:56.407581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c06a8f89-4d88-4d71-83db-d567a69ef902</td>\n",
       "      <td>0383ce58-b47d-4923-abdd-d58d583d7bb2</td>\n",
       "      <td>[{'song_id': 'TRSXSSK128F146EB46', 'song_name'...</td>\n",
       "      <td>Mozilla/5.0 (iPod; U; CPU iPhone OS 3_1 like M...</td>\n",
       "      <td>2020-02-19T04:27:31.957162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7118b8ac-75fe-426a-bf6c-09044ed64011</td>\n",
       "      <td>579ef099-ffed-410c-916a-05c222d7a734</td>\n",
       "      <td>[{'song_id': 'TRRKCXY128F42B08EC', 'song_name'...</td>\n",
       "      <td>Opera/8.77.(X11; Linux x86_64; lb-LU) Presto/2...</td>\n",
       "      <td>2020-01-28T20:10:19.161986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                user_id                            session_id  \\\n",
       "0  6b287203-7cab-4f1a-b1a4-2b5076294682  04a5e8ac-1acd-48dc-88b9-651c4ddf489c   \n",
       "1  958ba0c2-cfc0-405e-a037-e644a4f34981  143e7ad2-e172-4590-aeaa-b50ee449e7b3   \n",
       "2  7d13cf48-d80e-4cbe-8581-cce0fd301acf  6817fe3c-dd7f-4885-936b-dbbbb40923f2   \n",
       "3  c06a8f89-4d88-4d71-83db-d567a69ef902  0383ce58-b47d-4923-abdd-d58d583d7bb2   \n",
       "4  7118b8ac-75fe-426a-bf6c-09044ed64011  579ef099-ffed-410c-916a-05c222d7a734   \n",
       "\n",
       "                                       session_items  \\\n",
       "0  [{'song_id': 'TRXKAGX128F9342DD7', 'song_name'...   \n",
       "1  [{'song_id': 'TRTHVBF128F935584D', 'song_name'...   \n",
       "2  [{'song_id': 'TRJOSHE12903CDBA6F', 'song_name'...   \n",
       "3  [{'song_id': 'TRSXSSK128F146EB46', 'song_name'...   \n",
       "4  [{'song_id': 'TRRKCXY128F42B08EC', 'song_name'...   \n",
       "\n",
       "                                          user_agent  \\\n",
       "0  Mozilla/5.0 (Windows NT 11.0) AppleWebKit/531....   \n",
       "1  Mozilla/5.0 (compatible; MSIE 5.0; Windows NT ...   \n",
       "2  Opera/9.90.(X11; Linux i686; cv-RU) Presto/2.9...   \n",
       "3  Mozilla/5.0 (iPod; U; CPU iPhone OS 3_1 like M...   \n",
       "4  Opera/8.77.(X11; Linux x86_64; lb-LU) Presto/2...   \n",
       "\n",
       "           session_start_time  \n",
       "0  2020-02-07T18:05:25.824461  \n",
       "1  2020-02-18T04:07:14.676057  \n",
       "2  2020-02-21T22:25:56.407581  \n",
       "3  2020-02-19T04:27:31.957162  \n",
       "4  2020-01-28T20:10:19.161986  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_df = pd.json_normalize(sessions_json)\n",
    "session_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_lastname</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_since</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a3141825-3a8c-4968-a3af-5362011ef7d5</td>\n",
       "      <td>Elizabeth</td>\n",
       "      <td>Carey</td>\n",
       "      <td>[46.32313, -0.45877, Niort, FR, Europe/Paris]</td>\n",
       "      <td>2020-12-22T14:15:35.936090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>923d55a6-26e9-4a61-b3e1-9c010e5db2cc</td>\n",
       "      <td>Joshua</td>\n",
       "      <td>Bishop</td>\n",
       "      <td>[46.75451, 33.34864, Nova Kakhovka, UA, Europe...</td>\n",
       "      <td>2023-09-20T02:26:02.939528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ff728e8b-0c5b-48f7-a133-a30bf86c25e3</td>\n",
       "      <td>Joseph</td>\n",
       "      <td>Mcclain</td>\n",
       "      <td>[32.57756, 71.52847, Mianwali, PK, Asia/Karachi]</td>\n",
       "      <td>2023-12-05T17:59:27.933557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9ae4d3aa-8cc8-42ac-beb4-5c9c799a392d</td>\n",
       "      <td>Jasmine</td>\n",
       "      <td>White</td>\n",
       "      <td>[35.6803, 51.0193, Shahre Jadide Andisheh, IR,...</td>\n",
       "      <td>2024-06-18T17:56:45.626088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>043010aa-9aad-4f63-8932-45eddada7856</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>Ibarra</td>\n",
       "      <td>[51.168, 7.973, Finnentrop, DE, Europe/Berlin]</td>\n",
       "      <td>2023-11-13T10:27:32.854497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                user_id  user_name user_lastname  \\\n",
       "0  a3141825-3a8c-4968-a3af-5362011ef7d5  Elizabeth         Carey   \n",
       "1  923d55a6-26e9-4a61-b3e1-9c010e5db2cc     Joshua        Bishop   \n",
       "2  ff728e8b-0c5b-48f7-a133-a30bf86c25e3     Joseph       Mcclain   \n",
       "3  9ae4d3aa-8cc8-42ac-beb4-5c9c799a392d    Jasmine         White   \n",
       "4  043010aa-9aad-4f63-8932-45eddada7856      Tyler        Ibarra   \n",
       "\n",
       "                                       user_location  \\\n",
       "0      [46.32313, -0.45877, Niort, FR, Europe/Paris]   \n",
       "1  [46.75451, 33.34864, Nova Kakhovka, UA, Europe...   \n",
       "2   [32.57756, 71.52847, Mianwali, PK, Asia/Karachi]   \n",
       "3  [35.6803, 51.0193, Shahre Jadide Andisheh, IR,...   \n",
       "4     [51.168, 7.973, Finnentrop, DE, Europe/Berlin]   \n",
       "\n",
       "                   user_since  \n",
       "0  2020-12-22T14:15:35.936090  \n",
       "1  2023-09-20T02:26:02.939528  \n",
       "2  2023-12-05T17:59:27.933557  \n",
       "3  2024-06-18T17:56:45.626088  \n",
       "4  2023-11-13T10:27:32.854497  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df = pd.json_normalize(users_json)\n",
    "user_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## 4 - ETL Pipeline with AWS Glue and Terraform\n",
    "\n",
    "Now you will start creating the required resources and infrastructure for your data pipeline. Remember that you will use a medallion architecture.\n",
    "\n",
    "The pipeline will be composed by the following steps:\n",
    "- An extraction job to get the data from the PostgreSQL Database. This data will be stored in the landing zone of your Data Lake.\n",
    "- An extraction job to get the data from the two API endpoints. This data will be stored in the landing zone of your Data Lake in JSON format.\n",
    "- A transformation job that takes the raw data extracted from the PostgreSQL Database, casts some fields to the correct data types, adds some metadata and stores the dataset in Iceberg format.\n",
    "- A transformation that takes the JSON data extracted from the API endpoints, normalizes some nested fields, adds metadata and stores the dataset in Iceberg format.\n",
    "- The creation of some schemas in your Data Warehouse hosted in Redshift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4-1'></a>\n",
    "### 4.1 - Landing Zone\n",
    "\n",
    "For the landing zone, you are going to create three Glue Jobs: one to extract the data from the PostgreSQL database and two to get the data from each API's endpoint. You are going to create those jobs using Terraform to guarantee that the infrastructure for each job will be always the same and changes can be tracked easily. Let's start by creating the jobs and then creating the infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.1. Go to the `terraform/assets/extract_jobs` folder. You will find two scripts\n",
    "\n",
    "- `de-c4w4a1-extract-songs-job.py`: this script extracts data from the PostgreSQL source database.\n",
    "- `de-c4w4a1-api-extract-job.py`: this script extracts data from the API. Endpoints can be provided through parameters.\n",
    "\n",
    "Open each of them and follow the instructions in the comments to complete the scripts. Save changes to both of the files.\n",
    "\n",
    "In a later section, you will run the Glue Jobs. Note that data in the landing zone of your Data Lake will be stored in subfolders named according to the ingestion date, which defaults to the server's current date in Pacific Time (UTC -7). This ensures consistent date alignment with the server timezone used for job scheduling and data partitioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.2. Open the `terraform/modules/extract_job/glue.tf` file. In this file you will set all the required resources to create the Glue Jobs. Complete the code following the instructions and save changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.3. Explore the rest of the files of the `extract_job` module to understand the whole infrastructure. Avoid performing further changes to other files. Here is the summary of what you can find in those files:\n",
    "- In `terraform/modules/extract_job/iam.tf` file you can find the creation of the role used to execute the Glue Jobs. There is also the attachment of a policy holding the permissions. Those permissions can be found directly in the `terraform/modules/extract_job/policies.tf` file.\n",
    "- The `terraform/modules/extract_job/network.tf` has the definition of the private subnet and the source database security group used to create the Glue Connection used to allow the Glue Jobs to connect to the source PostgreSQL database.\n",
    "- The `terraform/modules/extract_job/variables.tf` contains the necessary input parameters for this module, while the `terraform/modules/extract_job/outputs.tf` sets the possible outputs that terraform will show in the console from this module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.4.  Open the `terraform/main.tf` file and uncomment the lines associated with the module named `extract_job` (lines 1 to 15); make sure to keep the rest of modules commented. Open the `terraform/outputs.tf` file and uncomment the outputs associated with the extract module (lines 5 to 20). Save changes in both of the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.5 Copy the glue scripts into the Scripts Bucket, run the following cells then you are ready to deploy the first module of your infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: terraform/assets/extract_jobs/de-c4w4a1-api-extract-job.py to s3://de-c4w4a1-800994367402-us-east-1-scripts/de-c4w4a1-api-extract-job.py\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ./terraform/assets/extract_jobs/de-c4w4a1-api-extract-job.py s3://{SCRIPTS_BUCKET_NAME}/de-c4w4a1-api-extract-job.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: terraform/assets/extract_jobs/de-c4w4a1-extract-songs-job.py to s3://de-c4w4a1-800994367402-us-east-1-scripts/de-c4w4a1-extract-songs-job.py\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ./terraform/assets/extract_jobs/de-c4w4a1-extract-songs-job.py s3://{SCRIPTS_BUCKET_NAME}/de-c4w4a1-extract-songs-job.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.6. In the terminal, go to the `terraform` folder and deploy the infrastructure with the following commands.\n",
    "\n",
    "```bash\n",
    "cd terraform\n",
    "terraform init\n",
    "terraform plan\n",
    "terraform apply\n",
    "```\n",
    "\n",
    "*Note*: Remember that the command `terraform apply` will prompt you to reply `yes`. If you have to apply changes to the glue job scripts, copy the scripts again with the step 4.1.5, then run `terraform destroy` and then reapply the infrastructure with `terraform apply`, both will prompt you to reply `yes`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: If there are errors in the commands or Terraform configuration files, the terminal may crash. \n",
    "When this happens, you will see the following message:\n",
    "\n",
    "![etl_diagram](images/terminal_crash.png)\n",
    "\n",
    "You can reopen the terminal by pressing <code>Ctrl + \\`</code> (or <code>Cmd + \\`</code>) or by navigating to View > Terminal. \n",
    "In the terminal, go again to the Terraform folder (`cd terraform`) and then try \n",
    "rerunning the required commands. The error should now appear in the terminal.\n",
    "If the terminal continues to crash, run the following command instead:\n",
    "`terraform apply -no-color  2> errors.txt`\n",
    "This will create a text file containing the error message without causing the terminal to crash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.7. You will get some outputs, in particular, you require the following three: `glue_api_users_extract_job`, `glue_sessions_users_extract_job` and `glue_rds_extract_job`. Those outputs correspond to the three glue jobs that will extract the data from the API endpoints and the database respectively. Use the following command in the terminal to execute each job based on its name. Replace `<JOB-NAME>` with the value of the terraform outputs (`de-c4w4a1-api-users-extract-job`, `de-c4w4a1-api-sessions-extract-job` and `de-c4w4a1-rds-extract-job`) respectively. You can run those three jobs in parallel.\n",
    "\n",
    "```bash\n",
    "aws glue start-job-run --job-name <JOB-NAME> | jq -r '.JobRunId'\n",
    "```\n",
    "\n",
    "You should get `JobRunID` in the output. Use this job run ID to track each job status by using this command, replacing the `<JOB-NAME>` and `<JobRunID>` placeholders.\n",
    "\n",
    "```bash\n",
    "aws glue get-job-run --job-name <JOB-NAME> --run-id <JobRunID> --output text --query \"JobRun.JobRunState\"\n",
    "```\n",
    "\n",
    "Wait until the statuses of those three jobs change to `SUCCEEDED` (each job should take around 3 mins)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: If the Glue job fails, you can check its status in the AWS Glue console, where an error message will be displayed. This message can help you debug issues in the Glue scripts. After updating the scripts, be sure to rerun the commands in step 4.1.5 to upload the updated scripts to the scripts bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4-2'></a>\n",
    "### 4.2 - Transformation Zone\n",
    "\n",
    "Once you have run the jobs that feed the first layer of your Data Lake three-tier architecture, it is time to generate the jobs to transform the data and store it in the second layer. The methodology will be similar to the previous zone: you will create the Glue Job scripts to take the data out of the landing layer, transform it and put it into the transformation zone layer. Then you will create the necessary resources in AWS using Terraform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.1. Go to the `terraform/assets/transform_jobs` folder. You will find two scripts:\n",
    "- `de-c4w4a1-transform-songs-job.py`\n",
    "- `de-c4w4a1-transform-json-job.py`\n",
    "\n",
    "The two scripts will take the data out of the `landing_zone` layer in the Data Lake and, after some transformations, will store the data into the `transform_zone` layer in Apache Iceberg format.\n",
    "\n",
    "Open each of them and follow the instructions in the comments to complete the scripts. Save changes to both of the files.\n",
    "\n",
    "If you want to know more about saving data into Apache Iceberg format using the Glue Catalog, you can check the [documentation](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-iceberg.html#aws-glue-programming-etl-format-iceberg-insert)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.2. Open the `terraform/modules/transform_job/glue.tf` file. Complete the code following the instructions and save changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.3. Open the `terraform/main.tf` file and uncomment the lines associated with the module named `transform_job` (lines 17 to 31). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.4. Open the `terraform/outputs.tf` file and uncomment the lines 22 to 34. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.5. Copy the glue script into the Scripts bucket with the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp ./terraform/assets/transform_jobs/de-c4w4a1-transform-json-job.py s3://{SCRIPTS_BUCKET_NAME}/de-c4w4a1-transform-json-job.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp ./terraform/assets/transform_jobs/de-c4w4a1-transform-songs-job.py s3://{SCRIPTS_BUCKET_NAME}/de-c4w4a1-transform-songs-job.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.6. In the terminal, go to the `terraform` folder and deploy the infrastructure with the following commands:\n",
    "\n",
    "```bash\n",
    "cd terraform\n",
    "terraform init\n",
    "terraform plan\n",
    "terraform apply\n",
    "```\n",
    "\n",
    "*Note*: Remember that the command `terraform apply` will prompt you to reply `yes`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.8. You will get some additional outputs, in particular, you require the following two: `glue_json_transformation_job` and `glue_songs_transformation_job`. Execute the two glue jobs, based on the name (`de-c4w4a1-json-transform-job` or `de-c4w4a1-songs-transform-job`). You can run those two jobs in parallel. Use the following command in the terminal:\n",
    "\n",
    "```bash\n",
    "aws glue start-job-run --job-name <JOB-NAME> | jq -r '.JobRunId'\n",
    "```\n",
    "\n",
    "And based on the job run ID track each job status by using this command:\n",
    "\n",
    "```bash\n",
    "aws glue get-job-run --job-name <JOB-NAME> --run-id <JobRunID> --output text --query \"JobRun.JobRunState\"\n",
    "```\n",
    "\n",
    "Wait until the jobs statuses change to `SUCCEEDED` (each job should take around 3 mins)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4-3'></a>\n",
    "### 4.3 - Serving Zone\n",
    "\n",
    "For the last layer of your Three-tier Data Lake architecture, you are going to use AWS Redshift as a Data Warehouse solution. The transformations will be performed directly inside Redshift, but you need to make the data available in that storage solution. For that, you will use Redshift Spectrum, which is a feature that allows you to run queries against data stored in S3 without having to load the data into Redshift tables. For that, you are required to use a Glue Catalog which was already created in the `transform` module.\n",
    "\n",
    "Follow the instructions below to finish setting up your resources for the `serving` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3.1. Open the file located at `terraform/modules/serving/iam.tf`. Complete the code and save changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3.2. Open the file `terraform/modules/serving/redshift.tf`. Complete the code and save changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3.4. Open the `terraform/main.tf` file and uncomment the lines associated with the module named `serving` (lines 33 to 50). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3.5. Uncomment the corresponding lines in the `terraform/outputs.tf` file (lines 37 to 44).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3.6. Deploy the infrastructure for the last layer. In the terminal, go to the `terraform` folder and run the commands:\n",
    "\n",
    "```bash\n",
    "cd terraform\n",
    "terraform init\n",
    "terraform plan\n",
    "terraform apply\n",
    "```\n",
    "\n",
    "*Note*: Remember that the command `terraform apply` will prompt you to reply `yes`.\n",
    "\n",
    "With that, you have deployed the required infrastructure for your three-tier data lake. The next step consists of modelling the data in your Redshift Data Warehouse to serve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5 - Data Modeling with dbt and Redshift Spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5-1'></a>\n",
    "### 5.1 - Redshift Setup\n",
    "\n",
    "Before working with DBT to model the data in the transformation layer into the serving layer, you need to use **Redshift Spectrum** to connect Redshift with the Iceberg tables. Spectrum allows you to query files from S3 directly from Redshift, it also has a special feature that allows us to read directly from Iceberg tables just by creating an external schema that points to the Glue database containing the tables. For this initial setup, you will use Terraform to set up the external schema and a normal schema for the serving layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1.1. Navigate to the `terraform` folder and run the serving module with the following command:\n",
    "\n",
    "```bash\n",
    "terraform apply -target=module.serving\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1.1. Navigate to the `terraform` folder and run the serving module with the following command:\n",
    "\n",
    "```bash\n",
    "terraform apply -target=module.serving\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5-2'></a>\n",
    "### 5.2 - Redshift Test\n",
    "\n",
    "To verify that the schemas were set up successfully, you will connect to the target Redshift cluster using the `%sql` magic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython-sql in /home/coder/miniconda/lib/python3.12/site-packages (0.4.1)\n",
      "Requirement already satisfied: prettytable<1 in /home/coder/miniconda/lib/python3.12/site-packages (from ipython-sql) (0.7.2)\n",
      "Requirement already satisfied: ipython>=1.0 in /home/coder/miniconda/lib/python3.12/site-packages (from ipython-sql) (8.27.0)\n",
      "Requirement already satisfied: sqlalchemy>=0.6.7 in /home/coder/miniconda/lib/python3.12/site-packages (from ipython-sql) (1.4.54)\n",
      "Requirement already satisfied: sqlparse in /home/coder/miniconda/lib/python3.12/site-packages (from ipython-sql) (0.5.3)\n",
      "Requirement already satisfied: six in /home/coder/miniconda/lib/python3.12/site-packages (from ipython-sql) (1.16.0)\n",
      "Requirement already satisfied: ipython-genutils>=0.1.0 in /home/coder/miniconda/lib/python3.12/site-packages (from ipython-sql) (0.2.0)\n",
      "Requirement already satisfied: decorator in /home/coder/miniconda/lib/python3.12/site-packages (from ipython>=1.0->ipython-sql) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/coder/miniconda/lib/python3.12/site-packages (from ipython>=1.0->ipython-sql) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /home/coder/miniconda/lib/python3.12/site-packages (from ipython>=1.0->ipython-sql) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/coder/miniconda/lib/python3.12/site-packages (from ipython>=1.0->ipython-sql) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/coder/miniconda/lib/python3.12/site-packages (from ipython>=1.0->ipython-sql) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /home/coder/miniconda/lib/python3.12/site-packages (from ipython>=1.0->ipython-sql) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /home/coder/miniconda/lib/python3.12/site-packages (from ipython>=1.0->ipython-sql) (5.14.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/coder/miniconda/lib/python3.12/site-packages (from ipython>=1.0->ipython-sql) (4.8.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/coder/miniconda/lib/python3.12/site-packages (from sqlalchemy>=0.6.7->ipython-sql) (3.1.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/coder/miniconda/lib/python3.12/site-packages (from jedi>=0.16->ipython>=1.0->ipython-sql) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/coder/miniconda/lib/python3.12/site-packages (from pexpect>4.3->ipython>=1.0->ipython-sql) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/coder/miniconda/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=1.0->ipython-sql) (0.2.5)\n",
      "Requirement already satisfied: executing in /home/coder/miniconda/lib/python3.12/site-packages (from stack-data->ipython>=1.0->ipython-sql) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /home/coder/miniconda/lib/python3.12/site-packages (from stack-data->ipython>=1.0->ipython-sql) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /home/coder/miniconda/lib/python3.12/site-packages (from stack-data->ipython>=1.0->ipython-sql) (0.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipython-sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2.1. Let's start by configuring the credentials, you can obtain the Redshift's cluster endpoint in the CloudFormation stack's outputs and replace the placeholder `<REDSHIFT_ENDPOINT>` with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDSHIFTDBHOST = 'de-c4w4a1-redshift-cluster.cyif9ih8xi1i.us-east-1.redshift.amazonaws.com'\n",
    "REDSHIFTDBPORT = 5439\n",
    "REDSHIFTDBNAME = 'dev'\n",
    "REDSHIFTDBUSER = 'defaultuser'\n",
    "REDSHIFTDBPASSWORD = 'Defaultuserpwrd1234+'\n",
    "\n",
    "redshift_connection_url = f'postgresql+psycopg2://{REDSHIFTDBUSER}:{REDSHIFTDBPASSWORD}@{REDSHIFTDBHOST}:{REDSHIFTDBPORT}/{REDSHIFTDBNAME}'\n",
    "%sql {redshift_connection_url}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2.2. Test the connection and the setup, this query will return the list of available schemas for the `dev` database, and the external schema and gold layer schema should appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "exercise": [
     "ex01"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql+psycopg2://defaultuser:***@de-c4w4a1-redshift-cluster.cyif9ih8xi1i.us-east-1.redshift.amazonaws.com:5439/dev\n",
      "2 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>database_name</th>\n",
       "        <th>schema_name</th>\n",
       "        <th>schema_owner</th>\n",
       "        <th>schema_type</th>\n",
       "        <th>schema_acl</th>\n",
       "        <th>source_database</th>\n",
       "        <th>schema_option</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>dev</td>\n",
       "        <td>information_schema</td>\n",
       "        <td>1</td>\n",
       "        <td>local</td>\n",
       "        <td>rdsdb=UCDA/rdsdb~=U/rdsdb</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>dev</td>\n",
       "        <td>public</td>\n",
       "        <td>1</td>\n",
       "        <td>local</td>\n",
       "        <td>rdsdb=UCDA/rdsdb~=UC/rdsdb</td>\n",
       "        <td>None</td>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[('dev', 'information_schema', 1, 'local', 'rdsdb=UCDA/rdsdb~=U/rdsdb', None, None),\n",
       " ('dev', 'public', 1, 'local', 'rdsdb=UCDA/rdsdb~=UC/rdsdb', None, None)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SHOW SCHEMAS FROM DATABASE dev "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2.3. Now, let's verify that the Iceberg tables where automatically imported into the external schema, let's query the tables available inside the external schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "exercise": [
     "ex02"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql+psycopg2://defaultuser:***@de-c4w4a1-redshift-cluster.cyif9ih8xi1i.us-east-1.redshift.amazonaws.com:5439/dev\n",
      "0 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>database_name</th>\n",
       "        <th>schema_name</th>\n",
       "        <th>table_name</th>\n",
       "        <th>table_type</th>\n",
       "        <th>table_acl</th>\n",
       "        <th>remarks</th>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql SHOW TABLES FROM SCHEMA dev.deftunes_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the Iceberg tables in the external schema to verify that Redshift can read from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "exercise": [
     "ex03"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql+psycopg2://defaultuser:***@de-c4w4a1-redshift-cluster.cyif9ih8xi1i.us-east-1.redshift.amazonaws.com:5439/dev\n",
      "(psycopg2.errors.InvalidSchemaName) schema \"deftunes_transform\" does not exist\n",
      "\n",
      "[SQL: select * from deftunes_transform.songs limit 10]\n",
      "(Background on this error at: https://sqlalche.me/e/14/f405)\n"
     ]
    }
   ],
   "source": [
    "%sql select * from deftunes_transform.songs limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "exercise": [
     "ex04"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql+psycopg2://defaultuser:***@de-c4w4a1-redshift-cluster.cyif9ih8xi1i.us-east-1.redshift.amazonaws.com:5439/dev\n",
      "(psycopg2.errors.InvalidSchemaName) schema \"deftunes_transform\" does not exist\n",
      "\n",
      "[SQL: select * from deftunes_transform.sessions limit 10]\n",
      "(Background on this error at: https://sqlalche.me/e/14/f405)\n"
     ]
    }
   ],
   "source": [
    "%sql select * from deftunes_transform.sessions limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "exercise": [
     "ex05"
    ],
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql+psycopg2://defaultuser:***@de-c4w4a1-redshift-cluster.cyif9ih8xi1i.us-east-1.redshift.amazonaws.com:5439/dev\n",
      "(psycopg2.errors.InvalidSchemaName) schema \"deftunes_transform\" does not exist\n",
      "\n",
      "[SQL: select * from deftunes_transform.users limit 10]\n",
      "(Background on this error at: https://sqlalche.me/e/14/f405)\n"
     ]
    }
   ],
   "source": [
    "%sql select * from deftunes_transform.users limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5-3'></a>\n",
    "### 5.3 - dbt Setup\n",
    "\n",
    "Now that you have set up the target database in Redshift, you will create a dbt project that connects to Redshift and allows you to model the transform layer tables into the final data model in the serving layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3.1. Create the new project using the following commands in the terminal.\n",
    "\n",
    "```bash\n",
    "cd ..\n",
    "dbt init dbt_modeling\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the command, dbt will ask you the engine to run the project on, select the option for Redshift. The CLI will ask you for the connection details, use the same connection values you used before to configure the `%sql` magic (step 5.2.1), when asked for the `dbname` input `dev`, for `schema` input `deftunes_serving`, for `threads` input `1`. \n",
    "\n",
    "![dbt setup](images/dbt_config.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3.2. To test the connection, run the following commands:\n",
    "\n",
    "```bash\n",
    "cd dbt_modeling\n",
    "dbt debug\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything was correctly configured, you should see the following text at the end of the output:\n",
    "\n",
    "```bash\n",
    "Connection test: [OK connection ok]\n",
    "```\n",
    "\n",
    "*Note*: If you had issues defining the connection details, you can use the `profiles.yml` file in the scripts folder as a guide to define the connection details, change the placeholder in the file with the Redshift cluster endpoint and then copy it to the following path `~/.dbt/profiles.yml ` with this command:\n",
    "\n",
    "```bash\n",
    "cp ../scripts/profiles.yml ~/.dbt/profiles.yml \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5-4'></a>\n",
    "### 5.4 - Data Modeling\n",
    "\n",
    "*Note*: This section is optional and not graded.\n",
    "\n",
    "5.4.1. Now that the dbt project has the initial setup, create a new folder named `serving_layer` in the models folder, this subfolder will contain the models associated with the star schema.\n",
    "```bash\n",
    "cd models\n",
    "mkdir serving_layer\n",
    "```\n",
    "\n",
    "5.4.2. In the `./dbt_modeling/dbt_project.yml`, change the `models` block to the following one:\n",
    "\n",
    "```yaml\n",
    "models:\n",
    "  dbt_modeling:\n",
    "    serving_layer:\n",
    "      +materialized: table\n",
    "```\n",
    "\n",
    "Save changes to the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4.3. Now you can prepare files for data modeling into the star schema. You will need to identify fact and dimensional tables, then create an SQL model file for each. Finally, inside the new folder, create a `schema.yml` file. You can look at the `example` folder if needed. Once you are done modelling the data, use the following command to run the models you created (make sure you are in the `dbt_modeling` project folder):\n",
    "\n",
    "```bash\n",
    "dbt run -s serving_layer\n",
    "```\n",
    "\n",
    "If all the model runs were successful, you should see an output like this one, where X is the number of models you created.\n",
    "```bash\n",
    "Completed successfully\n",
    "\n",
    "Done. PASS=X WARN=0 ERROR=0 SKIP=0 TOTAL=X\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4.4. The final test for your models will be for you to query them using the `%sql` magic. Run the following query for each table to get a sample and verify that the model definition was correct (replace the placeholder `<TABLE_NAME>`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SHOW TABLES FROM SCHEMA dev.deftunes_serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM deftunes_serving.<TABLE_NAME> limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql DROP SCHEMA deftunes_serving CASCADE;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the first part of the capstone, you set up a data architecture for the new business operation of DeFtunes, you implemented a basic data pipeline that can be improved later on with an iterative approach. In the second part of the capstone, you will improve upon the existing data architecture adding orchestration, data visualization and data quality checks. "
   ]
  }
 ],
 "metadata": {
  "grader_version": "1",
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
