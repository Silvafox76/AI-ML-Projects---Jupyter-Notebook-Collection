{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "167d2988-b357-4854-81ad-4e59f2a212c6",
   "metadata": {},
   "source": [
    "<br>\n",
    "<a href=\"https://www.nvidia.com/en-us/training/\">\n",
    "    <div style=\"width: 55%; background-color: white; margin-top: 50px;\">\n",
    "    <img src=\"https://dli-lms.s3.amazonaws.com/assets/general/nvidia-logo.png\"\n",
    "         width=\"400\"\n",
    "         height=\"186\"\n",
    "         style=\"margin: 0px -25px -5px; width: 300px\"/>\n",
    "</a>\n",
    "<h1 style=\"line-height: 1.4;\"><font color=\"#76b900\"><b>Rapid Application Development<br>using Large Language Models</b></h1>\n",
    "<h2><b>Notebook 6.5: [Advanced]</b> Running A GenAI Server</h2>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030c9a27-949f-477c-a8f1-2fb93356921c",
   "metadata": {},
   "source": [
    "This notebook complements **Notebook 6** by providing a dedicated notebook - and hence a dedicated event loop - to kickstart a series of GenAI microservices. This notebook will enable the local model APIs that will be used through the rest of the course, and features some more advanced tangents for those interested. \n",
    "\n",
    "As a prelude to **multi-GPU scaled deployment**, this notebook provides an oversimplified but functionally-sufficient example of an **on-device deployment workflow** which can be used to offer a standardized API for a consumer-grade or on-device deployment strategy. Since this course uses a single A100 instance and has many notebooks, you will run into problems if any one notebook overallocates model resources or fails to clear cache. As such, this is your change to deploy some smaller models once and call them as necessary. \n",
    "\n",
    "> **NOTE:** This notebook is an advanced tangent, should be ran to facilitate some later feature sets, and can be optionally understood if you have the time and interest. **Do not feel pressured to understand everything in this notebook and halt your progress because of it!**\n",
    "\n",
    "> **NOTE:** Before running this notebook, we recommend shutting down all active kernel instances from notebooks 5 and below. This notebook is intended to commandeer the majority of your A100 resources to launch multiple models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b767b6a-09fd-4a24-927e-2d4b102ced10",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "## **Part 1:** Making A Custom Server For SDXL with FastAPI\n",
    "\n",
    "To start, we can make a server to wrap the SDXL API you experimented with at the end of Notebook 5. This is intended as an example application derived largely from the HuggingFace documented example ([narrative](https://huggingface.co/docs/diffusers/main/en/using-diffusers/create_a_server) and [code](https://github.com/huggingface/diffusers/blob/main/examples/server/server.py)), and provides a peek behind the curtain of inference server deployments without exploring too much of the complexity. \n",
    "\n",
    "**Warning:** FastAPI and server development is not within the scope of this class, and serves as a simplified example for those interested. Feel free to run the service as-is and assume the API will roughly work and operate reliably enough to fulfill the exercises, but we do not recommend using this solution for production. If you are interested in productionalizing an image generation server, consider using:\n",
    "- [Stable Diffusion NIM offerings](https://build.nvidia.com/stabilityai/stable-diffusion-xl) for self-hosted options.\n",
    "- [Automatic1111-style](https://github.com/NVIDIA/Stable-Diffusion-WebUI-TensorRT/tree/main) and [ComfyUI-style](https://github.com/comfyanonymous/ComfyUI_TensorRT) Stable-Diffusion WebUIs for streamlined accelerated deployments. \n",
    "- Hosted APIs like those offered by OpenAI and StabilityAI for non-self-hosted options.\n",
    "- Inference server projects like [vLLM](https://github.com/vllm-project/vllm) for inspirations on productionalized deployments. \n",
    "\n",
    "#### **Note The Following Features:**\n",
    "- **This is sufficient for a background server that mimics OpenAI's Dalle API for a single-user use-case.**\n",
    "- It roughly follows OpenAI's Dalle API with minimal functionality of generating a novel image.\n",
    "- Once the server is running, other services can call it via a port interface without having to reload or re-construct the resource allocated in the server. \n",
    "\n",
    "#### **Note The Following Critical Limitations:**\n",
    "- **It is explicitly not threadsafe!** This is outlined in the documentation narrative, and is important for multi-user use-cases. This server is only sufficient for a single user with relatively little allowed concurrency. \n",
    "- **There is no meaningful model discovery endpoint,** and it is generally non-trivial for an external user to understand how to use this server without digging into the code. This is omitted for brevity, since you have full access to the code.\n",
    "- **The server saves the file directly to a temporary file on the host system.** This is simplified for convenience, and is sufficient for meaningful interactions in a local deployment context.\n",
    "    - [OpenAI](https://platform.openai.com/docs/guides/images) would save such an image to a temporary publicly-accessible address. The caller can download the image and operate on it as usual.\n",
    "    - [NVIDIA NIM microservices](https://docs.api.nvidia.com/nim/reference/stabilityai-stable-diffusion-xl-infer), and thereby `build.nvidia.com`, returns a buffer in its response which can be decoded and interpretted as an image by the caller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77bb2be5-b15b-48cb-a782-88ff5b759830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sdxl_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sdxl_app.py\n",
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import traceback\n",
    "import uuid\n",
    "from contextlib import asynccontextmanager\n",
    "from time import time\n",
    "from typing import Optional\n",
    "\n",
    "import aiohttp\n",
    "import torch\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "#########################################################################################\n",
    "## Configuration and Logging Setup; global settings and checks\n",
    "#########################################################################################\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"TextToImageService\")\n",
    "MODEL_PATH = os.getenv(\"MODEL_PATH\", \"stabilityai/stable-diffusion-xl-base-1.0\")\n",
    "IMAGE_DIR = os.path.join(\"/dli/task/generated_images\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "#########################################################################################\n",
    "## FastAPI Application Setup\n",
    "#########################################################################################\n",
    "app = FastAPI()  # Initialize FastAPI app\n",
    "globals = {      # Shared global resources\n",
    "    \"pipeline\": None,\n",
    "}\n",
    "\n",
    "#########################################################################################\n",
    "## Lifespan Context Manager: Dictates what all happens to construct and tear down server\n",
    "#########################################################################################\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    \"\"\"\n",
    "    Manages application lifespan:\n",
    "    - Initializes shared resources (e.g., HTTP client, model pipeline).\n",
    "    - Cleans up resources on shutdown.\n",
    "    \"\"\"\n",
    "    if DEVICE != \"cuda\":\n",
    "        raise RuntimeError(\"CUDA device is required for this application.\")\n",
    "    os.makedirs(IMAGE_DIR, exist_ok=True)  # Ensure image directory exists\n",
    "    session = aiohttp.ClientSession()\n",
    "    try:\n",
    "        logger.info(\"Loading model on CUDA...\")\n",
    "        globals[\"pipeline\"] = DiffusionPipeline.from_pretrained(\n",
    "            MODEL_PATH, torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\",\n",
    "        ).to(DEVICE)\n",
    "        globals[\"pipeline\"].enable_xformers_memory_efficient_attention()\n",
    "        # Perform a warm-up operation to force memory allocation\n",
    "        dummy_prompt = \"A blank test image\" \n",
    "        generator = torch.Generator(device=DEVICE).manual_seed(0)\n",
    "        _ = globals[\"pipeline\"](dummy_prompt, num_inference_steps=1, generator=generator)\n",
    "        ## Announce server is ready, list basic route info\n",
    "        logger.info(\"Server Ready. Routes:\")\n",
    "        [logger.info(f\" - {r} ({getattr(r, 'description', '')})\") for r in app.routes]\n",
    "        yield  ## Yields scope to allow server to run. On shutdown, will get scope back\n",
    "    finally:\n",
    "        await session.close()\n",
    "        logger.warning(\"Application shutdown complete.\")\n",
    "\n",
    "app.router.lifespan_context = lifespan\n",
    "\n",
    "#########################################################################################\n",
    "## API Endpoints. These functions can be called from any user with access to server port\n",
    "#########################################################################################\n",
    "\n",
    "@app.get(\"/\")\n",
    "@app.post(\"/\")\n",
    "@app.options(\"/\")\n",
    "async def base():\n",
    "    \"\"\"Root endpoint for a simple health check or welcome message.\"\"\"\n",
    "    return {\"message\": \"Welcome to Diffusers! Use this service to generate images from prompts.\"}\n",
    "\n",
    "class TextToImageInput(BaseModel):\n",
    "    \"\"\"Schema for image generation input.\"\"\"\n",
    "    model: str\n",
    "    prompt: str\n",
    "    size: Optional[str] = None\n",
    "    n: Optional[int] = None\n",
    "\n",
    "@app.post(\"/v1/images/generations\")\n",
    "async def generate_image(image_input: TextToImageInput):\n",
    "    \"\"\"Endpoint to generate an image from a given prompt using the shared diffusion pipeline.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"[Request] {image_input}\")\n",
    "        if globals[\"pipeline\"] is None:\n",
    "            raise HTTPException(status_code=500, detail=\"Model pipeline is not initialized\")\n",
    "        generator = torch.Generator(device=DEVICE)\n",
    "        generator.manual_seed(random.randint(0, 10000000))\n",
    "        # Run pipeline in a thread pool to avoid blocking the event loop\n",
    "        loop = asyncio.get_event_loop()\n",
    "        pipe_kws = {**image_input.__dict__, \"generator\": generator}\n",
    "        pipe_kws[\"num_images_per_prompt\"] = pipe_kws.get(\"n\") or 1\n",
    "        start_time = time()\n",
    "        output = await loop.run_in_executor(None, lambda: globals[\"pipeline\"](**pipe_kws))\n",
    "        end_time = time() - start_time\n",
    "        image_urls = [save_image(img) for img in output.images]\n",
    "        logger.info(f\" - Generated {image_urls} over {end_time:.4f}s\")\n",
    "        return {\"data\": [{\"url\": url} for url in image_urls]}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during image generation: {str(e)}\")\n",
    "        trace = f\"{str(e)}\\n{traceback.format_exc()}\"\n",
    "        raise HTTPException(status_code=500, detail=f\"Image generation failed: {trace}\")\n",
    "\n",
    "def save_image(image) -> str:\n",
    "    \"\"\"Saves the generated image to the designated directory and returns its public URL.\"\"\"\n",
    "    filename = f\"draw_{uuid.uuid4().hex[:8]}.png\"\n",
    "    image_path = os.path.join(IMAGE_DIR, filename)\n",
    "    logger.info(f\"Saving image to {image_path}\")\n",
    "    image.save(image_path)\n",
    "    return image_path\n",
    "        \n",
    "#########################################################################################\n",
    "## Standalone Execution for Local Development\n",
    "#########################################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Starting application...\")\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d81e7ead-415c-4379-aea0-1c6b2558573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall -y pynvml\n",
    "# !pip install --upgrade nvidia-ml-py langchain-nvidia-ai-endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e419d76-d1b5-47cc-b72f-75eeba8d98aa",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "## **Part 2:** Getting Pre-Made Standardized Servers Using vLLM\n",
    "\n",
    "For a higher layer of abstraction, many people like to use inference services like [vLLM](https://github.com/vllm-project/vllm) to help deploy scalable and standardized LLM APIs. This framework automatically integrated with the Transformers model repository to download model weights and deserialize from config to pipeline. At the same time, it also offers abstractions for memory management, multi-user deployment, and API standards to wrap a resilient server around the loaded model resource. \n",
    "\n",
    "Using vLLM, we will deploy two small models that can run from within this notebook on our A100 resource:\n",
    "\n",
    "#### **1. A small vision language model ([SmolVLM-Instruct](https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct))**.\n",
    "\n",
    "This model, developed by HuggingFace TB Research Team for small language model use-cases, uses the multimodal fusion principles from Notebook 5 to perform joint reasoning with images and text. At the same time, it adopts the standard OpenAI-style API to largely interoperate with the likes of [**OpenAI's GPT-4o**](https://openai.com/index/hello-gpt-4o/), [**NVIDIA's NVLM**](https://arxiv.org/abs/2409.11402), and the open-sourced [**Llama 3.2 (2024)**](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/).\n",
    "\n",
    "To deploy an inference server, the following command will work in our environment:\n",
    "\n",
    "```sh\n",
    "vllm serve HuggingFaceTB/SmolVLM-Instruct\n",
    "--max_model_len 8192\n",
    "--gpu-memory-utilization 0.5\n",
    "--enforce-eager\n",
    "--max-num-seqs 16\n",
    "--port 9002\n",
    "```\n",
    "\n",
    "#### **2. A small language model with (minimum-viable) tooling support ([Llama-3.2-3B](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct))**.\n",
    "\n",
    "This model, developed by the META AI research team, offers a small language model that has been trained, to some extend, to abide by function calling to some extend. Of note, this model is not usually recommended for function-calling out-of-the-box without some much-needed fine-tuning, but it can be used sufficiently for our simpler use-cases. \n",
    "\n",
    "To deploy an inference server, the following command will work in our environment:\n",
    "\n",
    "```sh\n",
    "vllm serve unsloth/Llama-3.2-3B-Instruct \n",
    "--enable-auto-tool-choice\n",
    "--tool-call-parser llama3_json\n",
    "--chat-template tool_chat_template_llama3.2_json.jinja\n",
    "--max_model_len 8192\n",
    "--gpu-memory-utilization 0.4\n",
    "--max-num-seqs 16\n",
    "--port 9001\n",
    "\n",
    "## NOTE: we're using unsloth secondarily because of some nice optimizations they use, \n",
    "## and primarily just to make it easier for you to load in the model without a HF token.\n",
    "## Note that you are still bound by the llama usage/general license agreement during use:\n",
    "## https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/blob/main/USE_POLICY.md\n",
    "```\n",
    "\n",
    "Note that these parameterizations are not meant to be recommendations, nor are they necessarily always good. Furthermore, there is still opportunities for resource contests and other issues. For productionalization, we highly recommend using NIM, which is a higher layer of abstraction over vLLM and the NVIDIA Triton acceleration framework.\n",
    "\n",
    "\n",
    "\n",
    "<details> \n",
    "    \n",
    "<summary><b>Argument Details</b></summary>\n",
    "\n",
    "- **Port (9001 / 9002):** Defines the specific network port on which the modelâ€™s server listens. This lets other processes communicate with our server and send requests to it.\n",
    "- **Max Model Length (8192):** Sets the upper limit for the number of input tokens the model can process in a single operation, affecting the model's ability to handle longer sequences.\n",
    "    - Impacts resource allocation (since KV cache space needs to be allocated), increases maximum per-call inference time/resource use, and opens up opportunies for derailment. \n",
    "- **GPU Memory Utilization (0.4 / 0.5):** Dictates the percentage of the total available GPU memory that each model instance can use, ensuring that some GPU resources are left over for other processes.\n",
    "    - You may notice that our diffusion model server does not have such a configuration... because this a VLLM-specific parameter which is tightly paired with its engine wrapper. Implementing this for diffusion would need to be done manually.\n",
    "    - You may also note that 0.4 + 0.5 + diffusion resources (.17 usually) > 1. This tends to be ok in our specific context because the models require excess memory during construction. An actual productionalized solution should enforce stronger bounds if resilience is required.\n",
    "- **Max Number of Sequences (16):** Controls how many sequences the model can process simultaneously, optimizing both computational efficiency and response time during parallel requests.\n",
    "- **Enforce Eager:** Forces the model to execute operations immediately and in a sequential order, which simplifies debugging and execution traceability.\n",
    "- **Chat Template:** Implements a Jinja template for standardizing API responses, facilitating consistent and predictable interactions with the model.\n",
    "    - In reality, all LLMs that offer a a chat completions API have this format, which helps to convert an intuitive \"message\" format to \"LLM input\" format. It's important here because the version that is passed in [implemented here and advertised to work well for Llama 3.2](https://github.com/vllm-project/vllm/blob/main/examples/tool_chat_template_llama3.2_json.jinja), defines several non-default edge cases that enable things like function calling. \n",
    "- **Auto Tool Choice:** Employs automated logic to select the appropriate tools for function parsing and execution, adapting to the model's capabilities without manual configuration. This will be explored in more detail in **Notebook 7**\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865723fb-cf9e-419f-9b22-129ca194acd4",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "## **Part 2.5:** BONUS: Wrangling These Servers With A Router\n",
    "\n",
    "Reading more into the OpenAI API standard, you will notice the following server-level requirements: \n",
    "- You should be able to discover what the models accessible to you from the server.\n",
    "- You should be able to communicate with the server in a standard and perdictable method.\n",
    "\n",
    "The is very useful because you can build software around these endpoints that adjusts itself to the available model pool and invokes the endpoints in a predictable fashion. However, we currently have two different endpoints for our two LLM models. This is a bit of a hinderance for calling these endpoints, as we can see in this invocation example:\n",
    "\n",
    "```python\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "import os \n",
    "\n",
    "## Saying Hello World to our model, ChatNVIDIA would auto-infer the name from discovery\n",
    "llm = ChatNVIDIA(model=\"unsloth/Llama-3.2-3B-Instruct\", base_url=\"http://0.0.0.0:9001/v1\")\n",
    "llm = ChatNVIDIA(model=\"HuggingFaceTB/SmolVLM-Instruct\", base_url=\"http://0.0.0.0:9002/v1\")\n",
    "llm = ChatNVIDIA(base_url=\"http://0.0.0.0:9001/v1\")  ## equivalent\n",
    "vlm = ChatNVIDIA(base_url=\"http://0.0.0.0:9002/v1\")  ## equivalent\n",
    "\n",
    "llm.invoke(\"Hello World!\")\n",
    "```\n",
    "\n",
    "Wouldn't it be nicer if we could have them all aggregate under a single server?\n",
    "\n",
    "```python\n",
    "## But wouldn't it be nicer if we could have both models come from the same connector?\n",
    "os.environ[\"NVIDIA_BASE_URL\"] = \"http://0.0.0.0:9004/v1\"\n",
    "llm = ChatNVIDIA(model=\"unsloth/Llama-3.2-3B-Instruct\")  ## equivalent\n",
    "vlm = ChatNVIDIA(model=\"HuggingFaceTB/SmolVLM-Instruct\")  ## equivalent\n",
    "```\n",
    "\n",
    "Below, we define a basic router application which should help with this process. This is merely for convenience, but illustrates how one could aggregate multiple servers under one umbrella. This is, in fact, what `build.nvidia.com` does under the hood, and is a gateway to understand the `llm_client` microservice for those interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfa1f675-2e0b-43b3-add3-f01d358a3905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing router_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile router_app.py\n",
    "import asyncio\n",
    "from contextlib import asynccontextmanager\n",
    "from fastapi import FastAPI, HTTPException, Request\n",
    "from fastapi.responses import Response, StreamingResponse, JSONResponse\n",
    "from functools import partial\n",
    "import httpx\n",
    "import json\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "# Initialize FastAPI application\n",
    "app = FastAPI()\n",
    "\n",
    "###################################################################################\n",
    "## Configuration and Defaults\n",
    "\n",
    "DEFAULTS = {\n",
    "    \"TIMEOUT\": 60,  # Request timeout in seconds\n",
    "    \"URLS\": \"0.0.0.0:9001,0.0.0.0:9002,llm_client:9000\",  # Backend URLs\n",
    "    \"FILTER_KEYWORDS\": \"meta,mistral,nvidia,llama-3.2,huggingfacetb\",  # Filter keywords\n",
    "}\n",
    "\n",
    "def get_var(key: str) -> str:\n",
    "    \"\"\"Fetch a configuration variable, falling back to defaults.\"\"\"\n",
    "    return os.getenv(key, DEFAULTS.get(key, \"\"))\n",
    "\n",
    "def get_urls() -> list:\n",
    "    \"\"\"Parse and normalize backend URLs.\"\"\"\n",
    "    return [url if \"://\" in url else f\"http://{url.strip()}\" for url in get_var(\"URLS\").split(\",\")]\n",
    "\n",
    "def get_filter_keywords() -> list:\n",
    "    \"\"\"Parse filter keywords for model filtering.\"\"\"\n",
    "    return [kw.strip().lower() for kw in get_var(\"FILTER_KEYWORDS\").split(\",\")]\n",
    "\n",
    "###################################################################################\n",
    "## Utilities\n",
    "\n",
    "async def fetch_models_from_server(server_url: str) -> dict:\n",
    "    \"\"\"Fetch available models from a single backend server.\"\"\"\n",
    "    try:\n",
    "        async with httpx.AsyncClient(timeout=int(get_var(\"TIMEOUT\"))) as client:\n",
    "            response = await client.get(f\"{server_url}/v1/models\")\n",
    "            if response.status_code == 200:\n",
    "                return {\"server\": server_url, \"models\": response.json().get(\"data\", [])}\n",
    "            return {\"error\": f\"Failed to fetch models from {server_url} with status {response.status_code}\"}\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return {\"error\": f\"Exception during model discovery from {server_url}: {str(e)}\"}\n",
    "\n",
    "async def discover_models() -> tuple:\n",
    "    \"\"\"\n",
    "    Discover models from all backend servers.\n",
    "    Returns:\n",
    "        - aggregated_models: List of models with metadata.\n",
    "        - errors: List of errors encountered during discovery.\n",
    "    \"\"\"\n",
    "    tasks = [fetch_models_from_server(server) for server in get_urls()]\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "    aggregated_models, errors = [], []\n",
    "    for server, result in zip(get_urls(), results):\n",
    "        if isinstance(result, dict) and \"models\" in result:\n",
    "            # Tag models with their originating server\n",
    "            for model in result[\"models\"]:\n",
    "                model[\"server\"] = server\n",
    "                aggregated_models.append(model)\n",
    "        elif isinstance(result, dict) and \"error\" in result:\n",
    "            errors.append(result[\"error\"])\n",
    "        else:\n",
    "            errors.append(f\"Unexpected result from {server}: {result}\")\n",
    "    return aggregated_models, errors\n",
    "\n",
    "def filter_models(models: list) -> list:\n",
    "    \"\"\"Filter models based on predefined keywords.\"\"\"\n",
    "    keywords = get_filter_keywords()\n",
    "    return [model for model in models if any(kw in model.get(\"id\", \"\").lower() for kw in keywords)]\n",
    "\n",
    "###################################################################################\n",
    "## Endpoints\n",
    "\n",
    "@app.get(\"/v1/models\")\n",
    "async def list_models():\n",
    "    \"\"\"List all available models from all backend servers.\"\"\"\n",
    "    try:\n",
    "        models, errors = await discover_models()\n",
    "        filtered_models = filter_models(models)\n",
    "        response = {\"object\": \"list\", \"data\": filtered_models}\n",
    "        if errors:\n",
    "            response[\"warnings\"] = errors\n",
    "        return JSONResponse(content=response)\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        raise HTTPException(status_code=500, detail=f\"Exception during model discovery: {str(e)}\")\n",
    "\n",
    "Client = partial(httpx.AsyncClient, timeout=httpx.Timeout(60))\n",
    "\n",
    "async def create_completion_base(request: Request, extension: str):\n",
    "    \"\"\"Forwards chat completion requests to OpenAPI endpoint.\"\"\"\n",
    "    try:\n",
    "        # Parse incoming request content\n",
    "        content = await request.body()\n",
    "        content = json.loads(content.decode())\n",
    "        model = content.get(\"model\")\n",
    "        stream = content.get(\"stream\", False)\n",
    "\n",
    "        # Prepare headers for the outgoing request\n",
    "        headers = {\n",
    "            key: value for key, value in request.headers.items()\n",
    "            if key.lower() not in [\"host\", \"content-length\"]\n",
    "        }\n",
    "\n",
    "        # Discover the target endpoint based on the requested model\n",
    "        models, _ = await discover_models()\n",
    "        model_entry = next((m for m in models if m[\"id\"] == model), None)\n",
    "        if not model_entry:\n",
    "            raise HTTPException(status_code=404, detail=f\"Model '{model}' not found.\")\n",
    "        target_server = model_entry[\"server\"]\n",
    "\n",
    "        # Prepare the request parameters\n",
    "        call_kws = {\n",
    "            \"url\": f\"{target_server}/v1/{extension}\",\n",
    "            \"headers\": headers,\n",
    "            \"data\": json.dumps(content).encode()\n",
    "        }\n",
    "\n",
    "        ############################################################\n",
    "        # Handle Non-Streaming Use Case\n",
    "        if not stream:\n",
    "            try: \n",
    "                async with Client() as client:\n",
    "                    response = await client.post(**call_kws)\n",
    "            except httpx.TimeoutException as e:\n",
    "                raise HTTPException(status_code=408)\n",
    "\n",
    "            filtered_headers = {\n",
    "                key: value for key, value in response.headers.items() \n",
    "                if key.lower() not in [\"content-length\", \"content-encoding\", \"transfer-encoding\"]\n",
    "            }\n",
    "\n",
    "            return Response(content=response.content, status_code=response.status_code, headers=filtered_headers)\n",
    "\n",
    "        ############################################################\n",
    "        ## Simple Use Case: Streaming\n",
    "        ## Create a generator to keep querying the response endpoint after initial response\n",
    "        ## NOTE: This is a weird way for keeping stream client open both for a potential\n",
    "        ##  initial exception raise and also as an argument to the StreamingResponse return.\n",
    "        async def respond_and_stream():\n",
    "            try: \n",
    "                async with Client().stream(\"POST\", **call_kws) as response:\n",
    "                    yield response\n",
    "                    ## This stage only gets invoked it the response is valid + streaming is enabled\n",
    "                    agen = response.aiter_bytes()\n",
    "                    async for cbytes in agen:\n",
    "                        yield cbytes\n",
    "            except httpx.TimeoutException as e:\n",
    "                raise HTTPException(status_code=408)\n",
    "    \n",
    "        ## Create response generator and process initial response\n",
    "        agen = respond_and_stream()\n",
    "        response = await agen.__anext__()\n",
    "        if response.status_code != 200:\n",
    "            response_bytes = await response.aread()\n",
    "            content = response_bytes\n",
    "            filtered_headers = {\n",
    "                key: value for key, value in response.headers.items() \n",
    "                if key.lower() not in [\"content-length\", \"content-encoding\", \"transfer-encoding\"]\n",
    "            }\n",
    "            return Response(content=content, status_code=response.status_code, headers=filtered_headers)\n",
    "        else: \n",
    "            return StreamingResponse(agen, media_type='text/event-stream')\n",
    "        return StreamingResponse(respond_and_stream(), media_type=\"text/event-stream\")\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        raise HTTPException(status_code=400, detail=\"Invalid JSON in request body.\")\n",
    "    except HTTPException as e:\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"An error occurred while processing the request: {str(e)}\"\n",
    "        )\n",
    "\n",
    "\n",
    "@app.post(\"/v1/{path:path}\")\n",
    "async def handle_request(request: Request, path: str):\n",
    "    \"\"\"Forwards requests based on the path to the appropriate OpenAPI endpoint.\"\"\"\n",
    "    return await create_completion_base(request, extension=path)\n",
    "\n",
    "###################################################################################\n",
    "## Health Check\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Simple health check endpoint.\"\"\"\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "###################################################################################\n",
    "## Application Lifecycle\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    \"\"\"Manage application startup and shutdown events.\"\"\"\n",
    "    print(f\"Configured backend servers: {get_urls()}\")\n",
    "    print(f\"Model filter keywords: {get_filter_keywords()}\")\n",
    "    yield\n",
    "    print(\"Application shutdown complete.\")\n",
    "\n",
    "app.router.lifespan_context = lifespan\n",
    "\n",
    "###################################################################################\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe642b96-4fa9-42ed-9fb4-c3cc4d2abdd1",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "## **Part 3**: Kickstarting The Servers In Our Notebook\n",
    "\n",
    "We've now explained how these servers are constructed, both behind the scenes and at a surface-level. **This is an advanced topic, so it's ok if this doesn't quite stick.** What's important is that you understand that:\n",
    "- **It's possible to kickstart many LLM servers that can be used by one or many users.**\n",
    "- **These servers can prevent resources from being allocated multiple times over.**\n",
    "- **For good compatability, these servers usually follow a standard or easy-to-use API.**\n",
    "\n",
    "We will need these features through the rest of the course, so please run the code cell below to kickstart your environment. This will kickstart each of your models, one at a time, until :9003 gets occupied with your Llama 3.2 model deployment. Once again, this process is documented here for your own reference, and is not necessary to understand as part of the course learning objectives.\n",
    "\n",
    "**NOTE:** \n",
    "- We recommend shutting down previous kernels to free up all of your resources for this action. \n",
    "- If you encounter an issue of freeze-up, please interrupt the kernel (square button) and try again.\n",
    "- This command will produce a running log. For that reason, there is no text below the following cell. Please return back to **Notebook 6** once you're done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79341214-6598-4d25-b09b-a6921bb32714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Processes\n",
      "\n",
      "STARTING \u001b[32m[Llama3B]\u001b[0m\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:12 api_server.py:651] vLLM API server version 0.6.5\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:12 api_server.py:652] args: Namespace(subparser='serve', model_tag='unsloth/Llama-3.2-3B-Instruct', config='', host=None, port=9001, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template='tool_chat_template_llama3.2_json.jinja', chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=True, tool_call_parser='llama3_json', tool_parser_plugin='', model='unsloth/Llama-3.2-3B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=8192, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.35, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, mm_cache_preprocessor=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7fe28d74e4d0>)\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:12 api_server.py:199] Started engine process with PID 3689\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:19 config.py:478] This model supports multiple tasks: {'score', 'classify', 'generate', 'embed', 'reward'}. Defaulting to 'generate'.\n",
      "\u001b[32m[Llama3B]\u001b[0m WARNING 05-27 14:46:19 cuda.py:98] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "\u001b[32m[Llama3B]\u001b[0m WARNING 05-27 14:46:19 config.py:604] Async output processing is not supported on the current platform type cuda.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:23 config.py:478] This model supports multiple tasks: {'score', 'reward', 'embed', 'classify', 'generate'}. Defaulting to 'generate'.\n",
      "\u001b[32m[Llama3B]\u001b[0m WARNING 05-27 14:46:23 cuda.py:98] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "\u001b[32m[Llama3B]\u001b[0m WARNING 05-27 14:46:23 config.py:604] Async output processing is not supported on the current platform type cuda.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:23 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='unsloth/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='unsloth/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=True,\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:25 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:25 model_runner.py:1092] Starting to load model unsloth/Llama-3.2-3B-Instruct...\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:25 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[31m[Llama3B-ERR]\u001b[0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "\u001b[31m[Llama3B-ERR]\u001b[0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.30it/s]\n",
      "\u001b[31m[Llama3B-ERR]\u001b[0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.33it/s]\n",
      "\u001b[31m[Llama3B-ERR]\u001b[0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.12it/s]\n",
      "\u001b[31m[Llama3B-ERR]\u001b[0m \n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:42 model_runner.py:1097] Loading model weights took 6.0160 GB\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:43 worker.py:241] Memory profiling takes 0.56 seconds\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:43 worker.py:241] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.35) = 27.70GiB\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:43 worker.py:241] model weights take 6.02GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 1.22GiB; the rest of the memory reserved for KV Cache is 20.35GiB.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:43 gpu_executor.py:76] # GPU blocks: 11909, # CPU blocks: 2340\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:43 gpu_executor.py:80] Maximum concurrency for 8192 tokens per request: 23.26x\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 llm_engine.py:446] init engine (profile, create kv cache, warmup model) took 2.73 seconds\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] Using supplied chat template:\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {{- bos_token }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {%- if custom_tools is defined %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {%- set tools = custom_tools %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {%- endif %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {%- if not tools_in_user_message is defined %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {%- set tools_in_user_message = false %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {%- endif %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {%- if not date_string is defined %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {%- if strftime_now is defined %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {%- else %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {%- set date_string = \"26 Jul 2024\" %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {%- endif %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {%- endif %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {%- if not tools is defined %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {%- set tools = none %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {%- endif %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {#- Find out if there are any images #}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {% set image_ns = namespace(has_images=false) %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {%- for message in messages %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {%- for content in message['content'] %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {%- if content['type'] == 'image' %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]             {%- set image_ns.has_images = true %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {%- endif %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {%- endfor %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {%- endfor %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {%- if messages[0]['role'] == 'system' %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {%- if messages[0]['content'] is string %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {%- set system_message = messages[0]['content']|trim %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {%- else %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {#- Support vLLM's transforming of a content string to JSON. #}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {%- set system_message = messages[0]['content'][0]['text']|trim %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {%- endif %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {%- set messages = messages[1:] %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {%- else %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {%- if tools is not none %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {%- set system_message = \"You are a helpful assistant with tool calling capabilities. Only reply with a tool call if the function exists in the library provided by the user. If it doesn't exist, just reply directly in natural language. When you receive a tool call response, use the output to format an answer to the original user question.\" %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {%- else %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {%- set system_message = \"\" %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {%- endif %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {%- endif %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {#- Including an image is not compatible with a system message #}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {%- if image_ns.has_images and not system_message == \"\" %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {{- raise_exception(\"Prompting with images is incompatible with system messages and tool use.\") }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {%- endif %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {#- System message, if there are no images #}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {%- if not image_ns.has_images %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {%- if tools is not none %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {{- \"Environment: ipython\\n\" }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {%- endif %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {%- if tools is not none and not tools_in_user_message %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call. \" }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}. ' }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {{- \"Do not use variables.\\n\\n\" }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {%- for t in tools %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]             {{- t | tojson(indent=4) }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]             {{- \"\\n\\n\" }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {%- endfor %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {%- endif %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {{- system_message }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {{- \"<|eot_id|>\" }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {%- endif %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {#- Custom tools are passed in a user message with some extra guidance #}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {%- if tools_in_user_message and not tools is none %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {#- Extract the first user message so we can plug it in here #}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {%- if messages | length != 0 %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {%- if messages[0]['content'] is string %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]             {%- set first_user_message = messages[0]['content']|trim %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {%- else %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]             {%- set first_user_message = messages[0]['content'] | selectattr('type', 'equalto', 'text') | map(attribute='text') | map('trim') | join('\\n') %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {%- endif %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {%- set messages = messages[1:] %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {%- else %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {%- endif %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}. ' }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {{- \"Do not use variables.\\n\\n\" }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {%- for t in tools %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {{- t | tojson(indent=4) }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {{- \"\\n\\n\" }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {%- endfor %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {{- first_user_message + \"<|eot_id|>\"}}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {%- endif %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {%- for message in messages %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {%- if message['content'] is string %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]             {{- message['content'] | trim}}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {%- else %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]             {%- for content in message['content'] %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]                 {%- if content['type'] == 'image' %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]                     {{- '<|image|>' }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]                 {%- elif content['type'] == 'text' %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]                     {{- content['text'] | trim }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]                 {%- endif %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]             {%- endfor %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {%- endif %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {{- '<|eot_id|>' }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {%- elif 'tool_calls' in message %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {%- if not message.tool_calls|length == 1 %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]             {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {%- endif %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {%- set tool_call = message.tool_calls[0].function %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {{- '\"parameters\": ' }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {{- tool_call.arguments | tojson }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {{- \"}\" }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {{- \"<|eot_id|>\" }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {%- if message.content is string %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]             {{- { \"output\": message.content } | tojson }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {%- else %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]             {%- for content in message['content']  %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]                 {%- if content['type']  == 'text' %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]                     {{- { \"output\": content['text']  } | tojson }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]                 {%- endif %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]             {%- endfor %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {%- endif %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]         {{- \"<|eot_id|>\" }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {%- endif %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {%- endfor %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {%- if add_generation_prompt %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]     {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586] {%- endif %}\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 api_server.py:586]\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 serving_chat.py:73] \"auto\" tool choice has been enabled please note that while the parallel_tool_calls client option is preset for compatibility reasons, it will be ignored.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 launcher.py:19] Available routes are:\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 launcher.py:27] Route: /docs, Methods: GET, HEAD\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 launcher.py:27] Route: /redoc, Methods: GET, HEAD\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 launcher.py:27] Route: /health, Methods: GET\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 launcher.py:27] Route: /version, Methods: GET\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 launcher.py:27] Route: /v1/embeddings, Methods: POST\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 launcher.py:27] Route: /score, Methods: POST\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:46:45 launcher.py:27] Route: /v1/score, Methods: POST\n",
      "\u001b[31m[Llama3B-ERR]\u001b[0m INFO:     Started server process [3634]\n",
      "\u001b[31m[Llama3B-ERR]\u001b[0m INFO:     Waiting for application startup.\n",
      "\u001b[31m[Llama3B-ERR]\u001b[0m INFO:     Application startup complete.\n",
      "\u001b[31m[Llama3B-ERR]\u001b[0m INFO:     Uvicorn running on http://0.0.0.0:9001 (Press CTRL+C to quit)\n",
      "\n",
      "STARTING \u001b[34m[SmolVLM]\u001b[0m\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:46:50 api_server.py:651] vLLM API server version 0.6.5\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:46:50 api_server.py:652] args: Namespace(subparser='serve', model_tag='HuggingFaceTB/SmolVLM-Instruct', config='', host=None, port=9002, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='HuggingFaceTB/SmolVLM-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=8192, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.35, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=16, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, mm_cache_preprocessor=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, dispatch_function=<function serve at 0x7f495db064d0>)\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:46:50 api_server.py:199] Started engine process with PID 4545\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:46:56 config.py:478] This model supports multiple tasks: {'embed', 'score', 'reward', 'generate', 'classify'}. Defaulting to 'generate'.\n",
      "\u001b[34m[SmolVLM]\u001b[0m WARNING 05-27 14:46:56 cuda.py:98] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "\u001b[34m[SmolVLM]\u001b[0m WARNING 05-27 14:46:56 config.py:604] Async output processing is not supported on the current platform type cuda.\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:00 config.py:478] This model supports multiple tasks: {'embed', 'score', 'reward', 'generate', 'classify'}. Defaulting to 'generate'.\n",
      "\u001b[34m[SmolVLM]\u001b[0m WARNING 05-27 14:47:00 cuda.py:98] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "\u001b[34m[SmolVLM]\u001b[0m WARNING 05-27 14:47:00 config.py:604] Async output processing is not supported on the current platform type cuda.\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:00 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='HuggingFaceTB/SmolVLM-Instruct', speculative_config=None, tokenizer='HuggingFaceTB/SmolVLM-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=HuggingFaceTB/SmolVLM-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=True,\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:01 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:02 model_runner.py:1092] Starting to load model HuggingFaceTB/SmolVLM-Instruct...\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:02 selector.py:249] Cannot use FlashAttention-2 backend for head size 72.\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:02 selector.py:129] Using XFormers backend.\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:02 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:06 weight_utils.py:288] No model.safetensors.index.json found in remote.\n",
      "\u001b[31m[SmolVLM-ERR]\u001b[0m Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[31m[SmolVLM-ERR]\u001b[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.34it/s]\n",
      "\u001b[31m[SmolVLM-ERR]\u001b[0m Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.34it/s]\n",
      "\u001b[31m[SmolVLM-ERR]\u001b[0m \n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:06 model_runner.py:1097] Loading model weights took 4.1976 GB\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:09 worker.py:241] Memory profiling takes 2.75 seconds\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:09 worker.py:241] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.35) = 27.70GiB\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:09 worker.py:241] model weights take 4.20GiB; non_torch_memory takes 0.19GiB; PyTorch activation peak memory takes 1.93GiB; the rest of the memory reserved for KV Cache is 21.38GiB.\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:09 gpu_executor.py:76] # GPU blocks: 7298, # CPU blocks: 1365\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:09 gpu_executor.py:80] Maximum concurrency for 8192 tokens per request: 14.25x\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:11 llm_engine.py:446] init engine (profile, create kv cache, warmup model) took 4.61 seconds\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:11 api_server.py:586] Using supplied chat template:\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:11 api_server.py:586] None\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:11 launcher.py:19] Available routes are:\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:11 launcher.py:27] Route: /openapi.json, Methods: HEAD, GET\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:11 launcher.py:27] Route: /docs, Methods: HEAD, GET\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:11 launcher.py:27] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:11 launcher.py:27] Route: /redoc, Methods: HEAD, GET\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:11 launcher.py:27] Route: /health, Methods: GET\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:11 launcher.py:27] Route: /tokenize, Methods: POST\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:11 launcher.py:27] Route: /detokenize, Methods: POST\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:11 launcher.py:27] Route: /v1/models, Methods: GET\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:11 launcher.py:27] Route: /version, Methods: GET\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:11 launcher.py:27] Route: /v1/chat/completions, Methods: POST\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:11 launcher.py:27] Route: /v1/completions, Methods: POST\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:11 launcher.py:27] Route: /v1/embeddings, Methods: POST\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:11 launcher.py:27] Route: /score, Methods: POST\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:47:11 launcher.py:27] Route: /v1/score, Methods: POST\n",
      "\u001b[31m[SmolVLM-ERR]\u001b[0m INFO:     Started server process [4463]\n",
      "\u001b[31m[SmolVLM-ERR]\u001b[0m INFO:     Waiting for application startup.\n",
      "\u001b[31m[SmolVLM-ERR]\u001b[0m INFO:     Application startup complete.\n",
      "\u001b[31m[SmolVLM-ERR]\u001b[0m INFO:     Uvicorn running on http://0.0.0.0:9002 (Press CTRL+C to quit)\n",
      "\n",
      "STARTING \u001b[36m[SDXL1.0]\u001b[0m\n",
      "\u001b[36m[SDXL1.0]\u001b[0m INFO:     Started server process [4940]\n",
      "\u001b[36m[SDXL1.0]\u001b[0m INFO:     Waiting for application startup.\n",
      "\u001b[36m[SDXL1.0]\u001b[0m INFO:TextToImageService:Loading model on CUDA...\n",
      "Loading pipeline components...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00,  8.47it/s]0<?, ?it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.69it/s]?, ?it/s]\n",
      "\u001b[36m[SDXL1.0]\u001b[0m INFO:TextToImageService:Server Ready. Routes:\n",
      "\u001b[36m[SDXL1.0]\u001b[0m INFO:TextToImageService: - Route(path='/openapi.json', name='openapi', methods=['GET', 'HEAD']) ()\n",
      "\u001b[36m[SDXL1.0]\u001b[0m INFO:TextToImageService: - Route(path='/docs', name='swagger_ui_html', methods=['GET', 'HEAD']) ()\n",
      "\u001b[36m[SDXL1.0]\u001b[0m INFO:TextToImageService: - Route(path='/docs/oauth2-redirect', name='swagger_ui_redirect', methods=['GET', 'HEAD']) ()\n",
      "\u001b[36m[SDXL1.0]\u001b[0m INFO:TextToImageService: - Route(path='/redoc', name='redoc_html', methods=['GET', 'HEAD']) ()\n",
      "\u001b[36m[SDXL1.0]\u001b[0m INFO:TextToImageService: - APIRoute(path='/', name='base', methods=['OPTIONS']) (Root endpoint for a simple health check or welcome message.)\n",
      "\u001b[36m[SDXL1.0]\u001b[0m INFO:TextToImageService: - APIRoute(path='/', name='base', methods=['POST']) (Root endpoint for a simple health check or welcome message.)\n",
      "\u001b[36m[SDXL1.0]\u001b[0m INFO:TextToImageService: - APIRoute(path='/', name='base', methods=['GET']) (Root endpoint for a simple health check or welcome message.)\n",
      "\u001b[36m[SDXL1.0]\u001b[0m INFO:TextToImageService: - APIRoute(path='/v1/images/generations', name='generate_image', methods=['POST']) (Endpoint to generate an image from a given prompt using the shared diffusion pipeline.)\n",
      "\u001b[36m[SDXL1.0]\u001b[0m INFO:     Application startup complete.\n",
      "\u001b[36m[SDXL1.0]\u001b[0m INFO:     Uvicorn running on http://0.0.0.0:9003 (Press CTRL+C to quit)\n",
      "\n",
      "STARTING \u001b[35m[ROUTER]\u001b[0m\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     Started server process [4999]\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     Waiting for application startup.\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     Application startup complete.\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     Uvicorn running on http://0.0.0.0:9004 (Press CTRL+C to quit)\n",
      "\u001b[36m[SDXL1.0]\u001b[0m INFO:TextToImageService:[Request] model='dall-e-3' prompt='a white siamese cat' size='512x512' n=4\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:21<00:00,  2.37it/s], ?it/s]\n",
      "\u001b[36m[SDXL1.0]\u001b[0m INFO:TextToImageService:Saving image to /dli/task/generated_images/draw_df4dc40f.png\n",
      "\u001b[36m[SDXL1.0]\u001b[0m INFO:TextToImageService:Saving image to /dli/task/generated_images/draw_13fcade3.png\n",
      "\u001b[36m[SDXL1.0]\u001b[0m INFO:TextToImageService:Saving image to /dli/task/generated_images/draw_f6a5e5cb.png\n",
      "\u001b[36m[SDXL1.0]\u001b[0m INFO:TextToImageService:Saving image to /dli/task/generated_images/draw_66f64a7e.png\n",
      "\u001b[36m[SDXL1.0]\u001b[0m INFO:TextToImageService: - Generated ['/dli/task/generated_images/draw_df4dc40f.png', '/dli/task/generated_images/draw_13fcade3.png', '/dli/task/generated_images/draw_f6a5e5cb.png', '/dli/task/generated_images/draw_66f64a7e.png'] over 22.3439s\n",
      "\u001b[36m[SDXL1.0]\u001b[0m INFO:     127.0.0.1:54540 - \"POST /v1/images/generations HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:39164 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:39178 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:35734 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:48:31 logger.py:37] Received request cmpl-42b117166eee4facb05419efd1c49e5f-0: prompt: \"Hello! How's it going\", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 9906, 0, 2650, 596, 433, 2133], lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:48:31 engine.py:267] Added request cmpl-42b117166eee4facb05419efd1c49e5f-0.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:48:31 metrics.py:467] Avg prompt throughput: 1.2 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:48:36 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 74.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:35746 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m Configured backend servers: ['http://0.0.0.0:9001', 'http://0.0.0.0:9002', 'http://llm_client:9000']\n",
      "\u001b[35m[ROUTER]\u001b[0m Model filter keywords: ['meta', 'mistral', 'nvidia', 'llama-3.2', 'huggingfacetb']\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:34016 - \"POST /v1/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:52418 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:41246 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:48:39 chat_utils.py:333] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:48:39 logger.py:37] Received request chatcmpl-acfddbc2e3264ceba4b1c4d6167acf9a: prompt: \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHello! How's it going<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:48:39 engine.py:267] Added request chatcmpl-acfddbc2e3264ceba4b1c4d6167acf9a.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:52428 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:60950 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:48:50 metrics.py:467] Avg prompt throughput: 2.9 tokens/s, Avg generation throughput: 18.8 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:49850 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:44124 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:48:59 logger.py:37] Received request chatcmpl-3f6b5e753c06494cb537867eb933284f: prompt: \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHello! How's it going<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:48:59 engine.py:267] Added request chatcmpl-3f6b5e753c06494cb537867eb933284f.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:48:59 metrics.py:467] Avg prompt throughput: 4.4 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:49856 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:54412 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:49864 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:44130 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:49:04 chat_utils.py:333] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:49:04 logger.py:37] Received request chatcmpl-f8fdc839d2d547bcbb7de1249aca6465: prompt: '<|im_start|>User: How did this happen?<end_of_utterance>\\nAssistant:', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:49:04 engine.py:267] Added request chatcmpl-f8fdc839d2d547bcbb7de1249aca6465.\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:44134 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:54428 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:35848 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:35442 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:10 logger.py:37] Received request chatcmpl-76715dc154b4478b9eb6854c05b8beb2: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about birds! A few sentences please.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:35860 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:10 engine.py:267] Added request chatcmpl-76715dc154b4478b9eb6854c05b8beb2.\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:53872 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:10 metrics.py:467] Avg prompt throughput: 4.3 tokens/s, Avg generation throughput: 2.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:35452 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:35456 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:35864 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:35472 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:35878 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:35880 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:35488 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:35890 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:12 logger.py:37] Received request chatcmpl-bf2c1fcad0004a22a490743e01f4f0f6: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about birds! 1 paragraph please.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=300, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:35906 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:12 engine.py:267] Added request chatcmpl-bf2c1fcad0004a22a490743e01f4f0f6.\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:53888 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:12 logger.py:37] Received request chatcmpl-d824079a92d54779a8df11046eccb12a: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about fish! One sentence please.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=100, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:35918 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:12 logger.py:37] Received request chatcmpl-3133f5e49c2b4b4ab1cd73c038bb1fe8: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about cats! 5 paragraphs please.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4096, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:53894 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:35926 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:12 logger.py:37] Received request chatcmpl-350812dfa6cf4afbaae7812944b72a19: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about dogs! 3 paragraphs please.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=500, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:53914 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:35940 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:53904 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:12 engine.py:267] Added request chatcmpl-d824079a92d54779a8df11046eccb12a.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:12 engine.py:267] Added request chatcmpl-3133f5e49c2b4b4ab1cd73c038bb1fe8.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:12 engine.py:267] Added request chatcmpl-350812dfa6cf4afbaae7812944b72a19.\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:49:14 metrics.py:467] Avg prompt throughput: 1.0 tokens/s, Avg generation throughput: 3.3 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:15 metrics.py:467] Avg prompt throughput: 35.7 tokens/s, Avg generation throughput: 131.4 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47322 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49226 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49242 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47330 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49256 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49266 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47340 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47350 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47352 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49274 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49284 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47354 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47368 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49296 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47384 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49302 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47390 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49316 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47400 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49320 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47416 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49330 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47424 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49332 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49346 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47434 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47440 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49354 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47456 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49360 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47464 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49366 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47478 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49374 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49382 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47490 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:21 logger.py:37] Received request chatcmpl-62384e8a6dbe45b99e284cd69bf44139: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about history! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:21 engine.py:267] Added request chatcmpl-62384e8a6dbe45b99e284cd69bf44139.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:21 metrics.py:467] Avg prompt throughput: 6.7 tokens/s, Avg generation throughput: 57.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47496 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49388 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47502 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49396 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47510 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49410 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49414 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47542 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 logger.py:37] Received request chatcmpl-40dfc76fa6574d429befed0c8783a165: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about oceans! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 logger.py:37] Received request chatcmpl-be43ef2939e943dfb40ebe1095059174: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about poetry! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 engine.py:267] Added request chatcmpl-40dfc76fa6574d429befed0c8783a165.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 engine.py:267] Added request chatcmpl-be43ef2939e943dfb40ebe1095059174.\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49422 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47586 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49438 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47612 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 logger.py:37] Received request chatcmpl-f0a629c6fd06420ba8d6d4f4afd84cd2: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about cats! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 logger.py:37] Received request chatcmpl-93d7fc4d171140b8aab98ee65e71337b: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about birds! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 logger.py:37] Received request chatcmpl-1a27da8be1544bb6a35890849618db50: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about mountains! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 logger.py:37] Received request chatcmpl-7ac04a5d638d45e2a8587df3a9704c8b: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about philosophy! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 logger.py:37] Received request chatcmpl-ceff9744f0b944a48b82e170a8ae8913: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about clouds! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 engine.py:267] Added request chatcmpl-f0a629c6fd06420ba8d6d4f4afd84cd2.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 engine.py:267] Added request chatcmpl-93d7fc4d171140b8aab98ee65e71337b.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 engine.py:267] Added request chatcmpl-1a27da8be1544bb6a35890849618db50.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 engine.py:267] Added request chatcmpl-7ac04a5d638d45e2a8587df3a9704c8b.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 engine.py:267] Added request chatcmpl-ceff9744f0b944a48b82e170a8ae8913.\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49440 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47622 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 logger.py:37] Received request chatcmpl-80cccb2cbad2488ba0c67a92a79bac4b: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about love! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 engine.py:267] Added request chatcmpl-80cccb2cbad2488ba0c67a92a79bac4b.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47628 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49446 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49450 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47662 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49456 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47688 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 logger.py:37] Received request chatcmpl-7796d30e13284f1eba7e8b16a69c2984: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about fear! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 logger.py:37] Received request chatcmpl-802efc13d2db4b0bad063c0c113a6bcd: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about lizards! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 logger.py:37] Received request chatcmpl-8933a73fd20a4ca9815aa02312f9e9e1: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about dogs! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 logger.py:37] Received request chatcmpl-17c43fa7b40b4e2b9fa81b66a447afd8: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about rain! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 logger.py:37] Received request chatcmpl-a1cda489701b4a1f8181323002fd470d: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about sunshine! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 logger.py:37] Received request chatcmpl-1bae20e9d99646d289280f2b90c2ddb6: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about adventure! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 logger.py:37] Received request chatcmpl-29a71b40890d47ad994cda0479ac706e: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about hamsters! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 logger.py:37] Received request chatcmpl-4af72cbdf1ea4dd19f4c0a2fdfb8315e: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about dragons! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 engine.py:267] Added request chatcmpl-7796d30e13284f1eba7e8b16a69c2984.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 engine.py:267] Added request chatcmpl-802efc13d2db4b0bad063c0c113a6bcd.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 engine.py:267] Added request chatcmpl-8933a73fd20a4ca9815aa02312f9e9e1.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 engine.py:267] Added request chatcmpl-17c43fa7b40b4e2b9fa81b66a447afd8.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 engine.py:267] Added request chatcmpl-a1cda489701b4a1f8181323002fd470d.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 engine.py:267] Added request chatcmpl-1bae20e9d99646d289280f2b90c2ddb6.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 engine.py:267] Added request chatcmpl-29a71b40890d47ad994cda0479ac706e.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 engine.py:267] Added request chatcmpl-4af72cbdf1ea4dd19f4c0a2fdfb8315e.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 logger.py:37] Received request chatcmpl-c05a7e5735124df7a67b0d0919471aa9: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about solitude! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 logger.py:37] Received request chatcmpl-908b29ad5232481b88e9d457888190a5: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about snow! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 engine.py:267] Added request chatcmpl-c05a7e5735124df7a67b0d0919471aa9.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:22 engine.py:267] Added request chatcmpl-908b29ad5232481b88e9d457888190a5.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:23 logger.py:37] Received request chatcmpl-3a25c8af33a4478c9e912a65497916f0: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about ice! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:23 logger.py:37] Received request chatcmpl-91b1a750c21845318d9f31b9862053a5: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about fireworks! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:23 logger.py:37] Received request chatcmpl-d4912164de864a579c8f84072dc41724: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about friendship! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:23 logger.py:37] Received request chatcmpl-134c1d8f6c704e4fa230df4c905db7d2: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about GPUs! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:23 logger.py:37] Received request chatcmpl-f6db54de4da2478faaa3a62a85d2fe8b: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about trees! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:23 logger.py:37] Received request chatcmpl-f1ddc67df1c649feafbfeaca933b5fec: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about order! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:23 logger.py:37] Received request chatcmpl-d442fce338b74d288e64e3cbb914a646: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about flowers! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:23 logger.py:37] Received request chatcmpl-177e61a251cd4f70869781f1e66f1006: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about chaos! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:23 logger.py:37] Received request chatcmpl-37a75b19788c41778a31f2dff1aa3e3b: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about happiness! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:23 engine.py:267] Added request chatcmpl-3a25c8af33a4478c9e912a65497916f0.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:23 engine.py:267] Added request chatcmpl-91b1a750c21845318d9f31b9862053a5.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:23 engine.py:267] Added request chatcmpl-d4912164de864a579c8f84072dc41724.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:23 engine.py:267] Added request chatcmpl-134c1d8f6c704e4fa230df4c905db7d2.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:23 engine.py:267] Added request chatcmpl-f6db54de4da2478faaa3a62a85d2fe8b.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:23 engine.py:267] Added request chatcmpl-f1ddc67df1c649feafbfeaca933b5fec.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:23 engine.py:267] Added request chatcmpl-d442fce338b74d288e64e3cbb914a646.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:23 engine.py:267] Added request chatcmpl-177e61a251cd4f70869781f1e66f1006.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:23 engine.py:267] Added request chatcmpl-37a75b19788c41778a31f2dff1aa3e3b.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47482 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43588 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47824 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49466 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47620 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47726 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43750 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43680 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47530 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47696 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47604 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47706 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47516 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47692 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47556 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47572 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47582 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47590 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47640 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47652 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47672 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47722 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43626 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43728 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43730 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43674 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43602 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43724 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43650 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43658 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43636 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43614 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43690 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43704 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43736 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43710 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47828 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47834 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49482 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49484 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47744 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47738 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43778 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO 05-27 14:49:24 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47748 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43764 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47786 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47802 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47770 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47762 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47768 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47780 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47792 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47808 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43784 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:24 logger.py:37] Received request chatcmpl-ec5e7381bb7940eca96ccd8f3999a7e9: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about rivers! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:24 engine.py:267] Added request chatcmpl-ec5e7381bb7940eca96ccd8f3999a7e9.\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43884 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43842 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43822 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49492 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47858 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43790 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43828 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43806 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43856 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43872 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47864 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49502 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47878 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49518 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49530 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47888 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47900 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49544 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47914 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49550 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49564 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47926 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49568 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47942 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47948 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49576 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49586 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47974 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49596 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47978 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:25 logger.py:37] Received request chatcmpl-9f71455aa5804e1daed93cebcf7dab9e: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about energy! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:25 logger.py:37] Received request chatcmpl-561f734248f342e19a1c09a4c5801c40: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about sadness! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:25 engine.py:267] Added request chatcmpl-9f71455aa5804e1daed93cebcf7dab9e.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:25 engine.py:267] Added request chatcmpl-561f734248f342e19a1c09a4c5801c40.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47992 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49598 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47994 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49602 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:48004 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49614 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:48018 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49630 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:48024 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49632 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49636 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:48046 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:49640 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:48060 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 logger.py:37] Received request chatcmpl-93ba6c928a814a05972b70e608551063: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about lakes! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 logger.py:37] Received request chatcmpl-fd4dc010d0414ba78fbbbea5dc39e13f: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about space! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 logger.py:37] Received request chatcmpl-6d2b391e679b47b1a96825867e668ca8: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about forests! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 engine.py:267] Added request chatcmpl-93ba6c928a814a05972b70e608551063.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 engine.py:267] Added request chatcmpl-fd4dc010d0414ba78fbbbea5dc39e13f.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 engine.py:267] Added request chatcmpl-6d2b391e679b47b1a96825867e668ca8.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 logger.py:37] Received request chatcmpl-3d8aa644c0a54f9fb27bd69e9be4e1ae: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about time! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 logger.py:37] Received request chatcmpl-c16514f6518b4f47a5b626ec48a526a4: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about deserts! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 logger.py:37] Received request chatcmpl-b6a5033ad9064e2d94267e879637f2cb: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about 24! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 engine.py:267] Added request chatcmpl-3d8aa644c0a54f9fb27bd69e9be4e1ae.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 engine.py:267] Added request chatcmpl-c16514f6518b4f47a5b626ec48a526a4.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 engine.py:267] Added request chatcmpl-b6a5033ad9064e2d94267e879637f2cb.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 logger.py:37] Received request chatcmpl-cd28ea269e3c471d9e66607facc3b70b: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about dance! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 engine.py:267] Added request chatcmpl-cd28ea269e3c471d9e66607facc3b70b.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 logger.py:37] Received request chatcmpl-d321bd214f674e57a1db50a66f13f079: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about 42! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 engine.py:267] Added request chatcmpl-d321bd214f674e57a1db50a66f13f079.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 logger.py:37] Received request chatcmpl-5213f0676d1940d682707ab06bd88faf: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about elephants! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 engine.py:267] Added request chatcmpl-5213f0676d1940d682707ab06bd88faf.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 logger.py:37] Received request chatcmpl-89a97f851a864684a7c9fbfcc97c5b87: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about infinity! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 logger.py:37] Received request chatcmpl-cc3ba5d646074a4db024a9bc8c4f4bfa: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about technology! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 logger.py:37] Received request chatcmpl-993d65036ed94bbc8bb5e5e9e110edad: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about rabbits! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 logger.py:37] Received request chatcmpl-aa47a975378641b19fed072daed6b27e: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about music! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 logger.py:37] Received request chatcmpl-ab20c09aeb7e499ab5eb4b05d1d77d83: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about science! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 logger.py:37] Received request chatcmpl-f7a2774a4a9046d3b526c5de230af980: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about art! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 logger.py:37] Received request chatcmpl-afe5c8a1957749c69eded4d3859a23f6: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about snakes! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 logger.py:37] Received request chatcmpl-70658148d971436195ed90686096dc04: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about stars! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 logger.py:37] Received request chatcmpl-8ea21a1a74204662ae234786a045f928: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 27 May 2025\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about planets! 100 words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 engine.py:267] Added request chatcmpl-89a97f851a864684a7c9fbfcc97c5b87.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 engine.py:267] Added request chatcmpl-cc3ba5d646074a4db024a9bc8c4f4bfa.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 engine.py:267] Added request chatcmpl-993d65036ed94bbc8bb5e5e9e110edad.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 engine.py:267] Added request chatcmpl-aa47a975378641b19fed072daed6b27e.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 engine.py:267] Added request chatcmpl-ab20c09aeb7e499ab5eb4b05d1d77d83.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 engine.py:267] Added request chatcmpl-f7a2774a4a9046d3b526c5de230af980.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 engine.py:267] Added request chatcmpl-afe5c8a1957749c69eded4d3859a23f6.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 engine.py:267] Added request chatcmpl-70658148d971436195ed90686096dc04.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 engine.py:267] Added request chatcmpl-8ea21a1a74204662ae234786a045f928.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47842 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43890 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:26 metrics.py:467] Avg prompt throughput: 423.3 tokens/s, Avg generation throughput: 850.2 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.9%, CPU KV cache usage: 0.0%.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47962 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43904 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:47964 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43900 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:48040 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43906 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:48078 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43930 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:48050 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:48062 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:48094 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:48110 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43918 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43950 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43914 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43940 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:48140 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43966 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:48126 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43976 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:44794 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:44042 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:44790 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43980 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:44802 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:44006 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:44810 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:44034 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:44820 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:44050 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:44768 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:44024 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:48156 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43988 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:44758 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:44780 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:44804 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:44016 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43994 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:43978 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:44834 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:42976 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:40256 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:42984 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:44844 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:40270 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:38 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 157.6 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO 05-27 14:49:48 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:60864 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:56076 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:47804 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
      "\u001b[32m[Llama3B]\u001b[0m INFO:     127.0.0.1:41404 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[34m[SmolVLM]\u001b[0m INFO:     127.0.0.1:36034 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
      "\u001b[35m[ROUTER]\u001b[0m INFO:     127.0.0.1:49596 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import uvicorn\n",
    "import subprocess\n",
    "import nest_asyncio\n",
    "from colorama import Fore, Style\n",
    "import traceback\n",
    "\n",
    "# Allow nested asyncio event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def run_cmd(cmd, label, color, start_event=None, end_event=None, end_trigger=\"Uvicorn running on\"):\n",
    "    if start_event:\n",
    "        await start_event.wait()  # Wait for the previous process to signal it's ready\n",
    "    print(f\"\\nSTARTING {color}[{label}]{Style.RESET_ALL}\")\n",
    "    process = await asyncio.create_subprocess_shell(\n",
    "        \" \".join(cmd.strip().split()),\n",
    "        stdout=asyncio.subprocess.PIPE,\n",
    "        stderr=asyncio.subprocess.PIPE,\n",
    "    )\n",
    "    async def read_stream(stream, color, label):\n",
    "        while True:\n",
    "            line = await stream.readline()\n",
    "            if not line: break\n",
    "            decoded_line = line.decode().strip()\n",
    "            print(f\"{color}[{label}]{Style.RESET_ALL} {decoded_line}\")\n",
    "            if end_event and end_trigger and end_trigger in decoded_line:\n",
    "                end_event.set()  # Signal that this process is ready when port is shown\n",
    "    stdout_task = asyncio.create_task(read_stream(process.stdout, color, label))\n",
    "    stderr_task = asyncio.create_task(read_stream(process.stderr, Fore.RED, f\"{label}-ERR\"))\n",
    "    await asyncio.wait([stdout_task, stderr_task])\n",
    "    await process.wait()\n",
    "\n",
    "async def run_llama(port=9000, **kwargs):\n",
    "    await run_cmd(f\"\"\"\n",
    "    vllm serve unsloth/Llama-3.2-3B-Instruct\n",
    "    --enable-auto-tool-choice\n",
    "    --tool-call-parser llama3_json\n",
    "    --chat-template tool_chat_template_llama3.2_json.jinja\n",
    "    --max_model_len 8192\n",
    "    --gpu-memory-utilization 0.35\n",
    "    --enforce-eager\n",
    "    --port {port}\n",
    "    \"\"\", label=\"Llama3B\", color=Fore.GREEN, **kwargs) \n",
    "\n",
    "async def run_smolvlm(port=9000, **kwargs):\n",
    "    await run_cmd(f\"\"\"\n",
    "    vllm serve HuggingFaceTB/SmolVLM-Instruct\n",
    "    --max_model_len 8192\n",
    "    --gpu-memory-utilization 0.35\n",
    "    --enforce-eager\n",
    "    --port {port}\n",
    "    --max-num-seqs 16\n",
    "    \"\"\", label=\"SmolVLM\", color=Fore.BLUE, **kwargs) \n",
    "\n",
    "async def run_sdxl(port=9000, **kwargs):\n",
    "    await run_cmd(\n",
    "        f\"PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True uvicorn sdxl_app:app --host 0.0.0.0 --port {port} --log-level info 2>&1\",\n",
    "        label=\"SDXL1.0\", color=Fore.CYAN, **kwargs) \n",
    "\n",
    "async def run_router(port=9000, **kwargs):\n",
    "    await run_cmd(f\"uvicorn router_app:app --host 0.0.0.0 --port {port} --log-level info 2>&1\",\n",
    "    label=\"ROUTER\", color=Fore.MAGENTA, **kwargs) \n",
    "    \n",
    "# Create event objects to coordinate the start of each process\n",
    "events = [None, None]\n",
    "def get_ev_pair(events=events):\n",
    "    events[:2] = [events[1], asyncio.Event()]\n",
    "    return {\"start_event\": events[0], \"end_event\": events[1]}\n",
    "\n",
    "print(\"Starting Processes\")\n",
    "await asyncio.gather(\n",
    "    run_llama   (9001, **get_ev_pair()),\n",
    "    run_smolvlm (9002, **get_ev_pair()),\n",
    "    run_sdxl    (9003, **get_ev_pair()),\n",
    "    run_router  (9004, **get_ev_pair()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2970be4d-6995-4286-b3b6-1c17c4e7e131",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/en-us/training/\">\n",
    "    <div style=\"width: 55%; background-color: white; margin-top: 50px;\">\n",
    "    <img src=\"https://dli-lms.s3.amazonaws.com/assets/general/nvidia-logo.png\"\n",
    "         width=\"400\"\n",
    "         height=\"186\"\n",
    "         style=\"margin: 0px -25px -5px; width: 300px\"/>\n",
    "</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
