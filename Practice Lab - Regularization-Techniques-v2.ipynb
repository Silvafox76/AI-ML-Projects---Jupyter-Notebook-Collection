{"cells":[{"cell_type":"markdown","id":"4165dc31-9797-4445-b04c-a0052ab922cb","metadata":{},"outputs":[],"source":["<center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"300\" alt=\"cognitiveclass.ai logo\">\n","</center>\n"]},{"cell_type":"markdown","id":"a1b6f089-9348-4344-bb85-101498ea776d","metadata":{},"outputs":[],"source":["# Regularization Techniques\n","\n","\n","Estimated time needed: **60** minutes\n","\n","The purpose of Regularization techniques is to reduce the degree of overfitting that can occur in Regression models. Overfitting leads to poor ability of the model to make predictions on the new, unseen data. As we saw in the previous Regression Lessons, with a creation of extra features, such as through polynomial regression, a model can become easily overfit. To reduce the overfitting, we can regularize the model, or in other words, we can decrease its degrees of freedom. A simple way to regularize polynomial model is to reduce the number of polynomial degrees. For a linear regression model, regularization is typically achieved by constraining the weights of the model. Regularizer imposes a penalty on the size of the coefficients of the model.\n","\n","In this lab, we will cover three types of regularizers:\n","\n","*   Ridge regression\n","*   Lasso regression\n","*   Elastic Net\n","\n","Each one has its own advantages and disadvantages. Lasso will eliminate many features and reduce overfitting in your linear model. Ridge will reduce the impact of the features that are not important in predicting your target. Elastic Net combines feature elimination from Lasso and feature coefficient reduction from the Ridge model to improve your model’s predictions.\n","\n","The common features of all these regularizers include using cross-validation to select hyperparameters and applying data normalization to improve the performance.\n","\n","## Objectives\n","\n","After completing this lab you will be able to:\n","\n","*   Understand the advantages and disadvantages of Ridge, Lasso and Elastic Net Regressions\n","*   Apply Ridge, Lasso and Elastic Net Regressions\n","*   Perform  hyperparameters Grid Search on a model using validation data\n"]},{"cell_type":"markdown","id":"7cec20b0-e82d-4aa4-811b-5e94b575f9e8","metadata":{},"outputs":[],"source":["***\n"]},{"cell_type":"markdown","id":"00546eff-a023-43c8-99e5-0221390cac1d","metadata":{},"outputs":[],"source":["## **Setup**\n"]},{"cell_type":"markdown","id":"2bde5a01-99c8-4bef-aabb-40b112183120","metadata":{},"outputs":[],"source":["For this lab, we will be using the following libraries:\n","\n","*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for managing the data.\n","*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for mathematical operations.\n","*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for visualizing the data.\n","*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for visualizing the data.\n","*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for machine learning and machine-learning-pipeline related functions.\n","*   [`scipy`](https://docs.scipy.org/doc/scipy/tutorial/stats.html/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for statistical computations.\n"]},{"cell_type":"markdown","id":"07da3353-c576-434e-82bb-e83a3de255bd","metadata":{},"outputs":[],"source":["## **Import the required libraries**\n"]},{"cell_type":"code","id":"5a3423d0-741b-48a6-9a3d-335a14f3f1a5","metadata":{},"outputs":[],"source":["# Install the required libraries\n!pip install -U scikit-learn\n!pip install pandas\n!pip install numpy\n!pip install seaborn\n!pip install matplotlib"]},{"cell_type":"code","id":"1e026902-e5f4-4ef0-ae41-7d645919f6b3","metadata":{},"outputs":[],"source":["# Surpress warnings:\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn"]},{"cell_type":"code","id":"f2d805dd-3a1e-41dd-b93e-26f0dc580185","metadata":{},"outputs":[],"source":["import pandas as pd\nimport numpy as np \n\nimport seaborn as sns \nimport matplotlib.pylab as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,ElasticNet\nfrom sklearn.metrics import r2_score \nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.decomposition import PCA\n\n"]},{"cell_type":"markdown","id":"d069a225-bb70-45d7-83cf-be7b992d0b58","metadata":{},"outputs":[],"source":["First, let's define some functions that will help us in the future analysis.\n","\n","Below function will calculate the $R^{2}$ on each feature given the input of the model.\n"]},{"cell_type":"code","id":"99353c36-0260-4ee8-a1eb-6fc622a488c2","metadata":{},"outputs":[],"source":["def get_R2_features(model,test=True): \n    #X: global  \n    features=list(X)\n    features.remove(\"three\")\n    \n    R_2_train=[]\n    R_2_test=[]\n\n    for feature in features:\n        model.fit(X_train[[feature]],y_train)\n        \n        R_2_test.append(model.score(X_test[[feature]],y_test))\n        R_2_train.append(model.score(X_train[[feature]],y_train))\n        \n    plt.bar(features,R_2_train,label=\"Train\")\n    plt.bar(features,R_2_test,label=\"Test\")\n    plt.xticks(rotation=90)\n    plt.ylabel(\"$R^2$\")\n    plt.legend()\n    plt.show()\n    print(\"Training R^2 mean value {} Testing R^2 mean value {} \".format(str(np.mean(R_2_train)),str(np.mean(R_2_test))) )\n    print(\"Training R^2 max value {} Testing R^2 max value {} \".format(str(np.max(R_2_train)),str(np.max(R_2_test))) )"]},{"cell_type":"markdown","id":"e44a990a-c23a-423e-ac11-20ba5d1dad9e","metadata":{},"outputs":[],"source":["Below function will plot the estimated coefficients for each feature and find $R^{2}$ on training and testing sets.\n"]},{"cell_type":"code","id":"c8e72736-e748-42a7-bc9b-98603fa7bf7e","metadata":{},"outputs":[],"source":["def plot_coef(X,model,name=None):\n    \n\n    plt.bar(X.columns[2:],abs(model.coef_[2:]))\n    plt.xticks(rotation=90)\n    plt.ylabel(\"$coefficients$\")\n    plt.title(name)\n    plt.show()\n    print(\"R^2 on training  data \",model.score(X_train, y_train))\n    print(\"R^2 on testing data \",model.score(X_test,y_test))\n    "]},{"cell_type":"markdown","id":"35d4c778-462f-43e9-97cb-b00015d13a3b","metadata":{},"outputs":[],"source":["Below function plots the distribution of two inputs.\n"]},{"cell_type":"code","id":"56bbd96d-4e2b-4980-aa39-63e19db76ee1","metadata":{},"outputs":[],"source":["def  plot_dis(y,yhat):\n    \n    plt.figure()\n    ax1 = sns.distplot(y, hist=False, color=\"r\", label=\"Actual Value\")\n    sns.distplot(yhat, hist=False, color=\"b\", label=\"Fitted Values\" , ax=ax1)\n    plt.legend()\n\n    plt.title('Actual vs Fitted Values')\n    plt.xlabel('Price (in dollars)')\n    plt.ylabel('Proportion of Cars')\n\n    plt.show()\n    plt.close()"]},{"cell_type":"markdown","id":"ee3967f9-8e08-4306-84fc-16542e1f76ab","metadata":{},"outputs":[],"source":["## **Reading and understanding our data**\n"]},{"cell_type":"markdown","id":"6b71c563-94d6-43fc-809b-7454eca904b8","metadata":{},"outputs":[],"source":["For this lab, we will be using the car sales dataset, hosted on IBM Cloud object storage. The dataset contains all the information about cars, the name of the manufacturer, the year it was launched, all car technical parameters, and the sale price. This dataset has already been pre-cleaned and encoded (using one-hot and label encoders) in the Linear Regression Notebook.\n"]},{"cell_type":"markdown","id":"e85a2304-e582-413c-8147-4e6fe99b97c1","metadata":{},"outputs":[],"source":["Let's read the data into *pandas* data frame and look at the first 5 rows using the `head()` method.\n"]},{"cell_type":"code","id":"c2c95e34-c508-4927-add1-5083a24911df","metadata":{},"outputs":[],"source":["data = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML240EN-SkillsNetwork/labs/encoded_car_data.csv')\ndata.head()"]},{"cell_type":"markdown","id":"e40dadcf-da42-49a6-9f46-45444002db2e","metadata":{},"outputs":[],"source":["We can find more information about the features and types using the `info()`  method.\n"]},{"cell_type":"code","id":"2522b670-5846-445f-b623-ecc78713291d","metadata":{},"outputs":[],"source":["data.info()"]},{"cell_type":"markdown","id":"b9475961-3c81-42d0-b191-b10ee37165d1","metadata":{},"outputs":[],"source":["## Data Preparation\n","\n","Let's first split our data into `X` features and `y` target.\n"]},{"cell_type":"code","id":"9eeba08e-fa2b-4cc9-82a3-552994353f16","metadata":{},"outputs":[],"source":["X = data.drop('price', axis=1)\ny = data.price"]},{"cell_type":"markdown","id":"6b23b972-0520-4942-82db-36efb23bed9d","metadata":{},"outputs":[],"source":["Now that we have split our data into training and testing sets, the training data is used for your model to recognize patterns using some criteria,the test data set it used to evaluate your model, as shown in the following image:\n"]},{"cell_type":"markdown","id":"b8819176-2cbe-4531-826e-dfbbfeb3858c","metadata":{},"outputs":[],"source":["<center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML240EN-SkillsNetwork/images/trin-test.png\">\n","</center>\n","<center>source scikit-learn.org</center>\n"]},{"cell_type":"markdown","id":"20cae5be-dceb-4e9a-8901-9fc3611a268f","metadata":{},"outputs":[],"source":["Now, we split our data, using <code>train_test_split</code> function, into the training and testing sets, allocating 30% of the data for testing.\n"]},{"cell_type":"code","id":"5d0ec3c3-1608-414f-ab43-8d22bcf7feab","metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1, random_state=42)\nprint(\"number of test samples :\", X_test.shape[0])\nprint(\"number of training samples:\",X_train.shape[0])\n"]},{"cell_type":"markdown","id":"fafc5b2d-514d-4cda-ad64-9c6d2d3cd0a6","metadata":{},"outputs":[],"source":["## Linear Regression\n","\n","In linear  regression we are trying to find the value of $\\textbf{w}$ that  minimizes the Mean Squared Error (MSE), we can represent this using the following expression:\n"]},{"cell_type":"markdown","id":"92511afb-ba21-4acf-965c-486b86536f8c","metadata":{},"outputs":[],"source":["$\\hat{\\textbf{w}}= \\underset{\\textbf{w}}{\\mathrm{argmin}} {\n","||\\textbf{y}  - \\textbf{Xw} ||^2_2}$\n"]},{"cell_type":"markdown","id":"514fc635-7834-4019-82de-8e26279f644c","metadata":{},"outputs":[],"source":["Where $\\textbf{y}$ is the target, $\\textbf{X}$ is the training set and $\\textbf{w}$ is the parameter weights. The resulting $\\hat{\\textbf{w}}$ is the best value to minimize the MSE, i.e., the distance between the target $\\textbf{y}$ and the estimate $\\textbf{Xw}$. We do this by fitting the model.\n"]},{"cell_type":"markdown","id":"73f1a49f-515a-4fb5-9878-78d2c439f34f","metadata":{},"outputs":[],"source":["Let's create a <code>LinearRegression</code> object, called `lm`.\n"]},{"cell_type":"code","id":"79104d00-c874-467d-b813-254d85710ddf","metadata":{},"outputs":[],"source":["lm = LinearRegression()"]},{"cell_type":"markdown","id":"20192a1c-5769-4348-a2eb-02e015fdc750","metadata":{},"outputs":[],"source":["Now, let's fit the model with multiple features on our X_train and y_train data.\n"]},{"cell_type":"code","id":"b4d348cf-ea58-4741-adff-ad11424ee040","metadata":{},"outputs":[],"source":["lm.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"7c137b8a-6bd0-4702-91d9-e21c624f3bb0","metadata":{},"outputs":[],"source":["We apply `predict(`) function on the testing data set.\n"]},{"cell_type":"code","id":"c3fbb51c-99ea-4538-aa3c-97758bf0286b","metadata":{},"outputs":[],"source":["predicted = lm.predict(X_test)"]},{"cell_type":"markdown","id":"40b255ad-ddca-4ff8-abbf-ef7a1f79fc59","metadata":{},"outputs":[],"source":["Let's calculate the `$R^2$` on both, training and testing data sets.\n"]},{"cell_type":"code","id":"d44f0ca0-a246-4f94-9681-afb8cfc504d6","metadata":{},"outputs":[],"source":["print(\"R^2 on training  data \",lm.score(X_train, y_train))\nprint(\"R^2 on testing data \",lm.score(X_test,y_test))"]},{"cell_type":"markdown","id":"851f2511-558a-4ac7-8347-988f313bbb78","metadata":{},"outputs":[],"source":["We can plot a distribution of the predicted values vs the actual values.\n"]},{"cell_type":"code","id":"23989f2e-15ea-45c6-8d4a-a520ada298f8","metadata":{},"outputs":[],"source":["plot_dis(y_test,predicted)"]},{"cell_type":"markdown","id":"cee1533c-cfd5-46ed-b0e8-d9c90de13a7e","metadata":{},"outputs":[],"source":["We can view the estimated coefficients for the linear regression problem and drop the top two coefficients, as they are two large.\n"]},{"cell_type":"code","id":"7a9e1aa2-62d8-4498-bd77-2456733c3436","metadata":{},"outputs":[],"source":["plot_coef(X,lm,name=\"Linear Regression\")"]},{"cell_type":"markdown","id":"26c23df8-47c5-4bc8-8aed-d6aeb23477ec","metadata":{},"outputs":[],"source":["## Ridge Regression\n"]},{"cell_type":"markdown","id":"fa852d41-6416-48ed-9b35-c1a855d8cbf1","metadata":{},"outputs":[],"source":["Let's review the Ridge Regression. Ridge Regression makes the prior assumption that our coefficients are normally distributed around zero. A regularization term, alpha, is added to the cost function. This forces the learning algorithm to not only fit the data but also keep the model weights as small as possible. The variance of the distribution is inversely proportional to the parameter alpha. This is also called the  L2 regularizer , as it adds a L2 penalty to the minimization term, as shown here:\n","\n","$\\hat{\\textbf{w}}= \\underset{\\textbf{w}}{\\mathrm{argmin}} {\n","||\\textbf{y}  - \\textbf{Xw} ||^2_2+ \\alpha  ||\\textbf{w}||_2 }$\n","\n","We minimize the MSE, but we also penalize large weights by including their magnitude $||\\textbf{w}||\\_2$ in the minimization term. This additional minimization term makes the model less susceptible to noise and makes the weights smaller. Alpha controls the takeoff between MSE and penalization or regularization term and is chosen via cross-validation.\n"]},{"cell_type":"markdown","id":"d38f21ef-2f8e-4bef-85c4-1a75646bef52","metadata":{},"outputs":[],"source":["Let's see  how the parameter alpha changes the model. Note, here our test data will be used as validation data. Also, the regularization term should only be added to the cost function during the training.\n","\n","Let's create a Ridge Regression object, setting the regularization parameter (alpha) to 0.01.\n"]},{"cell_type":"code","id":"79bbf666-39f1-4199-b542-a5dbdb1bfd25","metadata":{},"outputs":[],"source":["rr = Ridge(alpha=0.01)\nrr"]},{"cell_type":"markdown","id":"35f3cd88-6506-4daf-9797-7b6fb5845ca2","metadata":{},"outputs":[],"source":["Like regular regression, you can fit the model using the `fit()` method.\n"]},{"cell_type":"code","id":"8155a8b6-4b7d-477d-a760-87f407e0aeac","metadata":{},"outputs":[],"source":["rr.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"f07a3e87-1bfd-487a-be64-6070ac37cda8","metadata":{},"outputs":[],"source":["Similarly, you can obtain a prediction:\n"]},{"cell_type":"code","id":"39c0a01d-8462-45a5-b206-ea24866b4c1b","metadata":{},"outputs":[],"source":["rr.predict(X_test)"]},{"cell_type":"markdown","id":"cc7837b6-2a01-46d5-b69f-b7a8ca2a7e71","metadata":{},"outputs":[],"source":["We can calculate the $R^2$ on the training and testing data.\n"]},{"cell_type":"code","id":"062b2f37-744c-4478-92e1-3169259a709f","metadata":{},"outputs":[],"source":["print(\"R^2 on training  data \",rr.score(X_train, y_train))\nprint(\"R^2 on testing data \",rr.score(X_test,y_test))"]},{"cell_type":"markdown","id":"73b6e257-4d2c-41cc-bb29-dd84b029e344","metadata":{},"outputs":[],"source":["Now let's compare the Ridge Regression and the Linear Regression  models. The results on the $R^2$ are about the same, and the coefficients seem to be smaller.\n"]},{"cell_type":"code","id":"1803b14a-d269-47fc-8c05-df178c650b28","metadata":{},"outputs":[],"source":["plot_coef(X,lm,name=\"Linear Regression\")\nplot_coef(X,rr,name=\"Ridge Regression\")"]},{"cell_type":"markdown","id":"d02292db-78c4-42eb-bc49-498d5f314eed","metadata":{},"outputs":[],"source":["If we increase alpha, the coefficients get smaller, but the results are not as good as our previous value of alpha.\n"]},{"cell_type":"code","id":"78cca8a0-109a-4991-8cac-e901f7afcfbe","metadata":{},"outputs":[],"source":["rr = Ridge(alpha=1)\nrr.fit(X_train, y_train)\nplot_coef(X,rr)"]},{"cell_type":"markdown","id":"491c4ab1-7dd9-4437-a39b-88f6f0eef1d9","metadata":{},"outputs":[],"source":["In general, we see that if we increase alpha, the coefficients get smaller, but the model performance relationship gets more complex. As a result, we use the validation data to select a value for alpha. Here, we plot the coefficients and $R^2$ of the test data on the vertical axes and alpha on the horizontal axis, as well the $R^2$ using the test data.\n"]},{"cell_type":"code","id":"73647a64-449f-4af6-b488-3068e6a273e8","metadata":{},"outputs":[],"source":["alphas = [0.00001,0.0001,0.001,0.01,0.1,1,10,100]\nR_2=[]\ncoefs = []\nfor alpha in alphas:\n    ridge = Ridge(alpha=alpha)\n    ridge.fit(X_train, y_train)\n    coefs.append(abs(ridge.coef_))\n    R_2.append(ridge.score(X_test,y_test))\n\n\nax = plt.gca()\nax.plot(alphas, coefs)\nax.set_xscale(\"log\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"weights\")\nplt.title(\"Ridge coefficients as a function of the regularization (regularization path)\")\nplt.show()\n\n\nax = plt.gca()\nax.plot(alphas, R_2)\nax.set_xscale(\"log\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"$R^2$\")\nplt.title(\"$R^2$ as a function of the regularization\")\nplt.show()          "]},{"cell_type":"markdown","id":"0faca810-91bb-4a93-865a-c51fd786466b","metadata":{},"outputs":[],"source":["As we increase alpha, the coefficients get smaller but the $R^2$ peaks when alpha is 1.\n"]},{"cell_type":"markdown","id":"2e11c20c-5389-4cba-960a-2e77debeb1c5","metadata":{},"outputs":[],"source":["## Exercise 1\n","\n","In this Exercise, plot the MSE as a function of alpha. What pattern do you notice?\n"]},{"cell_type":"code","id":"21e2d289-2728-457b-9c36-d5a66a33382d","metadata":{},"outputs":[],"source":["# Enter your code and run the cell\n"]},{"cell_type":"markdown","id":"f22d0943-fd56-4506-9dfa-25f515c9907b","metadata":{},"outputs":[],"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","alphas = [0.00001,0.0001,0.001,0.01,0.1,1,10]\n","MEAN_SQE=[]\n","\n","for alpha in alphas:\n","    ridge = Ridge(alpha=alpha)\n","    ridge.fit(X_train, y_train)\n","    MEAN_SQE.append(mean_squared_error(ridge.predict(X_test),y_test))\n","\n","ax = plt.gca()\n","ax.plot(alphas, MEAN_SQE)\n","ax.set_xscale(\"log\")\n","plt.xlabel(\"alpha\")\n","plt.ylabel(\"MSE\")\n","plt.title(\"$MSE$ as a function of the regularization\")\n","plt.show()\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"bb1b404e-9650-45ea-b744-a2575cc2b3b9","metadata":{},"outputs":[],"source":["<details>\n","    <summary><strong>Answers</strong> (Click Here)</summary>\n","A small alpha leads to over-fitting but as alpha gets larger the MSE decreases. When alpha gets too large the MSE increases leading to underfitting. The optimal point seems to be in the middle .\n","</details>\n"]},{"cell_type":"markdown","id":"cdd7218a-e407-48de-96da-c652a2694e43","metadata":{},"outputs":[],"source":["## Pipeline\n"]},{"cell_type":"markdown","id":"5b8796a5-7bb5-4ed8-82b0-a86016f14cda","metadata":{},"outputs":[],"source":["We can also create a Pipeline object and apply a set of transforms sequentially. Then, we can apply Polynomial Features, perform data standardization then apply Ridge regression.  Data Pipelines simplify the steps of processing the data. We use the module `Pipeline` to create a pipeline. We also use `StandardScaler` step in our pipeline. Scaling our data is necessary step in Ridge regression as it will penalize features with a large magnitude.\n","\n","Now, we create a pipeline object.\n"]},{"cell_type":"code","id":"3d9ea873-5cec-4eb3-861c-5ac49bfd948c","metadata":{},"outputs":[],"source":["Input=[ ('polynomial', PolynomialFeatures(include_bias=False,degree=2)),('ss',StandardScaler() ), ('model',Ridge(alpha=1))]\npipe = Pipeline(Input)"]},{"cell_type":"markdown","id":"d245071a-e950-4445-8dfc-3a2d9836f384","metadata":{},"outputs":[],"source":["We fit the object.\n"]},{"cell_type":"code","id":"75b09502-cdb6-4327-b43e-3553a8268aab","metadata":{},"outputs":[],"source":["pipe.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"b04daecd-8cda-4626-8b8b-f09f8a6cb5ff","metadata":{},"outputs":[],"source":["We can calculate the score on the test data.\n"]},{"cell_type":"code","id":"cda94379-17d9-42a4-9df5-9969144527db","metadata":{},"outputs":[],"source":["predicted=pipe.predict(X_test)\npipe.score(X_test, y_test)"]},{"cell_type":"markdown","id":"fab7b4db-96f3-4a67-978e-3efd0a64d2d9","metadata":{},"outputs":[],"source":["Looking for hyperparameters can get difficult with loops. The problem will get worse as we add more transforms such as polynomial transform. Therefore, we can use `GridSearchCV` to make things simpler.\n"]},{"cell_type":"markdown","id":"c9bac321-ea5d-4370-85cc-aa382b2da45c","metadata":{},"outputs":[],"source":["## GridSearchCV\n"]},{"cell_type":"markdown","id":"e3e85012-703a-4106-8feb-7a7df0b163f3","metadata":{},"outputs":[],"source":["To search for the best combination of hyperparameters we can create a  `GridSearchCV()` function as a dictionary of parameter values. The parameters of pipelines can be set by using the name of the key, separated by \"\\__\", then the parameter. Here, we look for different polynomial degrees and different values of alpha.\n"]},{"cell_type":"code","id":"5d6c8565-6108-48b0-9979-ed6b369a2f6b","metadata":{},"outputs":[],"source":["param_grid = {\n    \"polynomial__degree\": [1,2,3,4],\n    \"model__alpha\":[0.0001,0.001,0.01,0.1,1,10]\n}"]},{"cell_type":"markdown","id":"6822ede3-9876-4d20-bb7c-5d8646e4f233","metadata":{},"outputs":[],"source":["Keys of the dictionary are the model \"key name \\__\" followed by the parameter as an attribute.\n"]},{"cell_type":"markdown","id":"b726fb18-8255-4351-bd8f-a451b6ecc6a5","metadata":{},"outputs":[],"source":["<b>polynomial\\_\\_degree</b>: is the degree of the polynomial; in this case 1, 2, 3, 4 and 5.\n","\n","<b>model\\_\\_alpha </b>: Regularization strength; must be a positive float.\n"]},{"cell_type":"markdown","id":"d137ec57-6f5c-49d3-af1e-2e50e8fea79b","metadata":{},"outputs":[],"source":["We create a `GridSearchCV` object and fit it. The method trains the model and the hyperparameters are selected via exhaustive search over the specified values.\n"]},{"cell_type":"code","id":"388e8425-5095-44c2-8d72-5bcaeb40a85e","metadata":{},"outputs":[],"source":["search = GridSearchCV(pipe, param_grid, n_jobs=2)\n\nsearch.fit(X_train, y_train)\nsearch"]},{"cell_type":"markdown","id":"89c4fca8-8534-4868-9016-1a319cf10c2b","metadata":{},"outputs":[],"source":["We can input the results into *pandas* `DataFrame()` as a dictionary with keys as column headers and values as columns and display the results.\n"]},{"cell_type":"code","id":"53881c54-ef81-4085-8da1-b35881752643","metadata":{},"outputs":[],"source":["pd.DataFrame(search.cv_results_).head()"]},{"cell_type":"markdown","id":"1663edb6-eba7-446e-bc58-ca9fa4ec0b9b","metadata":{},"outputs":[],"source":["There are some other useful attributes:\n"]},{"cell_type":"markdown","id":"6fe6a7ec-6779-4846-9d55-4f7c50542a51","metadata":{},"outputs":[],"source":["`best_score_`: mean cross-validated score of the `best_estimator`.\n","\n","`best_params_dict`: parameter setting that gives the best results on the hold-out data.\n"]},{"cell_type":"code","id":"30851969-a844-41fa-b2d1-125547a97f40","metadata":{},"outputs":[],"source":["print(\"best_score_: \",search.best_score_)\nprint(\"best_params_: \",search.best_params_)"]},{"cell_type":"markdown","id":"4b5e1793-082b-4ee5-b723-92bcb8646778","metadata":{},"outputs":[],"source":["We can call `predict()` on the estimator with the best found parameters.\n"]},{"cell_type":"code","id":"658549f1-47e5-4030-afca-b92933417d98","metadata":{},"outputs":[],"source":["\npredict = search.predict(X_test)\n\npredict "]},{"cell_type":"markdown","id":"da8611d6-b88a-47c2-8592-a815dc361988","metadata":{},"outputs":[],"source":["We can find the best model.\n"]},{"cell_type":"code","id":"49972f60-570e-4de2-8318-f3e5e155f333","metadata":{},"outputs":[],"source":["best=search.best_estimator_\nbest"]},{"cell_type":"markdown","id":"0512a246-8123-4de3-9ffc-4fb080397488","metadata":{},"outputs":[],"source":["As we can see from the above output, it is five degree polynomial with alpha value of 0.0001.\n","Now, let's make a prediction.\n"]},{"cell_type":"code","id":"27f9b119-b762-4b64-9cb2-4da27755f17b","metadata":{},"outputs":[],"source":["predict = best.predict(X_test)\npredict"]},{"cell_type":"markdown","id":"b1e09194-9576-453c-a87e-4711ea325a03","metadata":{},"outputs":[],"source":["We can calculate the $R^2$ on the test data.\n"]},{"cell_type":"code","id":"c4f04d65-c97f-4d54-badc-d3c5a9489e59","metadata":{},"outputs":[],"source":["best.score(X_test, y_test)"]},{"cell_type":"markdown","id":"c38b8dd1-01d1-4462-81b0-c4f4a1373cf0","metadata":{},"outputs":[],"source":["As we see, using Ridge Regression polynomial function works better than all other models. Finely, we can train our model on the entire data set!\n"]},{"cell_type":"code","id":"7ef9d470-4b42-4b76-aee8-492387731ec1","metadata":{},"outputs":[],"source":["best.fit(X,y)"]},{"cell_type":"markdown","id":"0903bf83-9f01-4793-b698-5fc5be128298","metadata":{},"outputs":[],"source":["## Exercise 2\n","\n","Perform grid search on the following features and plot the results by completing the following lines of code:\n"]},{"cell_type":"code","id":"b76e1c21-4bd4-408e-8192-f6c68349c992","metadata":{},"outputs":[],"source":["# Run the cell\ncolumns=['wheelbase', 'curbweight', 'enginesize', 'boreratio', 'horsepower',\n       'carlength', 'carwidth', 'citympg']\n"]},{"cell_type":"code","id":"751c9f9f-5a7a-4a01-9e3a-2611fa552c01","metadata":{},"outputs":[],"source":["# Enter your code and run the cell\n"]},{"cell_type":"markdown","id":"67486c08-4c80-47fd-87dd-db1671f5e487","metadata":{},"outputs":[],"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n"," \n","for column in columns:\n","    search.fit(X_train[[column]], y_train)\n","    x=np.linspace(X_test[[column]].min(), X_test[[column]].max(),num=100)\n","    plt.plot(x,search.predict(x.reshape(-1,1)),label=\"prediction\")\n","    plt.plot(X_test[column],y_test,'ro',label=\"y\")\n","    plt.xlabel(column)\n","    plt.ylabel(\"y\")\n","    plt.legend()\n","    plt.show()\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"d242a53d-18e4-4232-a3fd-b57cedc0d7b4","metadata":{},"outputs":[],"source":["## Lasso Regression\n"]},{"cell_type":"markdown","id":"70a47934-f156-43c5-8130-5f8ad91fbac0","metadata":{},"outputs":[],"source":["In this section, let's review the Lasso (Least Absolute Shrinkage and Selection Operator) Regression. Lasso Regression makes the prior assumption that our coefficients have Laplace (double-exponential) distribution around zero. The scale parameter of the distribution is inversely proportional to the parameter alpha. The main advantage of LASSO Regression is that many coefficients are set to zero, therefore they are not required. This has many advantages, one of them is that you may not need to collect and/or store all of the features. This may save resources. For example, if the feature was some medical test, you would no longer need to perform that test. Let's see how the parameter alpha changes the model.  We minimize the MSE, but we also penalize large weights by including their sum of absolute values $||\\textbf{w}||_1$ , symbolically:\n"]},{"cell_type":"markdown","id":"6603e96f-62c1-4f07-bcf1-24c9ecb2ea66","metadata":{},"outputs":[],"source":["$\\hat{\\textbf{w}}= \\underset{\\textbf{w}}{\\mathrm{argmin}} {\n","||\\textbf{y}  - \\textbf{Xw} ||^2_2+ \\alpha  ||\\textbf{w}||_1 }$\n","\n","This regularization or penalty term makes many coefficients zero, making the model easy to understand and can also be used for feature selection. There are some drawbacks to this technique. It takes longer time to train and the solution may not be unique. Alpha controls the trade-off between MSE and penalization or regularization term and is chosen via cross-validation.  Let's see how the parameter alpha changes the model. Note, as before, our test data will be used as validation data. Let's create a Ridge Regression object, setting the regularization parameter (alpha) to 0.01.\n"]},{"cell_type":"code","id":"5af41560-fa49-4236-bf4c-c1a4fc6c1093","metadata":{},"outputs":[],"source":["la = Lasso(alpha=0.1)\nla.fit(X_train,y_train)\nla"]},{"cell_type":"markdown","id":"ed796a27-97bb-4735-9848-cbf1146abe33","metadata":{},"outputs":[],"source":["Let's make a prediction.\n"]},{"cell_type":"code","id":"c2367a60-adf8-48a4-9efb-b88e26bd7bc1","metadata":{},"outputs":[],"source":["predicted = la.predict(X_test)\npredicted"]},{"cell_type":"markdown","id":"510fe36e-85e8-4f11-974e-f6a2ae226fcc","metadata":{},"outputs":[],"source":["Let's calculate the $R^2$ on the training and testing data and see how it performs compared to the other methods.\n"]},{"cell_type":"code","id":"81ce6c74-f5b9-4a63-9848-0c252d250d2f","metadata":{},"outputs":[],"source":["print(\"R^2 on training  data \",lm.score(X_train, y_train))\nprint(\"R^2 on testing data \",lm.score(X_test,y_test))"]},{"cell_type":"markdown","id":"151a4202-0d8f-4b29-90ab-535b56a9bf0a","metadata":{},"outputs":[],"source":["If we compare the Lasso Regression to the  Ridge Regression model we see that the results on the $R^2$ are slightly worse, but most of the coefficients are zero.\n"]},{"cell_type":"code","id":"1272d6d8-1563-447f-b84e-c5a1557da289","metadata":{},"outputs":[],"source":["plot_coef(X,rr,name=\"Ridge Regression\")\nplot_coef(X,la,name=\"Lasso Regression\")"]},{"cell_type":"markdown","id":"81bf0a14-0366-4e60-be2d-2cdc819faef1","metadata":{},"outputs":[],"source":["Similar to the Ridge Regression, if we increase the value of alpha, the coefficients will get smaller. Additionally, many coefficients become zero. Moreover, the model performance relationship becomes more complex. As a result, we use the validation data to select a value for alpha. Here, we plot the coefficients and $R^2$ of the test data on the vertical axes and alpha values on the horizontal axis.\n"]},{"cell_type":"code","id":"2a8ab332-af96-469b-af72-6d1e62688ee8","metadata":{},"outputs":[],"source":["alphas = [0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000]\nR_2=[]\ncoefs = []\nfor alpha in alphas:\n    la=Lasso(alpha=alpha)\n    \n    la.fit(X_train, y_train)\n    coefs.append(abs(la.coef_))\n    R_2.append(la.score(X_test,y_test))\n\n\nax = plt.gca()\nax.plot(alphas, coefs)\nax.set_xscale(\"log\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"weights\")\nplt.title(\"Ridge coefficients as a function of the regularization (regularization path)\")\nplt.show()\n\n\nax = plt.gca()\nax.plot(alphas, R_2)\nax.set_xscale(\"log\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"$R^2$\")\nplt.title(\"$R^2$ as a function of the regularization\")\nplt.show()"]},{"cell_type":"markdown","id":"06575219-6c87-4860-9c3e-88d1298cb8ef","metadata":{},"outputs":[],"source":["## Pipeline\n"]},{"cell_type":"markdown","id":"1d3789df-ce3e-4617-83c9-5b0e57fd3671","metadata":{},"outputs":[],"source":["We can also create a Pipeline object and apply a set of transforms sequentially. Then, we can apply polynomial features, perform data standardization, then apply Lasso Regression.  We also use `StandardScaler` as a step in our pipeline. Scaling your data is necessary step in LASSO Regression, as it will penalize features with a large magnitudes.\n","\n","We start by creating a pipeline object.\n"]},{"cell_type":"code","id":"1d98cb73-ef36-408a-8783-4a177c317d89","metadata":{},"outputs":[],"source":["Input=[ ('polynomial', PolynomialFeatures(include_bias=False,degree=2)),('ss',StandardScaler() ), ('model',Lasso(alpha=1, tol = 0.2))]\npipe = Pipeline(Input)"]},{"cell_type":"markdown","id":"29235fff-3966-49c0-8421-637d89836998","metadata":{},"outputs":[],"source":["Then we fit the object, and make our predictions.\n"]},{"cell_type":"code","id":"3c3866ff-1992-44f6-8725-e269a99617ca","metadata":{},"outputs":[],"source":["pipe.fit(X_train, y_train)\npipe.predict(X_test)"]},{"cell_type":"markdown","id":"14a2110b-b886-4ea4-84dd-80daf960d559","metadata":{},"outputs":[],"source":["We can calculate the $R^2$ on the training and testing data sets.\n"]},{"cell_type":"code","id":"8ef3686b-81bf-415d-b453-a041e3ec3bdd","metadata":{},"outputs":[],"source":["print(\"R^2 on training  data \",pipe.score(X_train, y_train))\nprint(\"R^2 on testing data \",pipe.score(X_test,y_test))"]},{"cell_type":"markdown","id":"54db307d-0e40-4520-8c99-65a79878b325","metadata":{},"outputs":[],"source":["As we see, some individual features perform similarly to using all the features (we removed the feature `three` ). Additionally, we see the smaller coefficients seem to correspond to a larger $R^{2}$, therefore  larger coefficients correspond to overfiting.\n"]},{"cell_type":"markdown","id":"291a0ea0-833b-499f-b5a3-3da30b417c44","metadata":{},"outputs":[],"source":["## GridSearchCV\n","\n","To search for the best combination of hyperparameters, we can create a  `GridSearchCV()` function as a dictionary of parameter values. The parameters of pipelines can be set by using the name of the key, separated by \"\\__\", then the parameter. Here, we look for different polynomial degrees and different values of alpha.\n"]},{"cell_type":"code","id":"e4cb9b68-7118-4cac-8050-6098b7302d6a","metadata":{},"outputs":[],"source":["param_grid = {\n    \"polynomial__degree\": [ 1, 2,3,4,5],\n    \"model__alpha\":[0.0001,0.001,0.01,0.1,1,10]\n}"]},{"cell_type":"markdown","id":"a6f2a0a1-d4e4-447a-85cf-3cf11d16518c","metadata":{},"outputs":[],"source":["To search for the best combination of hyperparameters, we create a  `GridSearchCV` object with a dictionary of parameter values.\n"]},{"cell_type":"code","id":"e0367321-a64e-4378-921d-82e234ea5b3f","metadata":{},"outputs":[],"source":["search = GridSearchCV(pipe, param_grid, n_jobs=1)\nsearch.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"1ac2a3b4-c34f-44ff-b8ab-9ea38d9f5b11","metadata":{},"outputs":[],"source":["Now, we can find the best model.\n"]},{"cell_type":"code","id":"c2e71bd7-f0a5-4ce6-bb81-bdced5bee95e","metadata":{},"outputs":[],"source":["best=search.best_estimator_\nbest"]},{"cell_type":"markdown","id":"c0116f7d-6ab5-46dd-8c93-cc515cde8a9c","metadata":{},"outputs":[],"source":["We can calculate the $R^2$ on the test data.\n"]},{"cell_type":"code","id":"61b75b61-e333-4a29-b452-be26070607ce","metadata":{},"outputs":[],"source":["best.score(X_test,y_test)"]},{"cell_type":"markdown","id":"c39447c7-b7aa-4025-8823-a7be133ca659","metadata":{},"outputs":[],"source":["## Elastic Net\n","\n","In this section, let's review the Elastic Net Regression. It combines L1 and L2 priors as regularizes or penalties. So, we can combine the two as follows:\n"]},{"cell_type":"markdown","id":"9529af0f-f47c-4a39-9b64-9a2ad7fe0d59","metadata":{},"outputs":[],"source":["$\\hat{\\textbf{w}}= \\underset{\\textbf{w}}{\\mathrm{argmin}} {\n","||\\textbf{y}  - \\textbf{Xw} ||^2_2+ \\alpha  \\rho||\\textbf{w}||\\_1\n","0.5 \\alpha (1 - \\rho)  ||\\textbf{w}||^2_2 } $\n"]},{"cell_type":"markdown","id":"e2b2e920-be96-4713-b181-88b0dc426b59","metadata":{},"outputs":[],"source":["Additionally to the alpha term ($\\alpha$), we have a mixing parameter, $\\rho$, such that 0 $\\le$ $\\rho$ $\\le$ 1. For $\\rho$=0, the penalty is an L2 regularization . For $\\rho=0$, it is L1 regularization; otherwise, it is a combination of L1 and L2. In *scikit-learn* the parameter is called  `l1_ratio`. Unlike the Ridge Regression, Elastic Net finds zero coefficients. In many cases Elastic Net performs better than Lasso, as it includes features that are correlated with one another. One drawback of the Elastic Net is you have two hyperparameters.  Lets create a model where `alpha=0.1`and `l1_ratio=0.5` and fit the data with this model.\n"]},{"cell_type":"code","id":"7e26ac6b-2c40-4046-87c4-50f187fd2613","metadata":{},"outputs":[],"source":["enet = ElasticNet(alpha=0.1, l1_ratio=0.5)\nenet.fit(X_train,y_train)"]},{"cell_type":"markdown","id":"041543d3-8c38-4817-92c3-9e559ab72421","metadata":{},"outputs":[],"source":["Let's make a prediction.\n"]},{"cell_type":"code","id":"f6f3038d-41a8-4251-8f0d-9864ef98f7e9","metadata":{},"outputs":[],"source":["\npredicted=enet.predict(X_test)\npredicted"]},{"cell_type":"markdown","id":"43d784df-7c9c-4e17-9560-c140f9fff0c5","metadata":{},"outputs":[],"source":["Let's calculate the $R^2$ on the test data.\n"]},{"cell_type":"code","id":"c84ec830-6ca2-4b13-9801-d3bc73983815","metadata":{},"outputs":[],"source":["print(\"R^2 on training  data \", enet.score(X_train, y_train))\nprint(\"R^2 on testing data \", enet.score(X_test,y_test))"]},{"cell_type":"markdown","id":"a73d21d5-559f-49d1-b76c-1623e143c014","metadata":{},"outputs":[],"source":["If we compare the Elastic Net to Lasso Regression and  Ridge Regression, we see the results on the $R^2$ are better than the Elastic Net and many of the coefficients are zero.\n"]},{"cell_type":"code","id":"25cf19e4-1a67-4bda-ad9e-02a2323cfb57","metadata":{},"outputs":[],"source":["plot_coef(X,la,name=\"Lasso Regression\")\nplot_coef(X,enet,name=\"Elastic net \")\n\n## graph that leads to error "]},{"cell_type":"markdown","id":"b256162d-db40-4532-b4df-a196a5ba19e0","metadata":{},"outputs":[],"source":["## Exercise 3\n","\n","Create and fit the Elastic Net model and the Ridge Regression models and plot the coefficients for both models using the `plot_coef()`function.\n"]},{"cell_type":"code","id":"808b82b3-9111-4808-98c4-6217bc8e02ef","metadata":{},"outputs":[],"source":["# Enter your code and run the cell\n"]},{"cell_type":"markdown","id":"0c4ca223-1a90-4e75-8934-6c0a890e306f","metadata":{},"outputs":[],"source":["\n","<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","enet = ElasticNet(alpha=0.01, l1_ratio=0)\n","enet.fit(X_train,y_train)\n","rr = Ridge(alpha=0.01)\n","rr.fit(X_train,y_train)\n","plot_coef(X,rr,name=\"Ridge Regression\")\n","\n","plot_coef(X,enet,name=\"Elastic net l1_ratio=0 \")\n","```\n","\n","</details> \n"]},{"cell_type":"markdown","id":"9535636e-f637-456b-863f-497e77de51bd","metadata":{},"outputs":[],"source":["## Exercise 4\n","\n","Create a Pipeline object, apply polynomial features (degree = 2), perform data standardization, then apply Elastic Net with `alpha=0.1` and  `l1_ratio=0.1` parameters. Fit the model using the training data, then calculate the $R^2$ on the training and testing data.\n"]},{"cell_type":"code","id":"787e9bec-ed0c-420a-97b0-6ed312eb24c6","metadata":{},"outputs":[],"source":["# Enter your code and run the cell\n"]},{"cell_type":"markdown","id":"97453699-d53c-43b5-9318-b6af01e7a330","metadata":{},"outputs":[],"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","Input=[ ('polynomial', PolynomialFeatures(include_bias=False,degree=2)),('ss',StandardScaler() ), ('model',ElasticNet(alpha=0.1, l1_ratio=0.1))]\n","pipe = Pipeline(Input)\n","pipe.fit(X_train, y_train)\n","print(\"R^2 on training  data \",pipe.score(X_train, y_train))\n","print(\"R^2 on testing data \",pipe.score(X_test,y_test))\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"f8771080-dee3-4638-aa33-f78a99aec770","metadata":{},"outputs":[],"source":["## Exercise 5\n"]},{"cell_type":"markdown","id":"40971ea6-d882-40cd-bff4-d3e69343715f","metadata":{},"outputs":[],"source":["Search for the best combination of  hyperparameters by creating  a  `GridSearchCV` object for Elastic Net Regression. Find the best parameter values using the pipeline object, as used in the above examples. Use`param_grid`, then find thee $R^2$ on the test data using the best estimator.\n"]},{"cell_type":"code","id":"5567e9d3-6c77-42b8-930d-cea1b38e68ae","metadata":{},"outputs":[],"source":["param_grid = {\n    \"polynomial__degree\": [ 1, 2,3,4,5],\n    \"model__alpha\":[0.0001,0.001,0.01,0.1,1,10],\n    \"model__l1_ratio\":[0.1,0.25,0.5,0.75,0.9]\n}\n"]},{"cell_type":"code","id":"5d69d71e-f938-4c21-b0de-26ce008e24cc","metadata":{},"outputs":[],"source":["# Enter your code and run the cell\n"]},{"cell_type":"markdown","id":"39819a68-f849-41bc-bcd7-39077683032e","metadata":{},"outputs":[],"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","Input=[ ('polynomial', PolynomialFeatures(include_bias=False,degree=2)),('ss',StandardScaler() ), ('model',ElasticNet(tol = 0.2))]\n","pipe = Pipeline(Input)\n","search = GridSearchCV(pipe, param_grid, n_jobs=2)\n","search.fit(X_test, y_test)\n","best=search.best_estimator_\n","best.score(X_test,y_test)\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"521ff4a5-631f-45d4-b67f-619b042ae6fd","metadata":{},"outputs":[],"source":["## Principal Component Analysis (Optional)\n"]},{"cell_type":"markdown","id":"d1f5a7ce-ecdd-4b95-a222-f0564fd61ffa","metadata":{},"outputs":[],"source":["In this example, we will explore Principal Component Analysis to reduce the dimensionality of our data.\n","We will do so by creating a Pipeline object first, then applying standard scaling and performing PCA, and then applying Elastic Net Regularization with the following parametrs: `tol=0.2`, `alpha=0.1` and  `l1_ratio=0.1`.\n","Finally, we will fit the model using the training data, then calculate the  $𝑅^2$  on the training and testing data sets.\n"]},{"cell_type":"markdown","id":"17281067-c8b1-42b2-af0b-98e63626a053","metadata":{},"outputs":[],"source":["Before adding PCA as a prep-processing step, we have to standardize our data. Scaling the features makes them have the same standard deviation.\n"]},{"cell_type":"code","id":"94065c63-e1e6-4b0f-ba83-5636aedf16bb","metadata":{},"outputs":[],"source":["scaler = StandardScaler()\nX_train[:] = scaler.fit_transform(X_train)\nX_train.columns = [f'{c} (scaled)' for c in X_train.columns]"]},{"cell_type":"markdown","id":"180bb170-4b75-40bf-b5d1-4beaae545e30","metadata":{},"outputs":[],"source":["Now, let's perform PCA.\n"]},{"cell_type":"code","id":"41d59a36-611b-45c1-91c4-a3048a4f2e15","metadata":{},"outputs":[],"source":["pca = PCA()\npca.fit(X_train)"]},{"cell_type":"markdown","id":"5a3559f5-628f-4503-954b-dbf0b1f68d18","metadata":{},"outputs":[],"source":["\n","We can find the projection of the dataset onto the principal components, let's call it X_train_hat , this is our \"new\" dataset, it is the same shape as the original dataset.\n"]},{"cell_type":"code","id":"44e7ac51-b927-4ec1-9441-7120c02c3e83","metadata":{},"outputs":[],"source":["X_train_hat = pca.transform(X_train)\nprint(X_train_hat.shape)"]},{"cell_type":"markdown","id":"0c60a92f-b6b2-4c25-a9a5-1e98971a3ab2","metadata":{},"outputs":[],"source":["Let's look at the new dataset as a dataframe.\n"]},{"cell_type":"code","id":"a5cbc658-e407-4573-aeda-31be47660020","metadata":{},"outputs":[],"source":["X_train_hat_PCA = pd.DataFrame(columns=[f'Projection  on Component {i+1}' for i in range(len(X_train.columns))], data=X_train_hat)\nX_train_hat_PCA.head()"]},{"cell_type":"markdown","id":"9d1d3943-08ee-4fc3-8929-6c986d67301d","metadata":{},"outputs":[],"source":["Now, let's see how much variance can be explained using these principal components (PCs).\n"]},{"cell_type":"code","id":"eed91591-28f3-49af-aa32-e9dac6a04703","metadata":{},"outputs":[],"source":["plt.plot(pca.explained_variance_ratio_)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.title(\"Component-wise variance and cumulative explained variance\")"]},{"cell_type":"markdown","id":"c96fc778-2a85-483e-bfeb-eb884234ce3b","metadata":{},"outputs":[],"source":["In the graph above, the component-wise variance is depicted by the blue line, and the cumulative explained variance is explained by the orange line. We are able to explain \\~100% of the variance using just the first 20 PCs. Let's filter our dataset down to these 20 PCs.\n"]},{"cell_type":"code","id":"7118e540-6242-420d-86a1-13ec8f6f5c89","metadata":{},"outputs":[],"source":["N = 20\nX_train_hat_PCA = X_train_hat_PCA.iloc[:, :N]"]},{"cell_type":"markdown","id":"ba9a5f4f-744b-437e-a880-ca16d50d413d","metadata":{},"outputs":[],"source":["Let's create an Elastic Net model where `tol=0.2`,  `alpha=0.1` and  `l1_ratio=0.1` and fit the data with this model.\n"]},{"cell_type":"code","id":"e915d088-fb90-4c32-b233-55b8f1c4bd47","metadata":{},"outputs":[],"source":["enet = ElasticNet(tol = 0.2, alpha=100, l1_ratio=0.75)\nenet.fit(X_train_hat_PCA, y_train)"]},{"cell_type":"markdown","id":"cea012ab-3f4d-445e-8371-c87e97ee17ae","metadata":{},"outputs":[],"source":["## Exercise 6 (Optional)\n","\n","In this Exercise, create a Pipeline object, apply standard scaling, perform PCA and then finally fit an Elastic Net with `tol=0.2`, `alpha=0.1` and  `l1_ratio=0.1` parameters. Calculate the scores, $R^2$, on the training and testing data sets.\n"]},{"cell_type":"code","id":"86e87488-4ac3-416b-98a3-7d9db6ddc59f","metadata":{},"outputs":[],"source":["# Enter your code and run the cell\n"]},{"cell_type":"markdown","id":"43a3165d-19a6-44f2-ac71-f134252d6cf9","metadata":{},"outputs":[],"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","\n","Input=[ ('scaler', StandardScaler()), ('pca', PCA(n_components = N)), ('model', ElasticNet(tol =0.2, alpha=0.1, l1_ratio=0.1))]\n","pipe = Pipeline(Input)\n","pipe.fit(X_train, y_train)\n","print(\"R^2 on training  data \", pipe.score(X_train, y_train))\n","print(\"R^2 on testing data \", pipe.score(X_test,y_test))\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"7dd490be-e940-4854-a991-eba68a5b4afa","metadata":{},"outputs":[],"source":["# Congratulations! - You have completed the lab\n"]},{"cell_type":"markdown","id":"f6965994-8c61-4771-a8a2-36f96c7ce0ed","metadata":{},"outputs":[],"source":["## Author\n"]},{"cell_type":"markdown","id":"24b9639b-0340-4e3c-ba4b-0fcf646ca19b","metadata":{},"outputs":[],"source":["<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2021-01-01\" target=\"_blank\">Joseph Santarcangelo</a>\n"]},{"cell_type":"markdown","id":"3ba72e2b-7485-4444-aad3-cbac30622475","metadata":{},"outputs":[],"source":["### Other Contributors\n","\n","[Svitlana Kramar](https://www.linkedin.com/in/svitlana-kramar?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01)\n","\n","[Kopal Garg](https://www.linkedin.com/in/gargkopal/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01)\n"]},{"cell_type":"markdown","id":"d17dccc4-a2a9-4457-880f-d222dfa685ca","metadata":{},"outputs":[],"source":["<!--## Change Log\n","-->\n"]},{"cell_type":"markdown","id":"c201bf78-263a-4859-95dd-d19d6976292d","metadata":{},"outputs":[],"source":["<!--| Date (YYYY-MM-DD) | Version | Changed By  | Change Description                               |\n","| ----------------- | ------- | ----------- | ------------------------------------------------ |\n","| 2022-05-02        | 0.1     | Svitlana K. | Reviewed and fixed minor code and grammar errors |\n","| 2022-05-10        | 0.2     | Kopal G.    | Added the PCA example                            |-->\n"]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.11.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"48fa6f6e37eeb4a01e290d8fba5ac6627acf2ebd0ac7ab5ee8bcaec8180996d8"},"nbformat":4,"nbformat_minor":4}